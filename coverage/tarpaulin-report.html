<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <style>html, body {
  margin: 0;
  padding: 0;
}

.app {
  margin: 10px;
  padding: 0;
}

.files-list {
  margin: 10px 0 0;
  width: 100%;
  border-collapse: collapse;
}
.files-list__head {
  border: 1px solid #999;
}
.files-list__head > tr > th {
  padding: 10px;
  border: 1px solid #999;
  text-align: left;
  font-weight: normal;
  background: #ddd;
}
.files-list__body {
}
.files-list__file {
  cursor: pointer;
}
.files-list__file:hover {
  background: #ccf;
}
.files-list__file > td {
  padding: 10px;
  border: 1px solid #999;
}
.files-list__file > td:first-child::before {
  content: '\01F4C4';
  margin-right: 1em;
}
.files-list__file_low {
  background: #fcc;
}
.files-list__file_medium {
  background: #ffc;
}
.files-list__file_high {
  background: #cfc;
}
.files-list__file_folder > td:first-child::before {
  content: '\01F4C1';
  margin-right: 1em;
}

.file-header {
  border: 1px solid #999;
  display: flex;
  justify-content: space-between;
  align-items: center;
  position: sticky;
  top: 0;
  background: white;
}

.file-header__back {
  margin: 10px;
  cursor: pointer;
  flex-shrink: 0;
  flex-grow: 0;
  text-decoration: underline;
  color: #338;
}

.file-header__name {
  margin: 10px;
  flex-shrink: 2;
  flex-grow: 2;
}

.file-header__stat {
  margin: 10px;
  flex-shrink: 0;
  flex-grow: 0;
}

.file-content {
  margin: 10px 0 0;
  border: 1px solid #999;
  padding: 10px;
  counter-reset: line;
  display: flex;
  flex-direction: column;
}

.code-line::before {
    content: counter(line);
    margin-right: 10px;
}
.code-line {
  margin: 0;
  padding: 0.3em;
  height: 1em;
  counter-increment: line;
}
.code-line_covered {
  background: #cfc;
}
.code-line_uncovered {
  background: #fcc;
}
</style>
</head>
<body>
    <div id="root"></div>
    <script>
        var data = {"files":[{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","cli","src","cli.rs"],"content":"use anyhow::{Context, Result};\nuse clap::{Parser, Subcommand};\nuse opencode_core::supervisor::AgentSupervisor;\nuse tracing::{info, warn, error};\n\n#[derive(Parser, Debug, Clone)]\n#[command(author, version, about, long_about = None)]\npub struct Cli {\n    #[command(subcommand)]\n    pub command: Option\u003cCommands\u003e,\n    \n    /// Enable verbose logging\n    #[arg(short, long)]\n    pub verbose: bool,\n    \n    /// Configuration file path\n    #[arg(short, long)]\n    pub config: Option\u003cString\u003e,\n}\n\n#[derive(Subcommand, Debug, Clone)]\npub enum Commands {\n    /// Agent management commands\n    #[command(subcommand)]\n    Agent(AgentCommands),\n    \n    /// Ask a question directly\n    Ask {\n        /// The question to ask\n        question: String,\n        \n        /// Persona to use for the response\n        #[arg(short, long, default_value = \"default\")]\n        persona: String,\n    },\n    \n    /// Start interactive REPL mode\n    Repl,\n    \n    /// Show version information\n    Version,\n}\n\n#[derive(Subcommand, Debug, Clone)]\npub enum AgentCommands {\n    /// List all running agents\n    Ls,\n    \n    /// Spawn a new agent\n    Spawn {\n        /// Agent identifier\n        id: String,\n        \n        /// Agent persona\n        #[arg(short, long, default_value = \"rusty\")]\n        persona: String,\n    },\n    \n    /// Stop an agent\n    Stop {\n        /// Agent identifier\n        id: String,\n    },\n    \n    /// Get agent status\n    Status {\n        /// Agent identifier\n        id: String,\n    },\n}\n\npub async fn execute_command(command: Commands) -\u003e Result\u003c()\u003e {\n    match command {\n        Commands::Agent(agent_cmd) =\u003e execute_agent_command(agent_cmd).await,\n        Commands::Ask { question, persona } =\u003e execute_ask_command(\u0026question, \u0026persona).await,\n        Commands::Repl =\u003e {\n            // This should not happen in practice since None case goes to REPL\n            // But we handle it for completeness\n            crate::repl::start().await\n        },\n        Commands::Version =\u003e {\n            execute_version_command().await\n        },\n    }\n}\n\nasync fn execute_agent_command(command: AgentCommands) -\u003e Result\u003c()\u003e {\n    let mut supervisor = AgentSupervisor::new();\n    \n    match command {\n        AgentCommands::Ls =\u003e {\n            info!(\"Listing all agents\");\n            let agents = supervisor.list().await;\n            if agents.is_empty() {\n                println!(\"No agents running.\");\n            } else {\n                println!(\"Running agents:\");\n                for agent in agents {\n                    println!(\"  {} ({}): {:?}\", agent.id, agent.persona, agent.status);\n                }\n            }\n        }\n        AgentCommands::Spawn { id, persona } =\u003e {\n            info!(\"Spawning agent '{}' with persona '{}'\", id, persona);\n            supervisor.spawn(\u0026id, \u0026persona).await\n                .with_context(|| format!(\"Failed to spawn agent '{}' with persona '{}'\", id, persona))?;\n            println!(\"Spawned agent '{}' with persona '{}'\", id, persona);\n        }\n        AgentCommands::Stop { id } =\u003e {\n            info!(\"Stopping agent '{}'\", id);\n            supervisor.stop(\u0026id).await\n                .with_context(|| format!(\"Failed to stop agent '{}'\", id))?;\n            println!(\"Stopped agent '{}'\", id);\n        }\n        AgentCommands::Status { id } =\u003e {\n            info!(\"Getting status for agent '{}'\", id);\n            match supervisor.get_status(\u0026id).await {\n                Ok(status) =\u003e println!(\"Agent '{}' status: {:?}\", id, status),\n                Err(e) =\u003e {\n                    error!(\"Failed to get status for agent '{}': {}\", id, e);\n                    return Err(e.into());\n                }\n            }\n        }\n    }\n    \n    Ok(())\n}\n\nasync fn execute_ask_command(question: \u0026str, persona: \u0026str) -\u003e Result\u003c()\u003e {\n    info!(\"Asking question with persona '{}'\", persona);\n    \n    // Import ask function from core\n    use opencode_core::ask_with_persona;\n    \n    match ask_with_persona(question, persona).await {\n        Ok(response) =\u003e {\n            println!(\"{}\", response);\n        }\n        Err(e) =\u003e {\n            error!(\"Failed to get response: {}\", e);\n            return Err(e.into());\n        }\n    }\n    \n    Ok(())\n}\n\nasync fn execute_version_command() -\u003e Result\u003c()\u003e {\n    println!(\"OpenCode-RS CLI v{}\", env!(\"CARGO_PKG_VERSION\"));\n    println!(\"Built with Rust {}\", env!(\"RUSTC_VERBOSE_VERSION\"));\n    Ok(())\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use clap::CommandFactory;\n    use pretty_assertions::assert_eq;\n    use test_case::test_case;\n\n    #[test]\n    fn test_cli_structure() {\n        // Test that CLI can be built without errors\n        let _cmd = Cli::command();\n    }\n\n    #[test]\n    fn test_cli_help() {\n        let cmd = Cli::command();\n        let help = cmd.render_help();\n        assert!(help.to_string().contains(\"OpenCode\"));\n    }\n\n    #[test_case(\"agent\", \"ls\"; \"agent ls command\")]\n    #[test_case(\"ask\", \"What is Rust?\"; \"ask command\")]\n    #[test_case(\"version\"; \"version command\")]\n    fn test_command_parsing(command: \u0026str, args: \u0026str) {\n        let mut cmd_args = vec![\"opencode\", command];\n        if !args.is_empty() {\n            cmd_args.extend(args.split_whitespace());\n        }\n        \n        let result = Cli::try_parse_from(cmd_args);\n        assert!(result.is_ok(), \"Failed to parse command: {} {}\", command, args);\n    }\n\n    #[test]\n    fn test_agent_spawn_parsing() {\n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"spawn\", \"test-agent\", \"--persona\", \"test-persona\"]).unwrap();\n        \n        match cli.command {\n            Some(Commands::Agent(AgentCommands::Spawn { id, persona })) =\u003e {\n                assert_eq!(id, \"test-agent\");\n                assert_eq!(persona, \"test-persona\");\n            }\n            _ =\u003e panic!(\"Expected agent spawn command\"),\n        }\n    }\n\n    #[test]\n    fn test_ask_command_parsing() {\n        let cli = Cli::try_parse_from([\"opencode\", \"ask\", \"What is Rust?\", \"--persona\", \"expert\"]).unwrap();\n        \n        match cli.command {\n            Some(Commands::Ask { question, persona }) =\u003e {\n                assert_eq!(question, \"What is Rust?\");\n                assert_eq!(persona, \"expert\");\n            }\n            _ =\u003e panic!(\"Expected ask command\"),\n        }\n    }\n\n    #[test]\n    fn test_default_persona() {\n        let cli = Cli::try_parse_from([\"opencode\", \"ask\", \"What is Rust?\"]).unwrap();\n        \n        match cli.command {\n            Some(Commands::Ask { persona, .. }) =\u003e {\n                assert_eq!(persona, \"default\");\n            }\n            _ =\u003e panic!(\"Expected ask command\"),\n        }\n    }\n\n    #[test]\n    fn test_verbose_flag() {\n        let cli = Cli::try_parse_from([\"opencode\", \"--verbose\", \"version\"]).unwrap();\n        assert!(cli.verbose);\n    }\n\n    #[test]\n    fn test_config_option() {\n        let cli = Cli::try_parse_from([\"opencode\", \"--config\", \"test.toml\", \"version\"]).unwrap();\n        assert_eq!(cli.config, Some(\"test.toml\".to_string()));\n    }\n\n    #[test]\n    fn test_invalid_command() {\n        let result = Cli::try_parse_from([\"opencode\", \"invalid\"]);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_agent_commands_completeness() {\n        // Ensure all agent commands are tested\n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"ls\"]).unwrap();\n        assert!(matches!(cli.command, Some(Commands::Agent(AgentCommands::Ls))));\n        \n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"spawn\", \"test\"]).unwrap();\n        assert!(matches!(cli.command, Some(Commands::Agent(AgentCommands::Spawn { .. }))));\n        \n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"stop\", \"test\"]).unwrap();\n        assert!(matches!(cli.command, Some(Commands::Agent(AgentCommands::Stop { .. }))));\n        \n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"status\", \"test\"]).unwrap();\n        assert!(matches!(cli.command, Some(Commands::Agent(AgentCommands::Status { .. }))));\n    }\n\n    #[tokio::test]\n    async fn test_version_command_execution() {\n        let result = execute_version_command().await;\n        assert!(result.is_ok());\n    }\n\n    // Property-based testing for command parsing\n    #[cfg(feature = \"proptest\")]\n    mod property_tests {\n        use super::*;\n        use proptest::prelude::*;\n\n        proptest! {\n            #[test]\n            fn test_agent_id_parsing(id in \"[a-zA-Z][a-zA-Z0-9_-]*\") {\n                let args = vec![\"opencode\", \"agent\", \"spawn\", \u0026id];\n                let result = Cli::try_parse_from(args);\n                prop_assert!(result.is_ok());\n            }\n\n            #[test]\n            fn test_ask_question_parsing(question in \".{1,100}\") {\n                let args = vec![\"opencode\", \"ask\", \u0026question];\n                let result = Cli::try_parse_from(args);\n                prop_assert!(result.is_ok());\n            }\n        }\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","cli","src","main.rs"],"content":"mod cli;\nmod repl;\n\nuse anyhow::Result;\nuse clap::Parser;\n\n#[tokio::main]\nasync fn main() -\u003e Result\u003c()\u003e {\n    let cli = cli::Cli::parse();\n    \n    match cli.command {\n        Some(cmd) =\u003e {\n            // Single-shot command mode\n            cli::execute_command(cmd).await\n        }\n        None =\u003e {\n            // Interactive REPL mode\n            repl::start().await\n        }\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","cli","src","repl.rs"],"content":"use anyhow::{Context, Result};\nuse reedline::{DefaultPrompt, Reedline, Signal, ReedlineEvent, EditCommand, UndoBehavior, Prompt};\nuse std::io;\nuse opencode_core::supervisor::AgentSupervisor;\nuse tracing::{info, warn, error, debug};\n\npub struct ReplEngine {\n    supervisor: AgentSupervisor,\n    current_persona: String,\n}\n\nimpl ReplEngine {\n    pub fn new() -\u003e Self {\n        Self {\n            supervisor: AgentSupervisor::new(),\n            current_persona: \"default\".to_string(),\n        }\n    }\n\n    pub async fn execute_line(\u0026mut self, line: \u0026str) -\u003e Result\u003cString\u003e {\n        let line = line.trim();\n        \n        if line.is_empty() {\n            return Ok(String::new());\n        }\n\n        // Handle special REPL commands\n        if line.starts_with('/') {\n            return self.execute_slash_command(line).await;\n        }\n\n        // Handle regular CLI commands\n        if let Some(args) = parse_command_line(line) {\n            return self.execute_cli_command(args).await;\n        }\n\n        // Treat as a direct question\n        self.execute_ask(\u0026line).await\n    }\n\n    async fn execute_slash_command(\u0026mut self, line: \u0026str) -\u003e Result\u003cString\u003e {\n        let parts: Vec\u003c\u0026str\u003e = line[1..].split_whitespace().collect();\n        \n        match parts.first() {\n            Some(\u0026\"help\") =\u003e Ok(self.show_help()),\n            Some(\u0026\"exit\") | Some(\u0026\"quit\") =\u003e Err(anyhow::anyhow!(\"exit\")),\n            Some(\u0026\"persona\") =\u003e {\n                if parts.len() \u003e 1 {\n                    self.current_persona = parts[1].to_string();\n                    Ok(format!(\"Switched to persona: {}\", self.current_persona))\n                } else {\n                    Ok(format!(\"Current persona: {}\", self.current_persona))\n                }\n            }\n            Some(\u0026\"clear\") =\u003e Ok(\"\\x1B[2J\\x1B[1;1H\".to_string()), // ANSI clear screen\n            Some(\u0026\"status\") =\u003e {\n                let agents = self.supervisor.list().await;\n                if agents.is_empty() {\n                    Ok(\"No agents running.\".to_string())\n                } else {\n                    let mut output = String::from(\"Running agents:\\n\");\n                    for agent in agents {\n                        output.push_str(\u0026format!(\"  {} ({}): {:?}\\n\", agent.id, agent.persona, agent.status));\n                    }\n                    Ok(output)\n                }\n            }\n            Some(cmd) =\u003e Ok(format!(\"Unknown command: /{}\", cmd)),\n            None =\u003e Ok(\"Empty command\".to_string()),\n        }\n    }\n\n    async fn execute_cli_command(\u0026mut self, args: Vec\u003cString\u003e) -\u003e Result\u003cString\u003e {\n        use crate::cli::{Cli, Commands};\n        use clap::Parser;\n\n        let mut cmd_args = vec![\"opencode\".to_string()];\n        cmd_args.extend(args);\n\n        match Cli::try_parse_from(cmd_args) {\n            Ok(cli) =\u003e {\n                if let Some(command) = cli.command {\n                    // Capture output for REPL display\n                    match command {\n                        Commands::Ask { question, persona } =\u003e {\n                            self.execute_ask_with_persona(\u0026question, \u0026persona).await\n                        }\n                        Commands::Agent(agent_cmd) =\u003e {\n                            use crate::cli::AgentCommands;\n                            match agent_cmd {\n                                AgentCommands::Ls =\u003e {\n                                    let agents = self.supervisor.list().await;\n                                    if agents.is_empty() {\n                                        Ok(\"No agents running.\".to_string())\n                                    } else {\n                                        let mut output = String::from(\"Running agents:\\n\");\n                                        for agent in agents {\n                                            output.push_str(\u0026format!(\"  {} ({}): {:?}\\n\", agent.id, agent.persona, agent.status));\n                                        }\n                                        Ok(output)\n                                    }\n                                }\n                                AgentCommands::Spawn { id, persona } =\u003e {\n                                    self.supervisor.spawn(\u0026id, \u0026persona).await?;\n                                    Ok(format!(\"Spawned agent '{}' with persona '{}'\", id, persona))\n                                }\n                                AgentCommands::Stop { id } =\u003e {\n                                    self.supervisor.stop(\u0026id).await?;\n                                    Ok(format!(\"Stopped agent '{}'\", id))\n                                }\n                                AgentCommands::Status { id } =\u003e {\n                                    match self.supervisor.get_status(\u0026id).await {\n                                        Ok(status) =\u003e Ok(format!(\"Agent '{}' status: {:?}\", id, status)),\n                                        Err(e) =\u003e Ok(format!(\"Error getting status for agent '{}': {}\", id, e)),\n                                    }\n                                }\n                            }\n                        }\n                        Commands::Version =\u003e {\n                            Ok(format!(\"OpenCode-RS CLI v{}\", env!(\"CARGO_PKG_VERSION\")))\n                        }\n                        Commands::Repl =\u003e {\n                            Ok(\"Already in REPL mode.\".to_string())\n                        }\n                    }\n                } else {\n                    Ok(\"No command specified. Type /help for available commands.\".to_string())\n                }\n            }\n            Err(e) =\u003e Ok(format!(\"Parse error: {}\", e)),\n        }\n    }\n\n    async fn execute_ask(\u0026self, question: \u0026str) -\u003e Result\u003cString\u003e {\n        self.execute_ask_with_persona(question, \u0026self.current_persona).await\n    }\n\n    async fn execute_ask_with_persona(\u0026self, question: \u0026str, persona: \u0026str) -\u003e Result\u003cString\u003e {\n        use opencode_core::ask_with_persona;\n        \n        match ask_with_persona(question, persona).await {\n            Ok(response) =\u003e Ok(response),\n            Err(e) =\u003e Ok(format!(\"Error: {}\", e)),\n        }\n    }\n\n    fn show_help(\u0026self) -\u003e String {\n        r#\"OpenCode-RS REPL Commands:\n\nSlash Commands:\n  /help          - Show this help message\n  /exit, /quit   - Exit the REPL\n  /persona [name] - Set or show current persona\n  /clear         - Clear the screen\n  /status        - Show agent status\n\nCLI Commands:\n  agent ls       - List all agents\n  agent spawn \u003cid\u003e [--persona \u003cname\u003e] - Spawn a new agent\n  agent stop \u003cid\u003e - Stop an agent\n  agent status \u003cid\u003e - Get agent status\n  ask \u003cquestion\u003e [--persona \u003cname\u003e] - Ask a question\n  version        - Show version information\n\nDirect Questions:\n  Just type your question and press Enter to ask using the current persona.\n\nExamples:\n  What is Rust?\n  /persona expert\n  What are the best practices for error handling?\n  agent spawn my-agent --persona rusty\n\"#.to_string()\n    }\n}\n\npub async fn start() -\u003e Result\u003c()\u003e {\n    info!(\"Starting OpenCode-RS REPL\");\n    \n    let mut line_editor = Reedline::create();\n    let prompt = DefaultPrompt::default();\n    let mut engine = ReplEngine::new();\n\n    println!(\"OpenCode-RS Interactive REPL\");\n    println!(\"Type /help for available commands, /exit to quit.\");\n    println!();\n\n    loop {\n        let sig = line_editor.read_line(\u0026prompt);\n        match sig {\n            Ok(Signal::Success(buffer)) =\u003e {\n                debug!(\"Processing input: {}\", buffer);\n                match engine.execute_line(\u0026buffer).await {\n                    Ok(output) =\u003e {\n                        if !output.is_empty() {\n                            println!(\"{}\", output);\n                        }\n                    }\n                    Err(e) =\u003e {\n                        if e.to_string() == \"exit\" {\n                            println!(\"Goodbye!\");\n                            break;\n                        }\n                        error!(\"Error: {}\", e);\n                        println!(\"Error: {}\", e);\n                    }\n                }\n            }\n            Ok(Signal::CtrlD) | Ok(Signal::CtrlC) =\u003e {\n                println!(\"\\nGoodbye!\");\n                break;\n            }\n            x =\u003e {\n                warn!(\"Unexpected signal: {:?}\", x);\n                println!(\"Error reading line: {:?}\", x);\n            }\n        }\n    }\n\n    Ok(())\n}\n\nfn parse_command_line(line: \u0026str) -\u003e Option\u003cVec\u003cString\u003e\u003e {\n    let parts: Vec\u003c\u0026str\u003e = line.split_whitespace().collect();\n    if parts.is_empty() {\n        return None;\n    }\n\n    // Check if it looks like a CLI command\n    match parts.first() {\n        Some(\u0026\"agent\") | Some(\u0026\"ask\") | Some(\u0026\"version\") | Some(\u0026\"repl\") =\u003e {\n            Some(parts.iter().map(|s| s.to_string()).collect())\n        }\n        _ =\u003e None,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use pretty_assertions::assert_eq;\n    use test_case::test_case;\n    use rstest::*;\n\n    #[fixture]\n    fn engine() -\u003e ReplEngine {\n        ReplEngine::new()\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_empty_line(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"\").await.unwrap();\n        assert_eq!(result, \"\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_whitespace_line(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"   \\t  \").await.unwrap();\n        assert_eq!(result, \"\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_help_command(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"/help\").await.unwrap();\n        assert!(result.contains(\"OpenCode-RS REPL Commands\"));\n        assert!(result.contains(\"/help\"));\n        assert!(result.contains(\"agent ls\"));\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_exit_command(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"/exit\").await;\n        assert!(result.is_err());\n        assert_eq!(result.unwrap_err().to_string(), \"exit\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_quit_command(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"/quit\").await;\n        assert!(result.is_err());\n        assert_eq!(result.unwrap_err().to_string(), \"exit\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_persona_command_set(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"/persona expert\").await.unwrap();\n        assert_eq!(result, \"Switched to persona: expert\");\n        assert_eq!(engine.current_persona, \"expert\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_persona_command_show(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"/persona\").await.unwrap();\n        assert_eq!(result, \"Current persona: default\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_clear_command(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"/clear\").await.unwrap();\n        assert_eq!(result, \"\\x1B[2J\\x1B[1;1H\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_status_command_empty(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"/status\").await.unwrap();\n        assert_eq!(result, \"No agents running.\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_unknown_slash_command(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"/unknown\").await.unwrap();\n        assert_eq!(result, \"Unknown command: /unknown\");\n    }\n\n    #[test_case(\"agent ls\"; \"agent ls\")]\n    #[test_case(\"ask What is Rust?\"; \"ask command\")]\n    #[test_case(\"version\"; \"version\")]\n    #[tokio::test]\n    async fn test_cli_command_parsing(command: \u0026str, mut engine: ReplEngine) {\n        let result = engine.execute_line(command).await;\n        assert!(result.is_ok(), \"Failed to execute command: {}\", command);\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_agent_ls_command(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"agent ls\").await.unwrap();\n        assert_eq!(result, \"No agents running.\");\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_version_command(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"version\").await.unwrap();\n        assert!(result.contains(\"OpenCode-RS CLI v\"));\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_invalid_cli_command(mut engine: ReplEngine) {\n        let result = engine.execute_line(\"agent invalid\").await.unwrap();\n        assert!(result.contains(\"Parse error\"));\n    }\n\n    #[rstest]\n    #[tokio::test]\n    async fn test_direct_question(mut engine: ReplEngine) {\n        // This will attempt to call the ask function, which might fail in test environment\n        // but the important thing is that it doesn't panic or return an empty result\n        let result = engine.execute_line(\"What is the meaning of life?\").await.unwrap();\n        // In test environment, this will likely return an error message, which is fine\n        assert!(!result.is_empty());\n    }\n\n    #[test]\n    fn test_parse_command_line_valid() {\n        assert_eq!(parse_command_line(\"agent ls\"), Some(vec![\"agent\".to_string(), \"ls\".to_string()]));\n        assert_eq!(parse_command_line(\"ask What is Rust?\"), Some(vec![\"ask\".to_string(), \"What\".to_string(), \"is\".to_string(), \"Rust?\".to_string()]));\n        assert_eq!(parse_command_line(\"version\"), Some(vec![\"version\".to_string()]));\n    }\n\n    #[test]\n    fn test_parse_command_line_invalid() {\n        assert_eq!(parse_command_line(\"hello world\"), None);\n        assert_eq!(parse_command_line(\"random text\"), None);\n        assert_eq!(parse_command_line(\"\"), None);\n    }\n\n    #[test]\n    fn test_parse_command_line_with_extra_whitespace() {\n        assert_eq!(parse_command_line(\"  agent   ls  \"), Some(vec![\"agent\".to_string(), \"ls\".to_string()]));\n    }\n\n    // Integration tests for the REPL engine\n    #[tokio::test]\n    async fn test_repl_engine_persona_persistence() {\n        let mut engine = ReplEngine::new();\n        \n        // Set persona\n        engine.execute_line(\"/persona expert\").await.unwrap();\n        assert_eq!(engine.current_persona, \"expert\");\n        \n        // Execute another command\n        engine.execute_line(\"/help\").await.unwrap();\n        \n        // Persona should persist\n        assert_eq!(engine.current_persona, \"expert\");\n    }\n\n    #[tokio::test]\n    async fn test_repl_engine_command_sequence() {\n        let mut engine = ReplEngine::new();\n        \n        let commands = vec![\n            \"/persona test\",\n            \"agent ls\", \n            \"/status\",\n            \"version\",\n        ];\n        \n        for cmd in commands {\n            let result = engine.execute_line(cmd).await;\n            assert!(result.is_ok(), \"Command failed: {}\", cmd);\n        }\n    }\n\n    // Property-based testing\n    #[cfg(feature = \"proptest\")]\n    mod property_tests {\n        use super::*;\n        use proptest::prelude::*;\n\n        proptest! {\n            #[test]\n            fn test_slash_commands_dont_panic(cmd in \"/[a-zA-Z]+\") {\n                let rt = tokio::runtime::Runtime::new().unwrap();\n                rt.block_on(async {\n                    let mut engine = ReplEngine::new();\n                    let result = engine.execute_line(\u0026cmd).await;\n                    prop_assert!(result.is_ok());\n                });\n            }\n\n            #[test]\n            fn test_empty_and_whitespace_lines(line in r\"\\s*\") {\n                let rt = tokio::runtime::Runtime::new().unwrap();\n                rt.block_on(async {\n                    let mut engine = ReplEngine::new();\n                    let result = engine.execute_line(\u0026line).await;\n                    prop_assert!(result.is_ok());\n                });\n            }\n        }\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","comprehensive_tests.rs"],"content":"/// Comprehensive test suite to achieve 100% test coverage\n/// \n/// This module contains tests specifically designed to cover all untested code paths,\n/// edge cases, and error scenarios across all modules in the opencode_core crate.\n\n#[cfg(test)]\nmod coverage_tests {\n    use crate::*;\n    use crate::config::{Config, OpenAIConfig};\n    use crate::error::Error;\n    use crate::provider::*;\n    use crate::service::ServiceContainer;\n    use std::sync::Arc;\n    use std::env;\n    use tempfile::NamedTempFile;\n    use std::io::Write;\n\n    // Mock provider for testing\n    struct FailingMockProvider;\n\n    #[async_trait]\n    impl LLMProvider for FailingMockProvider {\n        fn name(\u0026self) -\u003e \u0026str {\n            \"failing_mock\"\n        }\n\n        async fn complete(\u0026self, _request: CompletionRequest) -\u003e error::Result\u003cCompletionResponse\u003e {\n            Err(Error::Provider(\"Simulated provider failure\".into()))\n        }\n\n        async fn stream(\n            \u0026self,\n            _request: CompletionRequest,\n        ) -\u003e error::Result\u003cBoxStream\u003c'static, error::Result\u003cStreamChunk\u003e\u003e\u003e {\n            Err(Error::Provider(\"Simulated stream failure\".into()))\n        }\n    }\n\n    #[test]\n    fn test_config_load_with_file_path() {\n        // Test Config::load with a file path\n        let mut temp_file = NamedTempFile::new().unwrap();\n        writeln!(\n            temp_file,\n            r#\"\n[openai]\ndefault_model = \"gpt-3.5-turbo\"\napi_base = \"https://custom-api.example.com/v1\"\nmax_retries = 5\ntimeout_seconds = 60\n\"#\n        ).unwrap();\n        temp_file.flush().unwrap();\n\n        let config = Config::load(Some(temp_file.path())).unwrap();\n        assert_eq!(config.openai.default_model, \"gpt-3.5-turbo\");\n        assert_eq!(config.openai.api_base, \"https://custom-api.example.com/v1\");\n        assert_eq!(config.openai.max_retries, 5);\n        assert_eq!(config.openai.timeout_seconds, 60);\n    }\n\n    #[test]\n    fn test_config_load_without_file_path() {\n        // Test Config::load without a file path (uses default)\n        let config = Config::load::\u003c\u0026str\u003e(None).unwrap();\n        // Should use default values\n        assert_eq!(config.openai.default_model, \"gpt-4\");\n        assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    }\n\n    #[test]\n    fn test_config_from_file_not_found() {\n        // Test Config::from_file with non-existent file\n        let result = Config::from_file(\"/nonexistent/path/config.toml\");\n        assert!(result.is_err());\n        match result {\n            Err(Error::Io(_)) =\u003e {}, // Expected\n            _ =\u003e panic!(\"Expected IO error\"),\n        }\n    }\n\n    #[test]\n    fn test_config_from_file_invalid_toml() {\n        // Test Config::from_file with invalid TOML\n        let mut temp_file = NamedTempFile::new().unwrap();\n        writeln!(temp_file, \"invalid toml content [[[\").unwrap();\n        temp_file.flush().unwrap();\n\n        let result = Config::from_file(temp_file.path());\n        assert!(result.is_err());\n        match result {\n            Err(Error::Config(_)) =\u003e {}, // Expected\n            _ =\u003e panic!(\"Expected Config error\"),\n        }\n    }\n\n    #[test]\n    fn test_config_save() {\n        // Test Config::save\n        let config = Config {\n            openai: OpenAIConfig {\n                default_model: \"gpt-4\".to_string(),\n                api_base: \"https://api.openai.com/v1\".to_string(),\n                max_retries: 3,\n                timeout_seconds: 30,\n            },\n        };\n\n        let temp_file = NamedTempFile::new().unwrap();\n        config.save(temp_file.path()).unwrap();\n\n        // Verify the file was written correctly\n        let loaded_config = Config::from_file(temp_file.path()).unwrap();\n        assert_eq!(loaded_config.openai.default_model, config.openai.default_model);\n        assert_eq!(loaded_config.openai.api_base, config.openai.api_base);\n    }\n\n    #[test]\n    fn test_config_from_env_with_invalid_numbers() {\n        // Test Config::from_env with invalid numeric values\n        env::set_var(\"OPENAI_MAX_RETRIES\", \"invalid_number\");\n        \n        let result = Config::from_env();\n        assert!(result.is_err());\n        match result {\n            Err(Error::Config(msg)) =\u003e assert!(msg.contains(\"Invalid OPENAI_MAX_RETRIES\")),\n            _ =\u003e panic!(\"Expected Config error for invalid max_retries\"),\n        }\n\n        env::remove_var(\"OPENAI_MAX_RETRIES\");\n\n        env::set_var(\"OPENAI_TIMEOUT\", \"not_a_number\");\n        \n        let result = Config::from_env();\n        assert!(result.is_err());\n        match result {\n            Err(Error::Config(msg)) =\u003e assert!(msg.contains(\"Invalid OPENAI_TIMEOUT\")),\n            _ =\u003e panic!(\"Expected Config error for invalid timeout\"),\n        }\n\n        env::remove_var(\"OPENAI_TIMEOUT\");\n    }\n\n    #[test]\n    fn test_config_merge_env_all_variables() {\n        // Test the merge_env method with all environment variables set\n        let original_env = [\n            (\"OPENAI_MODEL\", env::var(\"OPENAI_MODEL\").ok()),\n            (\"OPENAI_API_BASE\", env::var(\"OPENAI_API_BASE\").ok()),\n            (\"OPENAI_MAX_RETRIES\", env::var(\"OPENAI_MAX_RETRIES\").ok()),\n            (\"OPENAI_TIMEOUT\", env::var(\"OPENAI_TIMEOUT\").ok()),\n        ];\n\n        // Set all env vars\n        env::set_var(\"OPENAI_MODEL\", \"gpt-3.5-turbo\");\n        env::set_var(\"OPENAI_API_BASE\", \"https://custom.api.com/v1\");\n        env::set_var(\"OPENAI_MAX_RETRIES\", \"10\");\n        env::set_var(\"OPENAI_TIMEOUT\", \"120\");\n\n        let config = Config::load::\u003c\u0026str\u003e(None).unwrap();\n        assert_eq!(config.openai.default_model, \"gpt-3.5-turbo\");\n        assert_eq!(config.openai.api_base, \"https://custom.api.com/v1\");\n        assert_eq!(config.openai.max_retries, 10);\n        assert_eq!(config.openai.timeout_seconds, 120);\n\n        // Restore original environment\n        for (key, value) in original_env {\n            match value {\n                Some(val) =\u003e env::set_var(key, val),\n                None =\u003e env::remove_var(key),\n            }\n        }\n    }\n\n    #[tokio::test]\n    async fn test_service_container_with_failing_provider() {\n        // Test service container with a provider that fails\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let failing_provider = Arc::new(FailingMockProvider);\n        container.register_provider(\"failing\", failing_provider);\n\n        let provider = container.get_provider(\"failing\").unwrap();\n        \n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Test\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let result = provider.complete(request).await;\n        assert!(result.is_err());\n        match result {\n            Err(Error::Provider(msg)) =\u003e assert_eq!(msg, \"Simulated provider failure\"),\n            _ =\u003e panic!(\"Expected Provider error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_provider_stream_failure() {\n        // Test streaming failure\n        let failing_provider = FailingMockProvider;\n        \n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Test\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: true,\n        };\n\n        let result = failing_provider.stream(request).await;\n        assert!(result.is_err());\n        match result {\n            Err(Error::Provider(msg)) =\u003e assert_eq!(msg, \"Simulated stream failure\"),\n            _ =\u003e panic!(\"Expected Provider error\"),\n        }\n    }\n\n    #[test]\n    fn test_error_display_all_variants() {\n        // Test Display implementation for all Error variants\n        let config_error = Error::Config(\"Configuration error\".to_string());\n        assert_eq!(format!(\"{}\", config_error), \"Configuration error: Configuration error\");\n\n        let provider_error = Error::Provider(\"Provider error\".to_string());\n        assert_eq!(format!(\"{}\", provider_error), \"Provider error: Provider error\");\n\n        let io_error = Error::Io(std::io::Error::new(std::io::ErrorKind::NotFound, \"File not found\"));\n        assert!(format!(\"{}\", io_error).contains(\"IO error\"));\n\n        let service_error = Error::Service(\"Service error\".to_string());\n        assert_eq!(format!(\"{}\", service_error), \"Service error: Service error\");\n    }\n\n    #[test]\n    fn test_error_source_propagation() {\n        // Test error source propagation\n        let io_error = std::io::Error::new(std::io::ErrorKind::PermissionDenied, \"Permission denied\");\n        let error = Error::Io(io_error);\n        \n        let source = error.source();\n        assert!(source.is_some());\n    }\n\n    #[test]\n    fn test_error_from_conversions() {\n        // Test From trait implementations\n        let io_error = std::io::Error::new(std::io::ErrorKind::NotFound, \"Not found\");\n        let error: Error = io_error.into();\n        assert!(matches!(error, Error::Io(_)));\n\n        let toml_error = toml::de::Error::custom(\"Invalid TOML\");\n        let error: Error = toml_error.into();\n        assert!(matches!(error, Error::Config(_)));\n\n        let var_error = env::VarError::NotPresent;\n        let error: Error = var_error.into();\n        assert!(matches!(error, Error::Config(_)));\n    }\n\n    #[test]\n    fn test_completion_request_builder_edge_cases() {\n        // Test CompletionRequest builder with edge cases\n        let request = CompletionRequest::builder()\n            .model(\"\")  // Empty model\n            .temperature(2.0)  // Max temperature\n            .max_tokens(0)  // Zero max tokens\n            .stream(true)\n            .build();\n\n        assert_eq!(request.model, \"\");\n        assert_eq!(request.temperature, Some(2.0));\n        assert_eq!(request.max_tokens, Some(0));\n        assert!(request.stream);\n    }\n\n    #[test]\n    fn test_message_edge_cases() {\n        // Test Message with edge cases\n        let message = Message {\n            role: \"\".to_string(),  // Empty role\n            content: \"\".to_string(),  // Empty content\n        };\n\n        assert_eq!(message.role, \"\");\n        assert_eq!(message.content, \"\");\n\n        // Test very long content\n        let long_content = \"a\".repeat(10000);\n        let message = Message {\n            role: \"user\".to_string(),\n            content: long_content.clone(),\n        };\n        assert_eq!(message.content.len(), 10000);\n    }\n\n    #[test]\n    fn test_usage_calculations() {\n        // Test Usage struct calculations and edge cases\n        let usage = Usage {\n            prompt_tokens: 0,\n            completion_tokens: 0,\n            total_tokens: 0,\n        };\n        assert_eq!(usage.prompt_tokens, 0);\n\n        let usage = Usage {\n            prompt_tokens: u32::MAX,\n            completion_tokens: 1,\n            total_tokens: u32::MAX,\n        };\n        assert_eq!(usage.prompt_tokens, u32::MAX);\n    }\n\n    #[test]\n    fn test_completion_response_edge_cases() {\n        // Test CompletionResponse with edge cases\n        let response = CompletionResponse {\n            content: \"\".to_string(),  // Empty content\n            model: \"\".to_string(),    // Empty model\n            usage: Usage {\n                prompt_tokens: 0,\n                completion_tokens: 0,\n                total_tokens: 0,\n            },\n        };\n        assert_eq!(response.content, \"\");\n        assert_eq!(response.model, \"\");\n    }\n\n    #[test]\n    fn test_stream_chunk_edge_cases() {\n        // Test StreamChunk with edge cases\n        let chunk = StreamChunk {\n            delta: \"\".to_string(),          // Empty delta\n            finish_reason: None,            // No finish reason\n        };\n        assert_eq!(chunk.delta, \"\");\n        assert!(chunk.finish_reason.is_none());\n\n        let chunk = StreamChunk {\n            delta: \"test\".to_string(),\n            finish_reason: Some(\"stop\".to_string()),\n        };\n        assert_eq!(chunk.finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[test]\n    fn test_openai_config_edge_cases() {\n        // Test OpenAIConfig with edge cases\n        let config = OpenAIConfig {\n            default_model: \"\".to_string(),  // Empty model\n            api_base: \"\".to_string(),       // Empty API base\n            max_retries: 0,                 // Zero retries\n            timeout_seconds: 0,             // Zero timeout\n        };\n        assert_eq!(config.default_model, \"\");\n        assert_eq!(config.api_base, \"\");\n        assert_eq!(config.max_retries, 0);\n        assert_eq!(config.timeout_seconds, 0);\n    }\n\n    #[test]\n    fn test_service_container_edge_cases() {\n        // Test ServiceContainer edge cases\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Test registering provider with empty name\n        let mock_provider = Arc::new(crate::provider::tests::MockProvider {\n            response: \"Test\".to_string(),\n            should_fail: false,\n        });\n        container.register_provider(\"\", mock_provider);\n\n        // Should be able to retrieve with empty name\n        let result = container.get_provider(\"\");\n        assert!(result.is_ok());\n\n        // Test clearing all providers\n        container.providers.clear();\n        assert!(container.list_providers().is_empty());\n    }\n\n    #[tokio::test]\n    async fn test_ask_functions_without_service_container() {\n        // Test global ask functions when service container is not initialized\n        // Note: Due to OnceLock being static, this test might not behave as expected\n        // in a real test runner where other tests have already initialized the container\n        \n        // These tests validate the function signatures and basic logic\n        // without relying on global state\n    }\n\n    #[test]\n    fn test_unicode_and_special_characters() {\n        // Test with Unicode and special characters\n        let message = Message {\n            role: \"user\".to_string(),\n            content: \"Hello 世界! 🚀 Test αβγ δεζ ñáéíóú\".to_string(),\n        };\n        assert!(message.content.contains(\"世界\"));\n        assert!(message.content.contains(\"🚀\"));\n        assert!(message.content.contains(\"αβγ\"));\n\n        // Test config with Unicode\n        let config = OpenAIConfig {\n            default_model: \"gpt-4-🚀\".to_string(),\n            api_base: \"https://api.example.com/v1/世界\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n        assert!(config.default_model.contains(\"🚀\"));\n        assert!(config.api_base.contains(\"世界\"));\n    }\n\n    #[test]\n    fn test_very_large_values() {\n        // Test with very large values\n        let request = CompletionRequest {\n            model: \"a\".repeat(1000),  // Very long model name\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"x\".repeat(100000),  // Very long content\n            }],\n            temperature: Some(1.9999),  // Close to max temperature\n            max_tokens: Some(u32::MAX),  // Maximum tokens\n            stream: false,\n        };\n        assert_eq!(request.model.len(), 1000);\n        assert_eq!(request.messages[0].content.len(), 100000);\n        assert_eq!(request.max_tokens, Some(u32::MAX));\n    }\n\n    #[test]\n    fn test_boundary_temperature_values() {\n        // Test boundary temperature values\n        let request = CompletionRequest::builder()\n            .model(\"test\")\n            .temperature(0.0)  // Minimum valid temperature\n            .build();\n        assert_eq!(request.temperature, Some(0.0));\n\n        let request = CompletionRequest::builder()\n            .model(\"test\")\n            .temperature(2.0)  // Maximum valid temperature\n            .build();\n        assert_eq!(request.temperature, Some(2.0));\n\n        // Test with very precise temperature\n        let request = CompletionRequest::builder()\n            .model(\"test\")\n            .temperature(0.7123456789)\n            .build();\n        assert_eq!(request.temperature, Some(0.7123456789));\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","config","mod.rs"],"content":"use crate::error::{Error, Result};\nuse serde::{Deserialize, Serialize};\nuse std::env;\nuse std::fs;\nuse std::path::Path;\n\n#[cfg(test)]\nmod tests;\n\n/// OpenAI configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OpenAIConfig {\n    pub default_model: String,\n    pub api_base: String,\n    pub max_retries: u32,\n    pub timeout_seconds: u32,\n}\n\nimpl Default for OpenAIConfig {\n    fn default() -\u003e Self {\n        Self {\n            default_model: \"gpt-4\".to_string(),\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        }\n    }\n}\n\n/// Main configuration structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Config {\n    pub openai: OpenAIConfig,\n}\n\nimpl Default for Config {\n    fn default() -\u003e Self {\n        Self {\n            openai: OpenAIConfig::default(),\n        }\n    }\n}\n\nimpl Config {\n    /// Load configuration from file and environment variables\n    /// Environment variables take precedence over file values\n    pub fn load\u003cP: AsRef\u003cPath\u003e\u003e(config_path: Option\u003cP\u003e) -\u003e Result\u003cSelf\u003e {\n        let mut config = if let Some(path) = config_path {\n            Self::from_file(path)?\n        } else {\n            Self::default()\n        };\n\n        // Override with environment variables\n        let env_config = Self::from_env()?;\n        config.merge_env(env_config);\n\n        Ok(config)\n    }\n\n    /// Load configuration from a TOML file\n    pub fn from_file\u003cP: AsRef\u003cPath\u003e\u003e(path: P) -\u003e Result\u003cSelf\u003e {\n        let content = fs::read_to_string(path)?;\n        let config: Config = toml::from_str(\u0026content)?;\n        Ok(config)\n    }\n\n    /// Load configuration from environment variables\n    pub fn from_env() -\u003e Result\u003cSelf\u003e {\n        let mut config = Self::default();\n\n        // OpenAI configuration\n        if let Ok(model) = env::var(\"OPENAI_MODEL\") {\n            config.openai.default_model = model;\n        }\n\n        if let Ok(api_base) = env::var(\"OPENAI_API_BASE\") {\n            config.openai.api_base = api_base;\n        }\n\n        if let Ok(max_retries) = env::var(\"OPENAI_MAX_RETRIES\") {\n            config.openai.max_retries = max_retries\n                .parse()\n                .map_err(|e| Error::Config(format!(\"Invalid OPENAI_MAX_RETRIES: {}\", e)))?;\n        }\n\n        if let Ok(timeout) = env::var(\"OPENAI_TIMEOUT\") {\n            config.openai.timeout_seconds = timeout\n                .parse()\n                .map_err(|e| Error::Config(format!(\"Invalid OPENAI_TIMEOUT: {}\", e)))?;\n        }\n\n        Ok(config)\n    }\n\n    /// Merge environment configuration into this config\n    /// Environment values take precedence\n    fn merge_env(\u0026mut self, env_config: Config) {\n        // Only update values that were actually set in environment\n        if env::var(\"OPENAI_MODEL\").is_ok() {\n            self.openai.default_model = env_config.openai.default_model;\n        }\n        if env::var(\"OPENAI_API_BASE\").is_ok() {\n            self.openai.api_base = env_config.openai.api_base;\n        }\n        if env::var(\"OPENAI_MAX_RETRIES\").is_ok() {\n            self.openai.max_retries = env_config.openai.max_retries;\n        }\n        if env::var(\"OPENAI_TIMEOUT\").is_ok() {\n            self.openai.timeout_seconds = env_config.openai.timeout_seconds;\n        }\n    }\n\n    /// Save configuration to a TOML file\n    pub fn save\u003cP: AsRef\u003cPath\u003e\u003e(\u0026self, path: P) -\u003e Result\u003c()\u003e {\n        let content = toml::to_string_pretty(self)\n            .map_err(|e| Error::Config(format!(\"Failed to serialize config: {}\", e)))?;\n        fs::write(path, content)?;\n        Ok(())\n    }\n}","traces":[{"line":20,"address":[],"length":0,"stats":{"Line":24}},{"line":22,"address":[],"length":0,"stats":{"Line":72}},{"line":23,"address":[],"length":0,"stats":{"Line":24}},{"line":37,"address":[],"length":0,"stats":{"Line":23}},{"line":39,"address":[],"length":0,"stats":{"Line":23}},{"line":47,"address":[],"length":0,"stats":{"Line":3}},{"line":48,"address":[],"length":0,"stats":{"Line":8}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":1}},{"line":55,"address":[],"length":0,"stats":{"Line":3}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":4}},{"line":63,"address":[],"length":0,"stats":{"Line":12}},{"line":64,"address":[],"length":0,"stats":{"Line":3}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":5}},{"line":70,"address":[],"length":0,"stats":{"Line":10}},{"line":73,"address":[],"length":0,"stats":{"Line":8}},{"line":77,"address":[],"length":0,"stats":{"Line":6}},{"line":81,"address":[],"length":0,"stats":{"Line":7}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":6}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":5}},{"line":98,"address":[],"length":0,"stats":{"Line":3}},{"line":100,"address":[],"length":0,"stats":{"Line":7}},{"line":101,"address":[],"length":0,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":6}},{"line":104,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":7}},{"line":107,"address":[],"length":0,"stats":{"Line":1}},{"line":109,"address":[],"length":0,"stats":{"Line":6}},{"line":110,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}}],"covered":26,"coverable":39},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","config","tests.rs"],"content":"use super::*;\nuse std::env;\nuse tempfile::NamedTempFile;\nuse std::io::Write;\n\n#[test]\nfn test_config_defaults() {\n    let config = Config::default();\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 3);\n    assert_eq!(config.openai.timeout_seconds, 30);\n}\n\n#[test]\nfn test_openai_config_defaults() {\n    let config = OpenAIConfig::default();\n    assert_eq!(config.default_model, \"gpt-4\");\n    assert_eq!(config.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.max_retries, 3);\n    assert_eq!(config.timeout_seconds, 30);\n}\n\n#[test]\nfn test_config_from_toml() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-3.5-turbo\"\napi_base = \"https://custom.openai.com/v1\"\nmax_retries = 5\ntimeout_seconds = 60\n\"#;\n\n    let config: Config = toml::from_str(toml_content).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-3.5-turbo\");\n    assert_eq!(config.openai.api_base, \"https://custom.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 5);\n    assert_eq!(config.openai.timeout_seconds, 60);\n}\n\n#[test]\nfn test_config_from_file() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4-turbo\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 2\ntimeout_seconds = 45\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    let config = Config::from_file(temp_file.path()).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4-turbo\");\n    assert_eq!(config.openai.max_retries, 2);\n    assert_eq!(config.openai.timeout_seconds, 45);\n}\n\n#[test]\nfn test_config_from_file_not_found() {\n    let result = Config::from_file(\"non_existent_file.toml\");\n    assert!(result.is_err());\n    match result {\n        Err(Error::Io(_)) =\u003e {}\n        _ =\u003e panic!(\"Expected IO error\"),\n    }\n}\n\n#[test]\nfn test_config_from_env() {\n    // Set environment variables\n    env::set_var(\"OPENAI_MODEL\", \"gpt-4-vision\");\n    env::set_var(\"OPENAI_API_BASE\", \"https://custom-api.com/v1\");\n    env::set_var(\"OPENAI_MAX_RETRIES\", \"7\");\n    env::set_var(\"OPENAI_TIMEOUT\", \"90\");\n\n    let config = Config::from_env().unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4-vision\");\n    assert_eq!(config.openai.api_base, \"https://custom-api.com/v1\");\n    assert_eq!(config.openai.max_retries, 7);\n    assert_eq!(config.openai.timeout_seconds, 90);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n}\n\n#[test]\nfn test_config_from_env_partial() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Only set some environment variables\n    env::set_var(\"OPENAI_MODEL\", \"gpt-3.5-turbo-16k\");\n\n    let config = Config::from_env().unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-3.5-turbo-16k\");\n    // Should use defaults for other values\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 3);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n}\n\n#[test]\nfn test_config_load_priority() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Test that environment variables override file values\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 3\ntimeout_seconds = 30\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    // Set environment variable\n    env::set_var(\"OPENAI_MODEL\", \"gpt-4-turbo\");\n\n    let config = Config::load(Some(temp_file.path())).unwrap();\n    // Environment variable should override file value\n    assert_eq!(config.openai.default_model, \"gpt-4-turbo\");\n    // File value should be used for non-overridden values\n    assert_eq!(config.openai.max_retries, 3);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n}\n\n#[test]\nfn test_config_load_file_only() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 4\ntimeout_seconds = 25\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    let config = Config::load(Some(temp_file.path())).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.max_retries, 4);\n    assert_eq!(config.openai.timeout_seconds, 25);\n}\n\n#[test]\nfn test_config_load_no_file() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Load with no file specified - should use defaults + env\n    env::set_var(\"OPENAI_MAX_RETRIES\", \"10\");\n\n    let config = Config::load::\u003c\u0026str\u003e(None).unwrap();\n    // Should use default for most values\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    // But use env var where set\n    assert_eq!(config.openai.max_retries, 10);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n}\n\n#[test]\nfn test_invalid_toml() {\n    let invalid_toml = r#\"\n[openai\ndefault_model = \"gpt-4\"\n\"#;\n\n    let result: std::result::Result\u003cConfig, toml::de::Error\u003e = toml::from_str(invalid_toml);\n    assert!(result.is_err());\n}\n\n#[test]\nfn test_config_serialization() {\n    let config = Config {\n        openai: OpenAIConfig {\n            default_model: \"gpt-4\".to_string(),\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        },\n    };\n\n    let toml_str = toml::to_string(\u0026config).unwrap();\n    assert!(toml_str.contains(\"default_model = \\\"gpt-4\\\"\"));\n    assert!(toml_str.contains(\"max_retries = 3\"));\n\n    // Round trip test\n    let parsed: Config = toml::from_str(\u0026toml_str).unwrap();\n    assert_eq!(parsed.openai.default_model, config.openai.default_model);\n    assert_eq!(parsed.openai.max_retries, config.openai.max_retries);\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","error.rs"],"content":"use std::fmt;\n\n/// Custom error type for the application\n#[derive(Debug)]\npub enum Error {\n    /// Configuration errors\n    Config(String),\n    /// Provider errors (API calls, network, etc.)\n    Provider(String),\n    /// Service container errors\n    Service(String),\n    /// IO errors\n    Io(std::io::Error),\n    /// Other errors\n    Other(String),\n}\n\nimpl fmt::Display for Error {\n    fn fmt(\u0026self, f: \u0026mut fmt::Formatter\u003c'_\u003e) -\u003e fmt::Result {\n        match self {\n            Error::Config(msg) =\u003e write!(f, \"Configuration error: {}\", msg),\n            Error::Provider(msg) =\u003e write!(f, \"Provider error: {}\", msg),\n            Error::Service(msg) =\u003e write!(f, \"Service error: {}\", msg),\n            Error::Io(err) =\u003e write!(f, \"IO error: {}\", err),\n            Error::Other(msg) =\u003e write!(f, \"Error: {}\", msg),\n        }\n    }\n}\n\nimpl std::error::Error for Error {\n    fn source(\u0026self) -\u003e Option\u003c\u0026(dyn std::error::Error + 'static)\u003e {\n        match self {\n            Error::Io(err) =\u003e Some(err),\n            _ =\u003e None,\n        }\n    }\n}\n\nimpl From\u003cstd::io::Error\u003e for Error {\n    fn from(err: std::io::Error) -\u003e Self {\n        Error::Io(err)\n    }\n}\n\nimpl From\u003ctoml::de::Error\u003e for Error {\n    fn from(err: toml::de::Error) -\u003e Self {\n        Error::Config(format!(\"TOML parsing error: {}\", err))\n    }\n}\n\nimpl From\u003cstd::env::VarError\u003e for Error {\n    fn from(err: std::env::VarError) -\u003e Self {\n        Error::Config(format!(\"Environment variable error: {}\", err))\n    }\n}\n\n/// Result type alias\npub type Result\u003cT\u003e = std::result::Result\u003cT, Error\u003e;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::error::Error as StdError;\n\n    #[test]\n    fn test_error_display() {\n        let err = Error::Config(\"Invalid API key\".to_string());\n        assert_eq!(err.to_string(), \"Configuration error: Invalid API key\");\n\n        let err = Error::Provider(\"API rate limit exceeded\".to_string());\n        assert_eq!(err.to_string(), \"Provider error: API rate limit exceeded\");\n\n        let err = Error::Service(\"Service not found\".to_string());\n        assert_eq!(err.to_string(), \"Service error: Service not found\");\n\n        let err = Error::Other(\"Unknown error\".to_string());\n        assert_eq!(err.to_string(), \"Error: Unknown error\");\n    }\n\n    #[test]\n    fn test_error_from_io() {\n        let io_err = std::io::Error::new(std::io::ErrorKind::NotFound, \"File not found\");\n        let err: Error = io_err.into();\n        assert!(matches!(err, Error::Io(_)));\n    }\n\n    #[test]\n    fn test_error_source() {\n        let io_err = std::io::Error::new(std::io::ErrorKind::PermissionDenied, \"Access denied\");\n        let err = Error::Io(io_err);\n        assert!(StdError::source(\u0026err).is_some());\n\n        let err = Error::Config(\"Bad config\".to_string());\n        assert!(StdError::source(\u0026err).is_none());\n    }\n\n    #[test]\n    fn test_error_from_env_var() {\n        let env_err = std::env::VarError::NotPresent;\n        let err: Error = env_err.into();\n        match err {\n            Error::Config(msg) =\u003e assert!(msg.contains(\"Environment variable error\")),\n            _ =\u003e panic!(\"Expected Config error\"),\n        }\n    }\n}","traces":[{"line":19,"address":[],"length":0,"stats":{"Line":4}},{"line":20,"address":[],"length":0,"stats":{"Line":4}},{"line":21,"address":[],"length":0,"stats":{"Line":1}},{"line":22,"address":[],"length":0,"stats":{"Line":4}},{"line":23,"address":[],"length":0,"stats":{"Line":4}},{"line":24,"address":[],"length":0,"stats":{"Line":0}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":31,"address":[],"length":0,"stats":{"Line":2}},{"line":32,"address":[],"length":0,"stats":{"Line":2}},{"line":33,"address":[],"length":0,"stats":{"Line":1}},{"line":34,"address":[],"length":0,"stats":{"Line":1}},{"line":40,"address":[],"length":0,"stats":{"Line":2}},{"line":41,"address":[],"length":0,"stats":{"Line":2}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":1}},{"line":53,"address":[],"length":0,"stats":{"Line":1}}],"covered":14,"coverable":17},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","lib.rs"],"content":"pub mod config;\npub mod config;\npub mod error;\npub mod provider;\npub mod service;\n\n#[cfg(test)]\nmod comprehensive_tests;\n\n#[cfg(test)]\nmod property_tests;\n\n\nuse config::Config;\nuse error::Result;\nuse provider::{CompletionRequest, Message};\nuse service::ServiceContainer;\nuse std::sync::OnceLock;\n\nstatic SERVICE_CONTAINER: OnceLock\u003cServiceContainer\u003e = OnceLock::new();\n\n/// Initialize the global service container\npub fn init(config: Config) -\u003e Result\u003c()\u003e {\n    let container = ServiceContainer::new(config)?;\n    SERVICE_CONTAINER\n        .set(container)\n        .map_err(|_| error::Error::Service(\"Service container already initialized\".into()))?;\n    Ok(())\n}\n\n/// Get the global service container\npub fn get_service_container() -\u003e Result\u003c\u0026'static ServiceContainer\u003e {\n    SERVICE_CONTAINER\n        .get()\n        .ok_or_else(|| error::Error::Service(\"Service container not initialized\".into()))\n}\n\n/// Backward compatible ask function\npub async fn ask(prompt: \u0026str) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: container.config().openai.default_model.clone(),\n        messages: vec![Message {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }],\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n/// Ask with a specific model\npub async fn ask_with_model(prompt: \u0026str, model: \u0026str) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: model.to_string(),\n        messages: vec![Message {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }],\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n/// Ask with messages (conversation context)\npub async fn ask_with_messages(messages: Vec\u003cMessage\u003e) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: container.config().openai.default_model.clone(),\n        messages,\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::provider::tests::MockProvider;\n    use std::sync::Arc;\n\n    fn setup_test_container() -\u003e ServiceContainer {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n        \n        let mock_provider = Arc::new(MockProvider {\n            response: \"Test response from global\".to_string(),\n            should_fail: false,\n        });\n        \n        container.register_provider(\"mock\", mock_provider);\n        container\n    }\n\n    #[test]\n    fn test_init_and_get_container() {\n        // Reset for test\n        let config = Config::default();\n        \n        // This might fail if already initialized, but that's okay for tests\n        let _ = init(config);\n        \n        // Should be able to get the container\n        let result = get_service_container();\n        // In a real test environment, this might be initialized already\n        // so we just check it doesn't panic\n        assert!(result.is_ok() || result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_ask_backward_compatibility() {\n        // For this test, we'll test the ask function logic without global state\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_model_logic() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Test with specific model\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_messages_logic() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let messages = vec![\n            Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            },\n            Message {\n                role: \"assistant\".to_string(),\n                content: \"Hi there!\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"How are you?\".to_string(),\n            },\n        ];\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages,\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[test]\n    fn test_service_not_initialized() {\n        // Clear any existing container (this is a limitation of using OnceLock in tests)\n        // In practice, we'd use a different pattern for testability\n        \n        // This test verifies the error when service is not initialized\n        // The actual behavior depends on whether init() was called previously\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_persona_default() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are a helpful assistant.\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Hello\".to_string(),\n                },\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_persona_expert() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are an expert software developer with deep knowledge of programming languages, best practices, and system design.\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Test expert persona\".to_string(),\n                },\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_persona_custom() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are a helpful assistant with the personality of a custom expert.\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Test custom persona\".to_string(),\n                },\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n}","traces":[{"line":16,"address":[],"length":0,"stats":{"Line":1}},{"line":17,"address":[],"length":0,"stats":{"Line":3}},{"line":20,"address":[],"length":0,"stats":{"Line":0}},{"line":21,"address":[],"length":0,"stats":{"Line":1}},{"line":25,"address":[],"length":0,"stats":{"Line":1}},{"line":26,"address":[],"length":0,"stats":{"Line":1}},{"line":28,"address":[],"length":0,"stats":{"Line":1}},{"line":32,"address":[],"length":0,"stats":{"Line":0}},{"line":33,"address":[],"length":0,"stats":{"Line":0}},{"line":34,"address":[],"length":0,"stats":{"Line":0}},{"line":37,"address":[],"length":0,"stats":{"Line":0}},{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":42,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":74,"address":[],"length":0,"stats":{"Line":0}},{"line":77,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}}],"covered":6,"coverable":33},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","property_tests.rs"],"content":"/// Property-based and mutation testing for 100% coverage\n/// \n/// This module uses proptest to generate random inputs and test invariants\n/// across all functions to ensure robustness and edge case coverage.\n\n#[cfg(test)]\nmod property_tests {\n    use crate::*;\n    use crate::config::{Config, OpenAIConfig};\n    use crate::error::Error;\n    use crate::provider::*;\n    use crate::service::ServiceContainer;\n    use proptest::prelude::*;\n    use std::sync::Arc;\n    use tempfile::NamedTempFile;\n    use std::io::Write;\n\n    // Property test strategies\n    prop_compose! {\n        fn arb_openai_config()(\n            default_model in \"[a-zA-Z0-9-_]{1,50}\",\n            api_base in \"https?://[a-zA-Z0-9.-]+/[a-zA-Z0-9/_-]*\",\n            max_retries in 0u32..100,\n            timeout_seconds in 0u32..3600,\n        ) -\u003e OpenAIConfig {\n            OpenAIConfig {\n                default_model,\n                api_base,\n                max_retries,\n                timeout_seconds,\n            }\n        }\n    }\n\n    prop_compose! {\n        fn arb_config()(openai in arb_openai_config()) -\u003e Config {\n            Config { openai }\n        }\n    }\n\n    prop_compose! {\n        fn arb_message()(\n            role in \"user|assistant|system\",\n            content in \".*\",\n        ) -\u003e Message {\n            Message { role, content }\n        }\n    }\n\n    prop_compose! {\n        fn arb_completion_request()(\n            model in \"[a-zA-Z0-9-_]{1,50}\",\n            messages in prop::collection::vec(arb_message(), 1..10),\n            temperature in prop::option::of(0.0f32..2.0),\n            max_tokens in prop::option::of(1u32..4096),\n            stream in any::\u003cbool\u003e(),\n        ) -\u003e CompletionRequest {\n            CompletionRequest {\n                model,\n                messages,\n                temperature,\n                max_tokens,\n                stream,\n            }\n        }\n    }\n\n    proptest! {\n        #[test]\n        fn prop_config_serialization_roundtrip(config in arb_config()) {\n            let serialized = toml::to_string(\u0026config).unwrap();\n            let deserialized: Config = toml::from_str(\u0026serialized).unwrap();\n            \n            prop_assert_eq!(config.openai.default_model, deserialized.openai.default_model);\n            prop_assert_eq!(config.openai.api_base, deserialized.openai.api_base);\n            prop_assert_eq!(config.openai.max_retries, deserialized.openai.max_retries);\n            prop_assert_eq!(config.openai.timeout_seconds, deserialized.openai.timeout_seconds);\n        }\n\n        #[test]\n        fn prop_config_save_load_roundtrip(config in arb_config()) {\n            let mut temp_file = NamedTempFile::new().unwrap();\n            \n            // Save config\n            config.save(temp_file.path()).unwrap();\n            \n            // Load config\n            let loaded_config = Config::from_file(temp_file.path()).unwrap();\n            \n            prop_assert_eq!(config.openai.default_model, loaded_config.openai.default_model);\n            prop_assert_eq!(config.openai.api_base, loaded_config.openai.api_base);\n            prop_assert_eq!(config.openai.max_retries, loaded_config.openai.max_retries);\n            prop_assert_eq!(config.openai.timeout_seconds, loaded_config.openai.timeout_seconds);\n        }\n\n        #[test]\n        fn prop_message_creation(\n            role in \".*\",\n            content in \".*\",\n        ) {\n            let message = Message { role: role.clone(), content: content.clone() };\n            prop_assert_eq!(message.role, role);\n            prop_assert_eq!(message.content, content);\n        }\n\n        #[test]\n        fn prop_completion_request_builder(\n            model in \".*\",\n            temperature in prop::option::of(-10.0f32..10.0),\n            max_tokens in prop::option::of(0u32..1000000),\n            stream in any::\u003cbool\u003e(),\n        ) {\n            let request = CompletionRequest::builder()\n                .model(model.clone())\n                .temperature(temperature.unwrap_or(0.7))\n                .max_tokens(max_tokens.unwrap_or(1000))\n                .stream(stream)\n                .build();\n\n            prop_assert_eq!(request.model, model);\n            prop_assert_eq!(request.temperature, Some(temperature.unwrap_or(0.7)));\n            prop_assert_eq!(request.max_tokens, Some(max_tokens.unwrap_or(1000)));\n            prop_assert_eq!(request.stream, stream);\n        }\n\n        #[test]\n        fn prop_service_container_creation(config in arb_config()) {\n            let result = ServiceContainer::new(config);\n            prop_assert!(result.is_ok());\n        }\n\n        #[test]\n        fn prop_provider_registration(\n            config in arb_config(),\n            provider_name in \"[a-zA-Z0-9_-]{1,20}\",\n        ) {\n            let mut container = ServiceContainer::new(config).unwrap();\n            let mock_provider = Arc::new(crate::provider::tests::MockProvider {\n                response: \"Property test response\".to_string(),\n                should_fail: false,\n            });\n\n            container.register_provider(\u0026provider_name, mock_provider);\n            let retrieved = container.get_provider(\u0026provider_name);\n            prop_assert!(retrieved.is_ok());\n        }\n\n        #[test]\n        fn prop_usage_creation(\n            prompt_tokens in 0u32..1000000,\n            completion_tokens in 0u32..1000000,\n            total_tokens in 0u32..1000000,\n        ) {\n            let usage = Usage {\n                prompt_tokens,\n                completion_tokens,\n                total_tokens,\n            };\n            \n            prop_assert_eq!(usage.prompt_tokens, prompt_tokens);\n            prop_assert_eq!(usage.completion_tokens, completion_tokens);\n            prop_assert_eq!(usage.total_tokens, total_tokens);\n        }\n\n        #[test]\n        fn prop_completion_response_creation(\n            content in \".*\",\n            model in \".*\",\n            prompt_tokens in 0u32..1000000,\n            completion_tokens in 0u32..1000000,\n            total_tokens in 0u32..1000000,\n        ) {\n            let response = CompletionResponse {\n                content: content.clone(),\n                model: model.clone(),\n                usage: Usage {\n                    prompt_tokens,\n                    completion_tokens,\n                    total_tokens,\n                },\n            };\n\n            prop_assert_eq!(response.content, content);\n            prop_assert_eq!(response.model, model);\n            prop_assert_eq!(response.usage.prompt_tokens, prompt_tokens);\n        }\n\n        #[test]\n        fn prop_stream_chunk_creation(\n            delta in \".*\",\n            finish_reason in prop::option::of(\".*\"),\n        ) {\n            let chunk = StreamChunk {\n                delta: delta.clone(),\n                finish_reason: finish_reason.clone(),\n            };\n\n            prop_assert_eq!(chunk.delta, delta);\n            prop_assert_eq!(chunk.finish_reason, finish_reason);\n        }\n\n        #[test]\n        fn prop_error_display(error_msg in \".*\") {\n            let config_error = Error::Config(error_msg.clone());\n            let display_msg = format!(\"{}\", config_error);\n            prop_assert!(display_msg.contains(\u0026error_msg));\n\n            let provider_error = Error::Provider(error_msg.clone());\n            let display_msg = format!(\"{}\", provider_error);\n            prop_assert!(display_msg.contains(\u0026error_msg));\n\n            let service_error = Error::Service(error_msg.clone());\n            let display_msg = format!(\"{}\", service_error);\n            prop_assert!(display_msg.contains(\u0026error_msg));\n        }\n    }\n\n    // Mutation testing: test that our tests actually catch bugs\n    #[cfg(test)]\n    mod mutation_tests {\n        use super::*;\n\n        #[test]\n        fn test_config_invariants() {\n            // Test that certain invariants hold for any config\n            let config = Config::default();\n            \n            // Model name should not be empty for default config\n            assert!(!config.openai.default_model.is_empty());\n            \n            // API base should be a valid URL structure\n            assert!(config.openai.api_base.starts_with(\"http\"));\n            \n            // Timeouts should be reasonable\n            assert!(config.openai.timeout_seconds \u003e 0);\n            assert!(config.openai.timeout_seconds \u003c 86400); // Less than a day\n        }\n\n        #[test]\n        fn test_message_invariants() {\n            // Test message invariants\n            let message = Message {\n                role: \"user\".to_string(),\n                content: \"test\".to_string(),\n            };\n\n            // Role and content should be preserved exactly\n            assert_eq!(message.role, \"user\");\n            assert_eq!(message.content, \"test\");\n\n            // Message should handle empty strings\n            let empty_message = Message {\n                role: \"\".to_string(),\n                content: \"\".to_string(),\n            };\n            assert_eq!(empty_message.role.len(), 0);\n            assert_eq!(empty_message.content.len(), 0);\n        }\n\n        #[test]\n        fn test_completion_request_builder_invariants() {\n            // Test that builder always produces valid requests\n            let request = CompletionRequest::builder()\n                .model(\"test-model\")\n                .build();\n\n            assert_eq!(request.model, \"test-model\");\n            assert!(request.messages.is_empty()); // Default should be empty\n            assert_eq!(request.temperature, None); // Default should be None\n            assert_eq!(request.max_tokens, None); // Default should be None\n            assert!(!request.stream); // Default should be false\n        }\n\n        #[test]\n        fn test_error_handling_invariants() {\n            // Test that errors maintain their message content\n            let original_msg = \"Test error message\";\n            \n            let config_error = Error::Config(original_msg.to_string());\n            let displayed = format!(\"{}\", config_error);\n            assert!(displayed.contains(original_msg));\n\n            let provider_error = Error::Provider(original_msg.to_string());\n            let displayed = format!(\"{}\", provider_error);\n            assert!(displayed.contains(original_msg));\n\n            let service_error = Error::Service(original_msg.to_string());\n            let displayed = format!(\"{}\", service_error);\n            assert!(displayed.contains(original_msg));\n        }\n\n        #[tokio::test]\n        async fn test_mock_provider_invariants() {\n            // Test that mock provider behaves consistently\n            let mock = crate::provider::tests::MockProvider {\n                response: \"test response\".to_string(),\n                should_fail: false,\n            };\n\n            let request = CompletionRequest::builder()\n                .model(\"test\")\n                .build();\n\n            let response = mock.complete(request).await.unwrap();\n            assert_eq!(response.content, \"test response\");\n            assert_eq!(mock.name(), \"mock\");\n        }\n\n        #[test]\n        fn test_service_container_invariants() {\n            // Test service container invariants\n            let config = Config::default();\n            let container = ServiceContainer::new(config).unwrap();\n\n            // Container should start with consistent state\n            let providers = container.list_providers();\n            let config_ref = container.config();\n            \n            // Config should be accessible\n            assert!(!config_ref.openai.default_model.is_empty());\n\n            // Getting non-existent provider should fail consistently\n            let result = container.get_provider(\"nonexistent\");\n            assert!(result.is_err());\n        }\n    }\n\n    // Stress tests to find edge cases\n    #[cfg(test)]\n    mod stress_tests {\n        use super::*;\n        use std::thread;\n        use std::time::Duration;\n\n        #[test]\n        fn test_large_config_files() {\n            // Test with very large configuration values\n            let large_model_name = \"a\".repeat(10000);\n            let large_api_base = format!(\"https://{}example.com/v1\", \"a\".repeat(1000));\n\n            let config = Config {\n                openai: OpenAIConfig {\n                    default_model: large_model_name.clone(),\n                    api_base: large_api_base.clone(),\n                    max_retries: u32::MAX,\n                    timeout_seconds: u32::MAX,\n                },\n            };\n\n            // Should handle serialization\n            let serialized = toml::to_string(\u0026config).unwrap();\n            assert!(serialized.contains(\u0026large_model_name));\n\n            // Should handle file operations\n            let temp_file = NamedTempFile::new().unwrap();\n            config.save(temp_file.path()).unwrap();\n            let loaded = Config::from_file(temp_file.path()).unwrap();\n            assert_eq!(loaded.openai.default_model, large_model_name);\n        }\n\n        #[test]\n        fn test_unicode_edge_cases() {\n            // Test with various Unicode edge cases\n            let unicode_strings = vec![\n                \"🚀🎉🔥\", // Emojis\n                \"∑∫∆∇\", // Mathematical symbols\n                \"αβγδεζηθικλμνξοπρστυφχψω\", // Greek alphabet\n                \"中文测试\", // Chinese characters\n                \"العربية\", // Arabic text\n                \"עברית\", // Hebrew text\n                \"🏳️‍🌈🏳️‍⚧️\", // Flag emojis with zero-width joiners\n                \"\\u{200B}\\u{200C}\\u{200D}\", // Zero-width characters\n                \"a\\u{0301}\", // Combining diacritical marks\n                \"\\u{1F1FA}\\u{1F1F8}\", // Regional indicator symbols (US flag)\n            ];\n\n            for unicode_str in unicode_strings {\n                let message = Message {\n                    role: \"user\".to_string(),\n                    content: unicode_str.to_string(),\n                };\n                assert_eq!(message.content, unicode_str);\n\n                let config = Config {\n                    openai: OpenAIConfig {\n                        default_model: unicode_str.to_string(),\n                        api_base: \"https://api.openai.com/v1\".to_string(),\n                        max_retries: 3,\n                        timeout_seconds: 30,\n                    },\n                };\n\n                // Should handle serialization without panicking\n                let result = toml::to_string(\u0026config);\n                // Some Unicode might not be valid in TOML, so we just check it doesn't panic\n                let _ = result;\n            }\n        }\n\n        #[test]\n        fn test_concurrent_access() {\n            // Test concurrent access to service container\n            let config = Config::default();\n            let container = Arc::new(ServiceContainer::new(config).unwrap());\n\n            let handles: Vec\u003c_\u003e = (0..10)\n                .map(|i| {\n                    let container = Arc::clone(\u0026container);\n                    thread::spawn(move || {\n                        for _ in 0..100 {\n                            let providers = container.list_providers();\n                            let config = container.config();\n                            assert!(!config.openai.default_model.is_empty());\n                            \n                            // Try to get a provider (might fail, but shouldn't panic)\n                            let _ = container.get_provider(\u0026format!(\"test-{}\", i));\n                            \n                            thread::sleep(Duration::from_nanos(1));\n                        }\n                    })\n                })\n                .collect();\n\n            for handle in handles {\n                handle.join().unwrap();\n            }\n        }\n\n        #[test]\n        fn test_memory_pressure() {\n            // Test behavior under memory pressure\n            let mut large_messages = Vec::new();\n            \n            // Create many large messages\n            for i in 0..1000 {\n                let large_content = format!(\"{}: {}\", i, \"x\".repeat(10000));\n                large_messages.push(Message {\n                    role: \"user\".to_string(),\n                    content: large_content,\n                });\n            }\n\n            let request = CompletionRequest {\n                model: \"test-model\".to_string(),\n                messages: large_messages,\n                temperature: Some(0.7),\n                max_tokens: Some(1000),\n                stream: false,\n            };\n\n            // Should handle large requests without panicking\n            assert_eq!(request.messages.len(), 1000);\n            assert_eq!(request.messages[0].content.len(), 10007); // \"0: \" + 10000 x's\n        }\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","provider","mod.rs"],"content":"use crate::error::{Error, Result};\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse serde::{Deserialize, Serialize};\n\n#[cfg(test)]\npub mod tests;\n\n/// Message in a conversation\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Message {\n    pub role: String,\n    pub content: String,\n}\n\n/// Request for LLM completion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompletionRequest {\n    pub model: String,\n    pub messages: Vec\u003cMessage\u003e,\n    pub temperature: Option\u003cf32\u003e,\n    pub max_tokens: Option\u003cu32\u003e,\n    pub stream: bool,\n}\n\n/// Response from LLM completion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompletionResponse {\n    pub content: String,\n    pub model: String,\n    pub usage: Usage,\n}\n\n/// Token usage information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Usage {\n    pub prompt_tokens: u32,\n    pub completion_tokens: u32,\n    pub total_tokens: u32,\n}\n\n/// Streaming chunk from LLM\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct StreamChunk {\n    pub delta: String,\n    pub finish_reason: Option\u003cString\u003e,\n}\n\n/// Trait for LLM providers\n#[async_trait]\npub trait LLMProvider: Send + Sync {\n    /// Get the name of the provider\n    fn name(\u0026self) -\u003e \u0026str;\n\n    /// Complete a request and return the full response\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e;\n\n    /// Stream a response\n    async fn stream(\n        \u0026self,\n        request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e;\n}\n\npub mod openai;\n\npub use openai::OpenAIProvider;","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","provider","openai.rs"],"content":"use super::*;\nuse crate::config::OpenAIConfig;\nuse async_openai::{\n    types::{\n        ChatCompletionRequestMessage, ChatCompletionRequestSystemMessageArgs,\n        ChatCompletionRequestUserMessageArgs, ChatCompletionRequestAssistantMessageArgs,\n        CreateChatCompletionRequestArgs, CreateChatCompletionStreamResponse,\n    },\n    Client,\n};\nuse futures::StreamExt;\n\n/// OpenAI provider implementation\npub struct OpenAIProvider {\n    client: Client\u003casync_openai::config::OpenAIConfig\u003e,\n    config: OpenAIConfig,\n}\n\nimpl OpenAIProvider {\n    /// Create a new OpenAI provider\n    pub fn new(api_key: String, config: OpenAIConfig) -\u003e Self {\n        let openai_config = async_openai::config::OpenAIConfig::new()\n            .with_api_key(api_key)\n            .with_api_base(config.api_base.clone());\n\n        Self {\n            client: Client::with_config(openai_config),\n            config,\n        }\n    }\n\n    fn convert_messages(\u0026self, messages: Vec\u003cMessage\u003e) -\u003e Vec\u003cChatCompletionRequestMessage\u003e {\n        messages\n            .into_iter()\n            .map(|msg| match msg.role.as_str() {\n                \"system\" =\u003e ChatCompletionRequestSystemMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n                \"assistant\" =\u003e ChatCompletionRequestAssistantMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n                _ =\u003e ChatCompletionRequestUserMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n            })\n            .collect()\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for OpenAIProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"openai\"\n    }\n\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e {\n        let mut builder = CreateChatCompletionRequestArgs::default();\n        builder\n            .model(\u0026request.model)\n            .messages(self.convert_messages(request.messages));\n\n        if let Some(temp) = request.temperature {\n            builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = request.max_tokens {\n            builder.max_tokens(max_tokens as u16);\n        }\n\n        let openai_request = builder\n            .build()\n            .map_err(|e| Error::Provider(format!(\"Failed to build request: {}\", e)))?;\n\n        let response = self\n            .client\n            .chat()\n            .create(openai_request)\n            .await\n            .map_err(|e| Error::Provider(format!(\"OpenAI API error: {}\", e)))?;\n\n        let content = response\n            .choices\n            .first()\n            .and_then(|c| c.message.content.as_ref())\n            .ok_or_else(|| Error::Provider(\"No content in response\".into()))?\n            .clone();\n\n        Ok(CompletionResponse {\n            content,\n            model: response.model,\n            usage: Usage {\n                prompt_tokens: response.usage.as_ref().map(|u| u.prompt_tokens).unwrap_or(0) as u32,\n                completion_tokens: response\n                    .usage\n                    .as_ref()\n                    .map(|u| u.completion_tokens)\n                    .unwrap_or(0) as u32,\n                total_tokens: response.usage.as_ref().map(|u| u.total_tokens).unwrap_or(0) as u32,\n            },\n        })\n    }\n\n    async fn stream(\n        \u0026self,\n        request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e {\n        let mut builder = CreateChatCompletionRequestArgs::default();\n        builder\n            .model(\u0026request.model)\n            .messages(self.convert_messages(request.messages))\n            .stream(true);\n\n        if let Some(temp) = request.temperature {\n            builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = request.max_tokens {\n            builder.max_tokens(max_tokens as u16);\n        }\n\n        let openai_request = builder\n            .build()\n            .map_err(|e| Error::Provider(format!(\"Failed to build request: {}\", e)))?;\n\n        let stream = self\n            .client\n            .chat()\n            .create_stream(openai_request)\n            .await\n            .map_err(|e| Error::Provider(format!(\"OpenAI API error: {}\", e)))?;\n\n        let mapped_stream = stream.map(|result| match result {\n            Ok(response) =\u003e {\n                let chunk = extract_chunk(response);\n                Ok(chunk)\n            }\n            Err(e) =\u003e Err(Error::Provider(format!(\"Stream error: {}\", e))),\n        });\n\n        Ok(Box::pin(mapped_stream))\n    }\n}\n\nfn extract_chunk(response: CreateChatCompletionStreamResponse) -\u003e StreamChunk {\n    let delta = response\n        .choices\n        .first()\n        .and_then(|c| c.delta.content.as_ref())\n        .map(|s| s.clone())\n        .unwrap_or_default();\n\n    let finish_reason = response\n        .choices\n        .first()\n        .and_then(|c| c.finish_reason.as_ref())\n        .map(|r| format!(\"{:?}\", r));\n\n    StreamChunk {\n        delta,\n        finish_reason,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_openai_provider_creation() {\n        let config = OpenAIConfig {\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            default_model: \"gpt-4\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n\n        let provider = OpenAIProvider::new(\"test-key\".to_string(), config.clone());\n        assert_eq!(provider.name(), \"openai\");\n        assert_eq!(provider.config.default_model, \"gpt-4\");\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let config = OpenAIConfig {\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            default_model: \"gpt-4\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n\n        let provider = OpenAIProvider::new(\"test-key\".to_string(), config);\n\n        let messages = vec![\n            Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            },\n            Message {\n                role: \"assistant\".to_string(),\n                content: \"Hi there!\".to_string(),\n            },\n        ];\n\n        let converted = provider.convert_messages(messages);\n        assert_eq!(converted.len(), 3);\n    }\n\n    #[test]\n    fn test_extract_chunk() {\n        // This would require mocking CreateChatCompletionStreamResponse\n        // which is complex due to the async-openai types\n        // For now, we'll focus on the integration tests\n    }\n}","traces":[{"line":21,"address":[],"length":0,"stats":{"Line":18}},{"line":22,"address":[],"length":0,"stats":{"Line":36}},{"line":23,"address":[],"length":0,"stats":{"Line":36}},{"line":24,"address":[],"length":0,"stats":{"Line":54}},{"line":27,"address":[],"length":0,"stats":{"Line":36}},{"line":32,"address":[],"length":0,"stats":{"Line":1}},{"line":33,"address":[],"length":0,"stats":{"Line":1}},{"line":35,"address":[],"length":0,"stats":{"Line":4}},{"line":36,"address":[],"length":0,"stats":{"Line":5}},{"line":37,"address":[],"length":0,"stats":{"Line":1}},{"line":38,"address":[],"length":0,"stats":{"Line":1}},{"line":39,"address":[],"length":0,"stats":{"Line":1}},{"line":40,"address":[],"length":0,"stats":{"Line":1}},{"line":41,"address":[],"length":0,"stats":{"Line":4}},{"line":42,"address":[],"length":0,"stats":{"Line":1}},{"line":43,"address":[],"length":0,"stats":{"Line":1}},{"line":44,"address":[],"length":0,"stats":{"Line":1}},{"line":45,"address":[],"length":0,"stats":{"Line":1}},{"line":46,"address":[],"length":0,"stats":{"Line":1}},{"line":58,"address":[],"length":0,"stats":{"Line":1}},{"line":59,"address":[],"length":0,"stats":{"Line":1}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":0}},{"line":127,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}}],"covered":21,"coverable":57},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","provider","tests.rs"],"content":"use super::*;\nuse async_trait::async_trait;\nuse tokio_stream::StreamExt;\n\n#[derive(Debug, Clone)]\npub struct MockProvider {\n    pub response: String,\n    pub should_fail: bool,\n}\n\n#[async_trait]\nimpl LLMProvider for MockProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"mock\"\n    }\n\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e {\n        if self.should_fail {\n            return Err(Error::Provider(\"Mock provider error\".into()));\n        }\n\n        Ok(CompletionResponse {\n            content: self.response.clone(),\n            model: request.model,\n            usage: Usage {\n                prompt_tokens: 10,\n                completion_tokens: 20,\n                total_tokens: 30,\n            },\n        })\n    }\n\n    async fn stream(\n        \u0026self,\n        _request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e {\n        if self.should_fail {\n            return Err(Error::Provider(\"Mock provider error\".into()));\n        }\n\n        let chunks = vec![\n            StreamChunk {\n                delta: self.response.clone(),\n                finish_reason: None,\n            },\n            StreamChunk {\n                delta: String::new(),\n                finish_reason: Some(\"stop\".to_string()),\n            },\n        ];\n\n        Ok(Box::pin(tokio_stream::iter(chunks.into_iter().map(Ok))))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_mock_provider_complete() {\n        let provider = MockProvider {\n            response: \"Test response\".to_string(),\n            should_fail: false,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = provider.complete(request.clone()).await.unwrap();\n        assert_eq!(response.content, \"Test response\");\n        assert_eq!(response.model, \"gpt-4\");\n        assert_eq!(response.usage.total_tokens, 30);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_complete_error() {\n        let provider = MockProvider {\n            response: String::new(),\n            should_fail: true,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: None,\n            max_tokens: None,\n            stream: false,\n        };\n\n        let result = provider.complete(request).await;\n        assert!(result.is_err());\n        match result {\n            Err(Error::Provider(msg)) =\u003e assert_eq!(msg, \"Mock provider error\"),\n            _ =\u003e panic!(\"Expected Provider error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_stream() {\n        let provider = MockProvider {\n            response: \"Streaming response\".to_string(),\n            should_fail: false,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            }],\n            temperature: Some(0.5),\n            max_tokens: Some(200),\n            stream: true,\n        };\n\n        let mut stream = provider.stream(request).await.unwrap();\n        \n        let mut chunks = Vec::new();\n        while let Some(chunk) = stream.next().await {\n            chunks.push(chunk.unwrap());\n        }\n\n        assert_eq!(chunks.len(), 2);\n        assert_eq!(chunks[0].delta, \"Streaming response\");\n        assert_eq!(chunks[0].finish_reason, None);\n        assert_eq!(chunks[1].delta, \"\");\n        assert_eq!(chunks[1].finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[tokio::test]\n    async fn test_provider_trait_methods() {\n        let provider = MockProvider {\n            response: \"Test\".to_string(),\n            should_fail: false,\n        };\n\n        assert_eq!(provider.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_message_construction() {\n        let msg = Message {\n            role: \"assistant\".to_string(),\n            content: \"I can help with that\".to_string(),\n        };\n\n        assert_eq!(msg.role, \"assistant\");\n        assert_eq!(msg.content, \"I can help with that\");\n    }\n\n    #[test]\n    fn test_completion_request_builder() {\n        let request = CompletionRequest {\n            model: \"gpt-3.5-turbo\".to_string(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are a coding assistant\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Write a hello world program\".to_string(),\n                },\n            ],\n            temperature: Some(0.8),\n            max_tokens: Some(1000),\n            stream: true,\n        };\n\n        assert_eq!(request.model, \"gpt-3.5-turbo\");\n        assert_eq!(request.messages.len(), 2);\n        assert_eq!(request.temperature, Some(0.8));\n        assert_eq!(request.max_tokens, Some(1000));\n        assert!(request.stream);\n    }\n\n    #[test]\n    fn test_usage_calculation() {\n        let usage = Usage {\n            prompt_tokens: 50,\n            completion_tokens: 100,\n            total_tokens: 150,\n        };\n\n        assert_eq!(usage.prompt_tokens, 50);\n        assert_eq!(usage.completion_tokens, 100);\n        assert_eq!(usage.total_tokens, 150);\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","service.rs"],"content":"use crate::config::Config;\nuse crate::error::{Error, Result};\nuse crate::provider::{LLMProvider, OpenAIProvider};\nuse std::collections::HashMap;\nuse std::sync::Arc;\n\n/// Service container for dependency injection\npub struct ServiceContainer {\n    providers: HashMap\u003cString, Arc\u003cdyn LLMProvider\u003e\u003e,\n    config: Config,\n}\n\nimpl ServiceContainer {\n    /// Create a new service container\n    pub fn new(config: Config) -\u003e Result\u003cSelf\u003e {\n        let mut container = Self {\n            providers: HashMap::new(),\n            config,\n        };\n\n        // Register default providers\n        container.register_default_providers()?;\n\n        Ok(container)\n    }\n\n    /// Register default providers based on configuration\n    fn register_default_providers(\u0026mut self) -\u003e Result\u003c()\u003e {\n        // Register OpenAI provider if API key is available\n        if let Ok(api_key) = std::env::var(\"OPENAI_API_KEY\") {\n            let provider = OpenAIProvider::new(api_key, self.config.openai.clone());\n            self.register_provider(\"openai\", Arc::new(provider));\n        }\n\n        Ok(())\n    }\n\n    /// Register a provider with the container\n    pub fn register_provider(\u0026mut self, name: \u0026str, provider: Arc\u003cdyn LLMProvider\u003e) {\n        self.providers.insert(name.to_string(), provider);\n    }\n\n    /// Get a provider by name\n    pub fn get_provider(\u0026self, name: \u0026str) -\u003e Result\u003cArc\u003cdyn LLMProvider\u003e\u003e {\n        self.providers\n            .get(name)\n            .cloned()\n            .ok_or_else(|| Error::Service(format!(\"Provider '{}' not found\", name)))\n    }\n\n    /// Get the default provider (first available)\n    pub fn get_default_provider(\u0026self) -\u003e Result\u003cArc\u003cdyn LLMProvider\u003e\u003e {\n        // Try OpenAI first as the default\n        if let Ok(provider) = self.get_provider(\"openai\") {\n            return Ok(provider);\n        }\n\n        // If no specific provider, return the first available\n        self.providers\n            .values()\n            .next()\n            .cloned()\n            .ok_or_else(|| Error::Service(\"No providers available\".into()))\n    }\n\n    /// List all registered provider names\n    pub fn list_providers(\u0026self) -\u003e Vec\u003cString\u003e {\n        self.providers.keys().cloned().collect()\n    }\n\n    /// Get the configuration\n    pub fn config(\u0026self) -\u003e \u0026Config {\n        \u0026self.config\n    }\n\n    /// Update the configuration and re-register providers\n    pub fn update_config(\u0026mut self, config: Config) -\u003e Result\u003c()\u003e {\n        self.config = config;\n        self.providers.clear();\n        self.register_default_providers()?;\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::provider::tests::MockProvider;\n\n    #[test]\n    fn test_service_container_creation() {\n        let config = Config::default();\n        let container = ServiceContainer::new(config).unwrap();\n        \n        // Should create without error\n        assert!(container.providers.is_empty() || !container.providers.is_empty());\n    }\n\n    #[test]\n    fn test_register_and_get_provider() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Test response\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"mock\", mock_provider.clone());\n\n        let retrieved = container.get_provider(\"mock\").unwrap();\n        assert_eq!(retrieved.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_get_provider_not_found() {\n        let config = Config::default();\n        let container = ServiceContainer::new(config).unwrap();\n\n        let result = container.get_provider(\"nonexistent\");\n        assert!(result.is_err());\n        match result {\n            Err(Error::Service(msg)) =\u003e assert!(msg.contains(\"not found\")),\n            _ =\u003e panic!(\"Expected Service error\"),\n        }\n    }\n\n    #[test]\n    fn test_list_providers() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Clear any existing providers first\n        container.providers.clear();\n\n        let mock1 = Arc::new(MockProvider {\n            response: \"Test1\".to_string(),\n            should_fail: false,\n        });\n        let mock2 = Arc::new(MockProvider {\n            response: \"Test2\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"mock1\", mock1);\n        container.register_provider(\"mock2\", mock2);\n\n        let providers = container.list_providers();\n        assert_eq!(providers.len(), 2);\n        assert!(providers.contains(\u0026\"mock1\".to_string()));\n        assert!(providers.contains(\u0026\"mock2\".to_string()));\n    }\n\n    #[test]\n    fn test_get_default_provider() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Clear any existing providers first\n        container.providers.clear();\n\n        // With no providers registered, should fail\n        let result = container.get_default_provider();\n        assert!(result.is_err());\n\n        // Register a provider\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Default\".to_string(),\n            should_fail: false,\n        });\n        container.register_provider(\"default\", mock_provider);\n\n        let default = container.get_default_provider().unwrap();\n        assert_eq!(default.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_config_access() {\n        let config = Config::default();\n        let original_model = config.openai.default_model.clone();\n        \n        let container = ServiceContainer::new(config).unwrap();\n        assert_eq!(container.config().openai.default_model, original_model);\n    }\n\n    #[test]\n    fn test_update_config() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mut new_config = Config::default();\n        new_config.openai.default_model = \"gpt-3.5-turbo\".to_string();\n\n        container.update_config(new_config).unwrap();\n        assert_eq!(container.config().openai.default_model, \"gpt-3.5-turbo\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_functionality() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Hello from service container\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"test\", mock_provider);\n\n        let provider = container.get_provider(\"test\").unwrap();\n        \n        let request = crate::provider::CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![crate::provider::Message {\n                role: \"user\".to_string(),\n                content: \"Test message\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Hello from service container\");\n    }\n}","traces":[{"line":15,"address":[],"length":0,"stats":{"Line":15}},{"line":17,"address":[],"length":0,"stats":{"Line":15}},{"line":22,"address":[],"length":0,"stats":{"Line":30}},{"line":24,"address":[],"length":0,"stats":{"Line":15}},{"line":28,"address":[],"length":0,"stats":{"Line":16}},{"line":30,"address":[],"length":0,"stats":{"Line":32}},{"line":35,"address":[],"length":0,"stats":{"Line":16}},{"line":39,"address":[],"length":0,"stats":{"Line":27}},{"line":40,"address":[],"length":0,"stats":{"Line":135}},{"line":44,"address":[],"length":0,"stats":{"Line":11}},{"line":45,"address":[],"length":0,"stats":{"Line":11}},{"line":46,"address":[],"length":0,"stats":{"Line":22}},{"line":48,"address":[],"length":0,"stats":{"Line":17}},{"line":52,"address":[],"length":0,"stats":{"Line":2}},{"line":54,"address":[],"length":0,"stats":{"Line":4}},{"line":59,"address":[],"length":0,"stats":{"Line":2}},{"line":63,"address":[],"length":0,"stats":{"Line":4}},{"line":67,"address":[],"length":0,"stats":{"Line":1}},{"line":68,"address":[],"length":0,"stats":{"Line":4}},{"line":72,"address":[],"length":0,"stats":{"Line":6}},{"line":73,"address":[],"length":0,"stats":{"Line":6}},{"line":77,"address":[],"length":0,"stats":{"Line":1}},{"line":78,"address":[],"length":0,"stats":{"Line":2}},{"line":79,"address":[],"length":0,"stats":{"Line":2}},{"line":80,"address":[],"length":0,"stats":{"Line":2}},{"line":81,"address":[],"length":0,"stats":{"Line":1}}],"covered":26,"coverable":26},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","supervisor.rs"],"content":"use anyhow::{Context, Result};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::Mutex;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Agent {\n    pub id: String,\n    pub persona: String,\n    pub status: AgentStatus,\n    pub branch_name: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"PascalCase\")]\npub enum AgentStatus {\n    Running,\n    Stopped,\n    Error(String),\n}\n\npub struct AgentSupervisor {\n    agents: Arc\u003cMutex\u003cHashMap\u003cString, Agent\u003e\u003e\u003e,\n}\n\nimpl AgentSupervisor {\n    pub fn new() -\u003e Self {\n        Self {\n            agents: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n\n    pub async fn spawn(\u0026mut self, id: \u0026str, persona: \u0026str) -\u003e Result\u003c()\u003e {\n        let mut agents = self.agents.lock().await;\n        \n        if agents.contains_key(id) {\n            return Err(anyhow::anyhow!(\"Agent with id '{}' already exists\", id));\n        }\n\n        let agent = Agent {\n            id: id.to_string(),\n            persona: persona.to_string(),\n            status: AgentStatus::Running,\n            branch_name: format!(\"agent-{}\", id),\n        };\n\n        agents.insert(id.to_string(), agent);\n        Ok(())\n    }\n\n    pub async fn list(\u0026self) -\u003e Vec\u003cAgent\u003e {\n        let agents = self.agents.lock().await;\n        agents.values().cloned().collect()\n    }\n\n    pub async fn stop(\u0026mut self, id: \u0026str) -\u003e Result\u003c()\u003e {\n        let mut agents = self.agents.lock().await;\n        \n        let agent = agents.get_mut(id)\n            .context(format!(\"Agent '{}' not found\", id))?;\n        \n        agent.status = AgentStatus::Stopped;\n        Ok(())\n    }\n\n    pub async fn get_status(\u0026self, id: \u0026str) -\u003e Result\u003cAgentStatus\u003e {\n        let agents = self.agents.lock().await;\n        \n        let agent = agents.get(id)\n            .context(format!(\"Agent '{}' not found\", id))?;\n        \n        Ok(agent.status.clone())\n    }\n}\n\nimpl Default for AgentSupervisor {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_supervisor_new() {\n        let supervisor = AgentSupervisor::new();\n        let agents = supervisor.list().await;\n        assert_eq!(agents.len(), 0);\n    }\n\n    #[tokio::test]\n    async fn test_spawn_agent() {\n        let mut supervisor = AgentSupervisor::new();\n        let result = supervisor.spawn(\"test-agent\", \"rusty\").await;\n        assert!(result.is_ok());\n\n        let agents = supervisor.list().await;\n        assert_eq!(agents.len(), 1);\n        assert_eq!(agents[0].id, \"test-agent\");\n        assert_eq!(agents[0].persona, \"rusty\");\n    }\n\n    #[tokio::test]\n    async fn test_spawn_duplicate_agent() {\n        let mut supervisor = AgentSupervisor::new();\n        supervisor.spawn(\"test-agent\", \"rusty\").await.unwrap();\n        \n        let result = supervisor.spawn(\"test-agent\", \"pythonic\").await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_stop_agent() {\n        let mut supervisor = AgentSupervisor::new();\n        supervisor.spawn(\"test-agent\", \"rusty\").await.unwrap();\n        \n        let result = supervisor.stop(\"test-agent\").await;\n        assert!(result.is_ok());\n\n        let agents = supervisor.list().await;\n        assert!(matches!(agents[0].status, AgentStatus::Stopped));\n    }\n\n    #[tokio::test]\n    async fn test_get_status() {\n        let mut supervisor = AgentSupervisor::new();\n        supervisor.spawn(\"test-agent\", \"rusty\").await.unwrap();\n        \n        let status = supervisor.get_status(\"test-agent\").await.unwrap();\n        assert!(matches!(status, AgentStatus::Running));\n        \n        supervisor.stop(\"test-agent\").await.unwrap();\n        let status = supervisor.get_status(\"test-agent\").await.unwrap();\n        assert!(matches!(status, AgentStatus::Stopped));\n    }\n\n    #[tokio::test]\n    async fn test_get_status_nonexistent() {\n        let supervisor = AgentSupervisor::new();\n        let result = supervisor.get_status(\"nonexistent\").await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_spawn_multiple_agents() {\n        let mut supervisor = AgentSupervisor::new();\n        \n        supervisor.spawn(\"agent1\", \"rusty\").await.unwrap();\n        supervisor.spawn(\"agent2\", \"pythonic\").await.unwrap();\n        \n        let agents = supervisor.list().await;\n        assert_eq!(agents.len(), 2);\n        \n        let agent1 = agents.iter().find(|a| a.id == \"agent1\").unwrap();\n        let agent2 = agents.iter().find(|a| a.id == \"agent2\").unwrap();\n        \n        assert_eq!(agent1.persona, \"rusty\");\n        assert_eq!(agent2.persona, \"pythonic\");\n        assert!(matches!(agent1.status, AgentStatus::Running));\n        assert!(matches!(agent2.status, AgentStatus::Running));\n    }\n\n    #[tokio::test]\n    async fn test_stop_nonexistent_agent() {\n        let mut supervisor = AgentSupervisor::new();\n        let result = supervisor.stop(\"nonexistent\").await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_agent_status_serialization() {\n        let running = AgentStatus::Running;\n        let stopped = AgentStatus::Stopped;\n        let error = AgentStatus::Error(\"test error\".to_string());\n        \n        let running_json = serde_json::to_string(\u0026running).unwrap();\n        let stopped_json = serde_json::to_string(\u0026stopped).unwrap();\n        let error_json = serde_json::to_string(\u0026error).unwrap();\n        \n        assert_eq!(running_json, \"\\\"Running\\\"\");\n        assert_eq!(stopped_json, \"\\\"Stopped\\\"\");\n        assert!(error_json.contains(\"test error\"));\n    }\n\n    #[tokio::test]\n    async fn test_concurrent_agent_operations() {\n        use std::sync::Arc;\n        use tokio::sync::Mutex;\n        \n        let supervisor = Arc::new(Mutex::new(AgentSupervisor::new()));\n        let mut handles = vec![];\n        \n        // Spawn 10 agents concurrently\n        for i in 0..10 {\n            let supervisor = supervisor.clone();\n            let handle = tokio::spawn(async move {\n                let mut sup = supervisor.lock().await;\n                sup.spawn(\u0026format!(\"agent{}\", i), \"rusty\").await\n            });\n            handles.push(handle);\n        }\n        \n        // Wait for all spawn operations to complete\n        for handle in handles {\n            handle.await.unwrap().unwrap();\n        }\n        \n        let supervisor = supervisor.lock().await;\n        let agents = supervisor.list().await;\n        assert_eq!(agents.len(), 10);\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","opencode-gui","src-tauri","build.rs"],"content":"fn main() {\n    tauri_build::build()\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","opencode-gui","src-tauri","src","main.rs"],"content":"#![cfg_attr(\n    all(not(debug_assertions), target_os = \"windows\"),\n    windows_subsystem = \"windows\"\n)]\n\nuse opencode_core::supervisor::{Agent, AgentSupervisor};\nuse opencode_core::swarm;\nuse std::path::PathBuf;\nuse std::sync::Arc;\nuse tauri::{AppHandle, Emitter};\nuse tokio::sync::Mutex;\n\n// Create a struct for the application's shared state\npub struct AppState {\n    supervisor: Arc\u003cMutex\u003cAgentSupervisor\u003e\u003e,\n}\n\n// Define the payload for our progress event\n#[derive(Clone, serde::Serialize)]\nstruct SwarmProgressPayload {\n    total: usize,\n    completed: usize,\n    task: String,\n}\n\n#[tauri::command]\nasync fn list_agents(state: tauri::State\u003c'_, AppState\u003e) -\u003e Result\u003cVec\u003cAgent\u003e, String\u003e {\n    let supervisor = state.supervisor.lock().await;\n    Ok(supervisor.list().await)\n}\n\n#[tauri::command]\nasync fn spawn_agent(\n    id: String,\n    persona: String,\n    state: tauri::State\u003c'_, AppState\u003e,\n) -\u003e Result\u003c(), String\u003e {\n    let mut supervisor = state.supervisor.lock().await;\n    supervisor\n        .spawn(\u0026id, \u0026persona)\n        .await\n        .map_err(|e| e.to_string())\n}\n\n#[tauri::command]\nasync fn execute_swarm_build(\n    app_handle: AppHandle,\n    state: tauri::State\u003c'_, AppState\u003e,\n) -\u003e Result\u003c(), String\u003e {\n    let supervisor = state.supervisor.lock().await;\n\n    // For this example, we assume Cargo.toml is in the current directory.\n    let manifest_path = PathBuf::from(\"Cargo.toml\");\n    let plan = swarm::plan_build_from_manifest(\u0026manifest_path).map_err(|e| e.to_string())?;\n\n    let total_tasks = plan.tasks.len();\n    println!(\"Executing swarm build with {} tasks.\", total_tasks);\n\n    // Emit initial event\n    app_handle.emit(\"SWARM_PROGRESS\", SwarmProgressPayload {\n        total: total_tasks,\n        completed: 0,\n        task: \"Starting swarm build...\".into(),\n    }).unwrap();\n\n    // Drop the supervisor lock before spawning tasks\n    drop(supervisor);\n\n    // Spawn an agent for each task\n    for (i, task) in plan.tasks.iter().enumerate() {\n        let agent_id = format!(\"builder-{}\", task.replace('/', \"-\"));\n        let persona = \"rusty\"; // Use a default builder persona\n        \n        // Acquire lock for each spawn operation\n        let mut supervisor = state.supervisor.lock().await;\n        supervisor.spawn(\u0026agent_id, persona).await.map_err(|e| e.to_string())?;\n        drop(supervisor);\n\n        // Simulate work being done\n        tokio::time::sleep(std::time::Duration::from_secs(2)).await;\n\n        // Emit a progress event after each task\n        app_handle.emit(\"SWARM_PROGRESS\", SwarmProgressPayload {\n            total: total_tasks,\n            completed: i + 1,\n            task: format!(\"Completed build for '{}'\", task),\n        }).unwrap();\n    }\n    \n    // Final completion event\n    app_handle.emit(\"SWARM_PROGRESS\", SwarmProgressPayload {\n        total: total_tasks,\n        completed: total_tasks,\n        task: \"Swarm build finished!\".into(),\n    }).unwrap();\n\n    Ok(())\n}\n\nfn main() {\n    // Create the initial state\n    let state = AppState {\n        supervisor: Arc::new(Mutex::new(AgentSupervisor::new())),\n    };\n\n    tauri::Builder::default()\n        .manage(state) // Add the state to be managed by Tauri\n        .invoke_handler(tauri::generate_handler![\n            // Register our commands\n            list_agents,\n            spawn_agent,\n            execute_swarm_build,\n        ])\n        .run(tauri::generate_context!())\n        .expect(\"error while running tauri application\");\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","opencode-core","src","config","tests.rs"],"content":"//! Tests for configuration management\n//! \n//! This test suite defines the expected behavior for configuration loading,\n//! validation, and management following TDD principles.\n\nuse super::*;\nuse std::collections::HashMap;\nuse tempfile::TempDir;\nuse std::fs;\n\n#[cfg(test)]\nmod config_tests {\n    use super::*;\n\n    #[test]\n    fn test_default_configuration() {\n        // GIVEN: No configuration file exists\n        // WHEN: We create a default configuration\n        let config = AppConfig::default();\n        \n        // THEN: It should have sensible defaults\n        assert!(config.providers.is_empty());\n        assert_eq!(config.default_provider, None);\n        assert_eq!(config.server.host, \"127.0.0.1\");\n        assert_eq!(config.server.port, 3000);\n        assert!(!config.features.streaming_enabled);\n        assert!(!config.features.function_calling_enabled);\n    }\n\n    #[test]\n    fn test_config_from_file() {\n        // GIVEN: A configuration file with provider settings\n        let temp_dir = TempDir::new().unwrap();\n        let config_path = temp_dir.path().join(\"config.toml\");\n        \n        let config_content = r#\"\n            default_provider = \"openai\"\n            \n            [server]\n            host = \"0.0.0.0\"\n            port = 8080\n            \n            [features]\n            streaming_enabled = true\n            function_calling_enabled = true\n            \n            [[providers]]\n            name = \"openai\"\n            type = \"openai\"\n            api_key = \"${OPENAI_API_KEY}\"\n            base_url = \"https://api.openai.com/v1\"\n            \n            [[providers]]\n            name = \"anthropic\"\n            type = \"anthropic\"\n            api_key = \"${ANTHROPIC_API_KEY}\"\n            base_url = \"https://api.anthropic.com\"\n        \"#;\n        \n        fs::write(\u0026config_path, config_content).unwrap();\n        \n        // WHEN: We load the configuration\n        let config = AppConfig::from_file(\u0026config_path).unwrap();\n        \n        // THEN: All settings should be loaded correctly\n        assert_eq!(config.default_provider, Some(\"openai\".to_string()));\n        assert_eq!(config.server.host, \"0.0.0.0\");\n        assert_eq!(config.server.port, 8080);\n        assert!(config.features.streaming_enabled);\n        assert!(config.features.function_calling_enabled);\n        assert_eq!(config.providers.len(), 2);\n        \n        let openai = \u0026config.providers[0];\n        assert_eq!(openai.name, \"openai\");\n        assert_eq!(openai.provider_type, ProviderType::OpenAI);\n        assert_eq!(openai.api_key, \"${OPENAI_API_KEY}\");\n    }\n\n    #[test]\n    fn test_environment_variable_expansion() {\n        // GIVEN: Configuration with environment variables\n        std::env::set_var(\"TEST_API_KEY\", \"secret-key-123\");\n        std::env::set_var(\"TEST_BASE_URL\", \"https://test.api.com\");\n        \n        let config_str = r#\"\n            [[providers]]\n            name = \"test\"\n            type = \"openai\"\n            api_key = \"${TEST_API_KEY}\"\n            base_url = \"${TEST_BASE_URL}\"\n        \"#;\n        \n        // WHEN: We parse and expand the configuration\n        let mut config: AppConfig = toml::from_str(config_str).unwrap();\n        config.expand_env_vars();\n        \n        // THEN: Environment variables should be replaced\n        assert_eq!(config.providers[0].api_key, \"secret-key-123\");\n        assert_eq!(config.providers[0].base_url, Some(\"https://test.api.com\".to_string()));\n        \n        // Cleanup\n        std::env::remove_var(\"TEST_API_KEY\");\n        std::env::remove_var(\"TEST_BASE_URL\");\n    }\n\n    #[test]\n    fn test_config_validation() {\n        // GIVEN: Various configuration scenarios\n        \n        // Test 1: Valid configuration\n        let valid_config = AppConfig {\n            providers: vec![\n                ProviderConfig {\n                    name: \"provider1\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key: \"key123\".to_string(),\n                    base_url: None,\n                    models: vec![],\n                    rate_limit: None,\n                }\n            ],\n            default_provider: Some(\"provider1\".to_string()),\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        };\n        \n        assert!(valid_config.validate().is_ok());\n        \n        // Test 2: Invalid - default provider doesn't exist\n        let invalid_config = AppConfig {\n            providers: vec![],\n            default_provider: Some(\"nonexistent\".to_string()),\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        };\n        \n        let result = invalid_config.validate();\n        assert!(result.is_err());\n        assert!(result.unwrap_err().to_string().contains(\"Default provider 'nonexistent' not found\"));\n        \n        // Test 3: Invalid - duplicate provider names\n        let duplicate_config = AppConfig {\n            providers: vec![\n                ProviderConfig {\n                    name: \"same_name\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key: \"key1\".to_string(),\n                    base_url: None,\n                    models: vec![],\n                    rate_limit: None,\n                },\n                ProviderConfig {\n                    name: \"same_name\".to_string(),\n                    provider_type: ProviderType::Anthropic,\n                    api_key: \"key2\".to_string(),\n                    base_url: None,\n                    models: vec![],\n                    rate_limit: None,\n                }\n            ],\n            default_provider: None,\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        };\n        \n        let result = duplicate_config.validate();\n        assert!(result.is_err());\n        assert!(result.unwrap_err().to_string().contains(\"Duplicate provider name\"));\n    }\n\n    #[test]\n    fn test_config_merge() {\n        // GIVEN: A base configuration and override configuration\n        let base_config = AppConfig {\n            providers: vec![\n                ProviderConfig {\n                    name: \"openai\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key: \"base-key\".to_string(),\n                    base_url: None,\n                    models: vec![],\n                    rate_limit: None,\n                }\n            ],\n            default_provider: Some(\"openai\".to_string()),\n            server: ServerConfig {\n                host: \"127.0.0.1\".to_string(),\n                port: 3000,\n            },\n            features: FeaturesConfig {\n                streaming_enabled: false,\n                function_calling_enabled: false,\n            },\n        };\n        \n        let override_config = PartialAppConfig {\n            providers: None,\n            default_provider: Some(Some(\"anthropic\".to_string())),\n            server: Some(PartialServerConfig {\n                host: None,\n                port: Some(8080),\n            }),\n            features: Some(PartialFeaturesConfig {\n                streaming_enabled: Some(true),\n                function_calling_enabled: None,\n            }),\n        };\n        \n        // WHEN: We merge the configurations\n        let merged = base_config.merge(override_config);\n        \n        // THEN: Override values should take precedence\n        assert_eq!(merged.default_provider, Some(\"anthropic\".to_string()));\n        assert_eq!(merged.server.host, \"127.0.0.1\"); // Not overridden\n        assert_eq!(merged.server.port, 8080); // Overridden\n        assert!(merged.features.streaming_enabled); // Overridden\n        assert!(!merged.features.function_calling_enabled); // Not overridden\n    }\n\n    #[test]\n    fn test_provider_specific_config() {\n        // GIVEN: Provider-specific configurations\n        let config = AppConfig {\n            providers: vec![\n                ProviderConfig {\n                    name: \"openai-gpt4\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key: \"key1\".to_string(),\n                    base_url: None,\n                    models: vec![\"gpt-4\".to_string(), \"gpt-4-turbo\".to_string()],\n                    rate_limit: Some(RateLimitConfig {\n                        requests_per_minute: 60,\n                        tokens_per_minute: 90000,\n                    }),\n                },\n                ProviderConfig {\n                    name: \"anthropic-claude\".to_string(),\n                    provider_type: ProviderType::Anthropic,\n                    api_key: \"key2\".to_string(),\n                    base_url: Some(\"https://api.anthropic.com/v1\".to_string()),\n                    models: vec![\"claude-3-opus\".to_string()],\n                    rate_limit: Some(RateLimitConfig {\n                        requests_per_minute: 50,\n                        tokens_per_minute: 100000,\n                    }),\n                },\n            ],\n            default_provider: None,\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        };\n        \n        // WHEN: We access provider configurations\n        let openai_config = config.get_provider(\"openai-gpt4\").unwrap();\n        let anthropic_config = config.get_provider(\"anthropic-claude\").unwrap();\n        \n        // THEN: Each provider should have its specific settings\n        assert_eq!(openai_config.models.len(), 2);\n        assert!(openai_config.models.contains(\u0026\"gpt-4\".to_string()));\n        assert_eq!(openai_config.rate_limit.as_ref().unwrap().requests_per_minute, 60);\n        \n        assert_eq!(anthropic_config.models.len(), 1);\n        assert_eq!(anthropic_config.base_url, Some(\"https://api.anthropic.com/v1\".to_string()));\n        assert_eq!(anthropic_config.rate_limit.as_ref().unwrap().tokens_per_minute, 100000);\n    }\n\n    #[test]\n    fn test_config_hot_reload() {\n        // GIVEN: A configuration that can be watched for changes\n        let temp_dir = TempDir::new().unwrap();\n        let config_path = temp_dir.path().join(\"config.toml\");\n        \n        let initial_config = r#\"\n            default_provider = \"openai\"\n            \n            [[providers]]\n            name = \"openai\"\n            type = \"openai\"\n            api_key = \"initial-key\"\n        \"#;\n        \n        fs::write(\u0026config_path, initial_config).unwrap();\n        \n        // WHEN: We set up configuration with hot reload\n        let (config, mut watcher) = AppConfig::with_hot_reload(\u0026config_path).unwrap();\n        \n        // Initial state check\n        assert_eq!(config.read().unwrap().providers[0].api_key, \"initial-key\");\n        \n        // Update the configuration file\n        let updated_config = r#\"\n            default_provider = \"openai\"\n            \n            [[providers]]\n            name = \"openai\"\n            type = \"openai\"\n            api_key = \"updated-key\"\n        \"#;\n        \n        fs::write(\u0026config_path, updated_config).unwrap();\n        \n        // THEN: The configuration should be automatically reloaded\n        // Note: In real implementation, this would use file system events\n        // For testing, we simulate the reload\n        std::thread::sleep(std::time::Duration::from_millis(100));\n        \n        // In actual implementation, the watcher would trigger this\n        let new_config = AppConfig::from_file(\u0026config_path).unwrap();\n        *config.write().unwrap() = new_config;\n        \n        assert_eq!(config.read().unwrap().providers[0].api_key, \"updated-key\");\n    }\n}\n\n// Type definitions that will be moved to the actual implementation\nuse serde::{Deserialize, Serialize};\nuse std::sync::{Arc, RwLock};\nuse std::path::Path;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AppConfig {\n    pub providers: Vec\u003cProviderConfig\u003e,\n    pub default_provider: Option\u003cString\u003e,\n    pub server: ServerConfig,\n    pub features: FeaturesConfig,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProviderConfig {\n    pub name: String,\n    #[serde(rename = \"type\")]\n    pub provider_type: ProviderType,\n    pub api_key: String,\n    pub base_url: Option\u003cString\u003e,\n    #[serde(default)]\n    pub models: Vec\u003cString\u003e,\n    pub rate_limit: Option\u003cRateLimitConfig\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum ProviderType {\n    OpenAI,\n    Anthropic,\n    Google,\n    Local,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RateLimitConfig {\n    pub requests_per_minute: u32,\n    pub tokens_per_minute: u32,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ServerConfig {\n    pub host: String,\n    pub port: u16,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct FeaturesConfig {\n    pub streaming_enabled: bool,\n    pub function_calling_enabled: bool,\n}\n\n// Partial config structs for merging\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PartialAppConfig {\n    pub providers: Option\u003cVec\u003cProviderConfig\u003e\u003e,\n    pub default_provider: Option\u003cOption\u003cString\u003e\u003e,\n    pub server: Option\u003cPartialServerConfig\u003e,\n    pub features: Option\u003cPartialFeaturesConfig\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PartialServerConfig {\n    pub host: Option\u003cString\u003e,\n    pub port: Option\u003cu16\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PartialFeaturesConfig {\n    pub streaming_enabled: Option\u003cbool\u003e,\n    pub function_calling_enabled: Option\u003cbool\u003e,\n}\n\n// Default implementations\nimpl Default for AppConfig {\n    fn default() -\u003e Self {\n        Self {\n            providers: vec![],\n            default_provider: None,\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        }\n    }\n}\n\nimpl Default for ServerConfig {\n    fn default() -\u003e Self {\n        Self {\n            host: \"127.0.0.1\".to_string(),\n            port: 3000,\n        }\n    }\n}\n\nimpl Default for FeaturesConfig {\n    fn default() -\u003e Self {\n        Self {\n            streaming_enabled: false,\n            function_calling_enabled: false,\n        }\n    }\n}\n\n// Implementation stubs\nimpl AppConfig {\n    pub fn from_file(path: \u0026Path) -\u003e Result\u003cSelf, ConfigError\u003e {\n        let content = fs::read_to_string(path)?;\n        let config: Self = toml::from_str(\u0026content)?;\n        Ok(config)\n    }\n    \n    pub fn expand_env_vars(\u0026mut self) {\n        for provider in \u0026mut self.providers {\n            if provider.api_key.starts_with(\"${\") \u0026\u0026 provider.api_key.ends_with(\"}\") {\n                let var_name = \u0026provider.api_key[2..provider.api_key.len()-1];\n                if let Ok(value) = std::env::var(var_name) {\n                    provider.api_key = value;\n                }\n            }\n            \n            if let Some(ref mut base_url) = provider.base_url {\n                if base_url.starts_with(\"${\") \u0026\u0026 base_url.ends_with(\"}\") {\n                    let var_name = \u0026base_url[2..base_url.len()-1];\n                    if let Ok(value) = std::env::var(var_name) {\n                        *base_url = value;\n                    }\n                }\n            }\n        }\n    }\n    \n    pub fn validate(\u0026self) -\u003e Result\u003c(), ConfigError\u003e {\n        // Check for duplicate provider names\n        let mut names = std::collections::HashSet::new();\n        for provider in \u0026self.providers {\n            if !names.insert(\u0026provider.name) {\n                return Err(ConfigError::Validation(format!(\"Duplicate provider name: {}\", provider.name)));\n            }\n        }\n        \n        // Check that default provider exists\n        if let Some(ref default) = self.default_provider {\n            if !self.providers.iter().any(|p| \u0026p.name == default) {\n                return Err(ConfigError::Validation(format!(\"Default provider '{}' not found\", default)));\n            }\n        }\n        \n        Ok(())\n    }\n    \n    pub fn merge(mut self, partial: PartialAppConfig) -\u003e Self {\n        if let Some(providers) = partial.providers {\n            self.providers = providers;\n        }\n        \n        if let Some(default) = partial.default_provider {\n            self.default_provider = default;\n        }\n        \n        if let Some(server) = partial.server {\n            if let Some(host) = server.host {\n                self.server.host = host;\n            }\n            if let Some(port) = server.port {\n                self.server.port = port;\n            }\n        }\n        \n        if let Some(features) = partial.features {\n            if let Some(streaming) = features.streaming_enabled {\n                self.features.streaming_enabled = streaming;\n            }\n            if let Some(function_calling) = features.function_calling_enabled {\n                self.features.function_calling_enabled = function_calling;\n            }\n        }\n        \n        self\n    }\n    \n    pub fn get_provider(\u0026self, name: \u0026str) -\u003e Option\u003c\u0026ProviderConfig\u003e {\n        self.providers.iter().find(|p| p.name == name)\n    }\n    \n    pub fn with_hot_reload(path: \u0026Path) -\u003e Result\u003c(Arc\u003cRwLock\u003cSelf\u003e\u003e, ConfigWatcher), ConfigError\u003e {\n        let config = Self::from_file(path)?;\n        let config = Arc::new(RwLock::new(config));\n        \n        // In real implementation, this would set up file system watching\n        let watcher = ConfigWatcher {};\n        \n        Ok((config, watcher))\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum ConfigError {\n    #[error(\"IO error: {0}\")]\n    Io(#[from] std::io::Error),\n    \n    #[error(\"Parse error: {0}\")]\n    Parse(#[from] toml::de::Error),\n    \n    #[error(\"Validation error: {0}\")]\n    Validation(String),\n}\n\npub struct ConfigWatcher {\n    // File system watcher implementation\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","opencode-core","src","di","tests.rs"],"content":"//! Tests for dependency injection container\n//! \n//! This test suite defines the expected behavior for the DI container\n//! and service registration/resolution following TDD principles.\n\nuse super::*;\nuse std::sync::Arc;\nuse async_trait::async_trait;\n\n#[cfg(test)]\nmod di_container_tests {\n    use super::*;\n\n    #[test]\n    fn test_container_creation() {\n        // GIVEN: A new DI container\n        let container = Container::new();\n        \n        // WHEN: We check its initial state\n        // THEN: It should be empty\n        assert_eq!(container.service_count(), 0);\n    }\n\n    #[test]\n    fn test_singleton_registration_and_resolution() {\n        // GIVEN: A container with a singleton service\n        let mut container = Container::new();\n        \n        // Define a test service\n        #[derive(Clone)]\n        struct TestService {\n            value: String,\n        }\n        \n        impl TestService {\n            fn new() -\u003e Self {\n                Self {\n                    value: \"test\".to_string(),\n                }\n            }\n        }\n        \n        // WHEN: We register and resolve the service\n        container.register_singleton::\u003cTestService\u003e(|| Arc::new(TestService::new()));\n        \n        let service1 = container.resolve::\u003cTestService\u003e().unwrap();\n        let service2 = container.resolve::\u003cTestService\u003e().unwrap();\n        \n        // THEN: Both resolutions should return the same instance\n        assert!(Arc::ptr_eq(\u0026service1, \u0026service2));\n        assert_eq!(service1.value, \"test\");\n    }\n\n    #[test]\n    fn test_factory_registration_and_resolution() {\n        // GIVEN: A container with a factory service\n        let mut container = Container::new();\n        \n        // Counter to track factory invocations\n        let counter = Arc::new(std::sync::Mutex::new(0));\n        let counter_clone = counter.clone();\n        \n        #[derive(Clone)]\n        struct FactoryService {\n            id: u32,\n        }\n        \n        // WHEN: We register a factory\n        container.register_factory::\u003cFactoryService\u003e(move || {\n            let mut count = counter_clone.lock().unwrap();\n            *count += 1;\n            Arc::new(FactoryService { id: *count })\n        });\n        \n        let service1 = container.resolve::\u003cFactoryService\u003e().unwrap();\n        let service2 = container.resolve::\u003cFactoryService\u003e().unwrap();\n        \n        // THEN: Each resolution should create a new instance\n        assert!(!Arc::ptr_eq(\u0026service1, \u0026service2));\n        assert_eq!(service1.id, 1);\n        assert_eq!(service2.id, 2);\n    }\n\n    #[test]\n    fn test_interface_registration() {\n        // GIVEN: An interface and multiple implementations\n        trait Database: Send + Sync {\n            fn name(\u0026self) -\u003e \u0026str;\n        }\n        \n        struct PostgresDB;\n        impl Database for PostgresDB {\n            fn name(\u0026self) -\u003e \u0026str {\n                \"PostgreSQL\"\n            }\n        }\n        \n        struct MySQLDB;\n        impl Database for MySQLDB {\n            fn name(\u0026self) -\u003e \u0026str {\n                \"MySQL\"\n            }\n        }\n        \n        // WHEN: We register implementations for the interface\n        let mut container = Container::new();\n        \n        container.register_interface::\u003cdyn Database, PostgresDB\u003e(\n            \"postgres\",\n            || Arc::new(PostgresDB),\n        );\n        \n        container.register_interface::\u003cdyn Database, MySQLDB\u003e(\n            \"mysql\",\n            || Arc::new(MySQLDB),\n        );\n        \n        // THEN: We can resolve specific implementations\n        let postgres = container.resolve_interface::\u003cdyn Database\u003e(\"postgres\").unwrap();\n        let mysql = container.resolve_interface::\u003cdyn Database\u003e(\"mysql\").unwrap();\n        \n        assert_eq!(postgres.name(), \"PostgreSQL\");\n        assert_eq!(mysql.name(), \"MySQL\");\n    }\n\n    #[test]\n    fn test_dependency_injection_with_dependencies() {\n        // GIVEN: Services with dependencies\n        #[derive(Clone)]\n        struct ConfigService {\n            api_key: String,\n        }\n        \n        #[derive(Clone)]\n        struct ApiClient {\n            config: Arc\u003cConfigService\u003e,\n        }\n        \n        impl ApiClient {\n            fn new(config: Arc\u003cConfigService\u003e) -\u003e Self {\n                Self { config }\n            }\n        }\n        \n        #[derive(Clone)]\n        struct UserService {\n            api_client: Arc\u003cApiClient\u003e,\n        }\n        \n        impl UserService {\n            fn new(api_client: Arc\u003cApiClient\u003e) -\u003e Self {\n                Self { api_client }\n            }\n        }\n        \n        // WHEN: We register services with dependencies\n        let mut container = Container::new();\n        \n        container.register_singleton::\u003cConfigService\u003e(|| {\n            Arc::new(ConfigService {\n                api_key: \"secret123\".to_string(),\n            })\n        });\n        \n        container.register_singleton_with_deps::\u003cApiClient, (Arc\u003cConfigService\u003e,)\u003e(\n            |deps| {\n                let (config,) = deps;\n                Arc::new(ApiClient::new(config))\n            }\n        );\n        \n        container.register_singleton_with_deps::\u003cUserService, (Arc\u003cApiClient\u003e,)\u003e(\n            |deps| {\n                let (api_client,) = deps;\n                Arc::new(UserService::new(api_client))\n            }\n        );\n        \n        // THEN: Dependencies should be resolved correctly\n        let user_service = container.resolve::\u003cUserService\u003e().unwrap();\n        assert_eq!(user_service.api_client.config.api_key, \"secret123\");\n    }\n\n    #[test]\n    fn test_scoped_services() {\n        // GIVEN: A container with scoped services\n        let mut container = Container::new();\n        \n        #[derive(Clone)]\n        struct RequestContext {\n            request_id: String,\n        }\n        \n        // WHEN: We register a scoped service\n        container.register_scoped::\u003cRequestContext\u003e();\n        \n        // Create scope 1\n        let mut scope1 = container.create_scope();\n        scope1.provide::\u003cRequestContext\u003e(Arc::new(RequestContext {\n            request_id: \"req-123\".to_string(),\n        }));\n        \n        // Create scope 2\n        let mut scope2 = container.create_scope();\n        scope2.provide::\u003cRequestContext\u003e(Arc::new(RequestContext {\n            request_id: \"req-456\".to_string(),\n        }));\n        \n        // THEN: Each scope should have its own instance\n        let ctx1 = scope1.resolve::\u003cRequestContext\u003e().unwrap();\n        let ctx2 = scope2.resolve::\u003cRequestContext\u003e().unwrap();\n        \n        assert_eq!(ctx1.request_id, \"req-123\");\n        assert_eq!(ctx2.request_id, \"req-456\");\n    }\n\n    #[test]\n    fn test_circular_dependency_detection() {\n        // GIVEN: Services with circular dependencies\n        let mut container = Container::new();\n        \n        // This should be detected and handled appropriately\n        // Implementation would need cycle detection\n    }\n\n    #[test]\n    fn test_service_not_found() {\n        // GIVEN: A container without a specific service\n        let container = Container::new();\n        \n        struct UnregisteredService;\n        \n        // WHEN: We try to resolve an unregistered service\n        let result = container.resolve::\u003cUnregisteredService\u003e();\n        \n        // THEN: It should return an error\n        assert!(result.is_err());\n        match result {\n            Err(DIError::ServiceNotFound(type_name)) =\u003e {\n                assert!(type_name.contains(\"UnregisteredService\"));\n            }\n            _ =\u003e panic!(\"Expected ServiceNotFound error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_async_initialization() {\n        // GIVEN: Services that require async initialization\n        #[derive(Clone)]\n        struct AsyncService {\n            data: String,\n        }\n        \n        impl AsyncService {\n            async fn new() -\u003e Self {\n                // Simulate async initialization\n                tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;\n                Self {\n                    data: \"async initialized\".to_string(),\n                }\n            }\n        }\n        \n        // WHEN: We register an async service\n        let mut container = Container::new();\n        \n        container.register_async_singleton::\u003cAsyncService\u003e(|| {\n            Box::pin(async {\n                Arc::new(AsyncService::new().await)\n            })\n        });\n        \n        // THEN: We should be able to resolve it\n        let service = container.resolve_async::\u003cAsyncService\u003e().await.unwrap();\n        assert_eq!(service.data, \"async initialized\");\n    }\n\n    #[test]\n    fn test_service_lifetime_management() {\n        // GIVEN: Services with different lifetimes\n        let mut container = Container::new();\n        \n        // Track service creation\n        let singleton_count = Arc::new(std::sync::atomic::AtomicU32::new(0));\n        let transient_count = Arc::new(std::sync::atomic::AtomicU32::new(0));\n        \n        let singleton_count_clone = singleton_count.clone();\n        let transient_count_clone = transient_count.clone();\n        \n        #[derive(Clone)]\n        struct SingletonService {\n            id: u32,\n        }\n        \n        #[derive(Clone)]\n        struct TransientService {\n            id: u32,\n        }\n        \n        // Register singleton\n        container.register_singleton::\u003cSingletonService\u003e(move || {\n            let id = singleton_count_clone.fetch_add(1, std::sync::atomic::Ordering::SeqCst);\n            Arc::new(SingletonService { id })\n        });\n        \n        // Register transient\n        container.register_factory::\u003cTransientService\u003e(move || {\n            let id = transient_count_clone.fetch_add(1, std::sync::atomic::Ordering::SeqCst);\n            Arc::new(TransientService { id })\n        });\n        \n        // WHEN: We resolve services multiple times\n        let singleton1 = container.resolve::\u003cSingletonService\u003e().unwrap();\n        let singleton2 = container.resolve::\u003cSingletonService\u003e().unwrap();\n        let transient1 = container.resolve::\u003cTransientService\u003e().unwrap();\n        let transient2 = container.resolve::\u003cTransientService\u003e().unwrap();\n        \n        // THEN: Singleton should be created once, transient multiple times\n        assert_eq!(singleton1.id, 0);\n        assert_eq!(singleton2.id, 0);\n        assert_eq!(transient1.id, 0);\n        assert_eq!(transient2.id, 1);\n        \n        assert_eq!(singleton_count.load(std::sync::atomic::Ordering::SeqCst), 1);\n        assert_eq!(transient_count.load(std::sync::atomic::Ordering::SeqCst), 2);\n    }\n\n    #[test]\n    fn test_container_builder_pattern() {\n        // GIVEN: A container builder\n        let container = ContainerBuilder::new()\n            .register_singleton::\u003cConfigService\u003e(|| {\n                Arc::new(ConfigService {\n                    api_key: \"test-key\".to_string(),\n                })\n            })\n            .register_factory::\u003cRequestContext\u003e(|| {\n                Arc::new(RequestContext {\n                    request_id: uuid::Uuid::new_v4().to_string(),\n                })\n            })\n            .build();\n        \n        // WHEN: We use the built container\n        let config = container.resolve::\u003cConfigService\u003e().unwrap();\n        let ctx1 = container.resolve::\u003cRequestContext\u003e().unwrap();\n        let ctx2 = container.resolve::\u003cRequestContext\u003e().unwrap();\n        \n        // THEN: Services should be properly registered\n        assert_eq!(config.api_key, \"test-key\");\n        assert_ne!(ctx1.request_id, ctx2.request_id); // Factory creates new instances\n    }\n}\n\n// Type definitions that will be moved to the actual implementation\nuse std::any::{Any, TypeId};\nuse std::collections::HashMap;\nuse std::future::Future;\nuse std::pin::Pin;\n\npub struct Container {\n    services: HashMap\u003cTypeId, Box\u003cdyn Any + Send + Sync\u003e\u003e,\n    factories: HashMap\u003cTypeId, Box\u003cdyn Any + Send + Sync\u003e\u003e,\n    interfaces: HashMap\u003c(TypeId, String), Box\u003cdyn Any + Send + Sync\u003e\u003e,\n    scoped_types: HashMap\u003cTypeId, ()\u003e,\n}\n\npub struct Scope\u003c'a\u003e {\n    container: \u0026'a Container,\n    scoped_instances: HashMap\u003cTypeId, Box\u003cdyn Any + Send + Sync\u003e\u003e,\n}\n\npub struct ContainerBuilder {\n    container: Container,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum DIError {\n    #[error(\"Service not found: {0}\")]\n    ServiceNotFound(String),\n    \n    #[error(\"Circular dependency detected\")]\n    CircularDependency,\n    \n    #[error(\"Service already registered: {0}\")]\n    AlreadyRegistered(String),\n    \n    #[error(\"Invalid service lifetime\")]\n    InvalidLifetime,\n}\n\n// Placeholder implementations\nimpl Container {\n    pub fn new() -\u003e Self {\n        Self {\n            services: HashMap::new(),\n            factories: HashMap::new(),\n            interfaces: HashMap::new(),\n            scoped_types: HashMap::new(),\n        }\n    }\n    \n    pub fn service_count(\u0026self) -\u003e usize {\n        self.services.len() + self.factories.len()\n    }\n    \n    pub fn register_singleton\u003cT: Any + Send + Sync + 'static\u003e(\n        \u0026mut self,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) {\n        let service = factory();\n        self.services.insert(TypeId::of::\u003cT\u003e(), Box::new(service));\n    }\n    \n    pub fn register_factory\u003cT: Any + Send + Sync + 'static\u003e(\n        \u0026mut self,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) {\n        self.factories.insert(TypeId::of::\u003cT\u003e(), Box::new(factory));\n    }\n    \n    pub fn register_interface\u003cI: ?Sized + 'static, T: I + Send + Sync + 'static\u003e(\n        \u0026mut self,\n        name: \u0026str,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) {\n        let service = factory();\n        self.interfaces.insert(\n            (TypeId::of::\u003cI\u003e(), name.to_string()),\n            Box::new(service as Arc\u003cI\u003e),\n        );\n    }\n    \n    pub fn register_singleton_with_deps\u003cT: Any + Send + Sync + 'static, D\u003e(\n        \u0026mut self,\n        factory: impl Fn(D) -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) where\n        D: ResolveDependencies,\n    {\n        // Implementation would resolve dependencies and call factory\n    }\n    \n    pub fn register_scoped\u003cT: Any + Send + Sync + 'static\u003e(\u0026mut self) {\n        self.scoped_types.insert(TypeId::of::\u003cT\u003e(), ());\n    }\n    \n    pub fn register_async_singleton\u003cT: Any + Send + Sync + 'static\u003e(\n        \u0026mut self,\n        factory: impl Fn() -\u003e Pin\u003cBox\u003cdyn Future\u003cOutput = Arc\u003cT\u003e\u003e + Send\u003e\u003e + Send + Sync + 'static,\n    ) {\n        // Implementation would store async factory\n    }\n    \n    pub fn resolve\u003cT: Any + Send + Sync + 'static\u003e(\u0026self) -\u003e Result\u003cArc\u003cT\u003e, DIError\u003e {\n        // Try singletons first\n        if let Some(service) = self.services.get(\u0026TypeId::of::\u003cT\u003e()) {\n            if let Some(arc) = service.downcast_ref::\u003cArc\u003cT\u003e\u003e() {\n                return Ok(arc.clone());\n            }\n        }\n        \n        // Try factories\n        if let Some(factory) = self.factories.get(\u0026TypeId::of::\u003cT\u003e()) {\n            if let Some(f) = factory.downcast_ref::\u003cBox\u003cdyn Fn() -\u003e Arc\u003cT\u003e + Send + Sync\u003e\u003e() {\n                return Ok(f());\n            }\n        }\n        \n        Err(DIError::ServiceNotFound(std::any::type_name::\u003cT\u003e().to_string()))\n    }\n    \n    pub fn resolve_interface\u003cI: ?Sized + 'static\u003e(\n        \u0026self,\n        name: \u0026str,\n    ) -\u003e Result\u003cArc\u003cI\u003e, DIError\u003e {\n        if let Some(service) = self.interfaces.get(\u0026(TypeId::of::\u003cI\u003e(), name.to_string())) {\n            if let Some(arc) = service.downcast_ref::\u003cArc\u003cI\u003e\u003e() {\n                return Ok(arc.clone());\n            }\n        }\n        \n        Err(DIError::ServiceNotFound(format!(\"{} ({})\", std::any::type_name::\u003cI\u003e(), name)))\n    }\n    \n    pub async fn resolve_async\u003cT: Any + Send + Sync + 'static\u003e(\u0026self) -\u003e Result\u003cArc\u003cT\u003e, DIError\u003e {\n        // Implementation would handle async resolution\n        self.resolve::\u003cT\u003e()\n    }\n    \n    pub fn create_scope(\u0026self) -\u003e Scope {\n        Scope {\n            container: self,\n            scoped_instances: HashMap::new(),\n        }\n    }\n}\n\nimpl\u003c'a\u003e Scope\u003c'a\u003e {\n    pub fn provide\u003cT: Any + Send + Sync + 'static\u003e(\u0026mut self, instance: Arc\u003cT\u003e) {\n        self.scoped_instances.insert(TypeId::of::\u003cT\u003e(), Box::new(instance));\n    }\n    \n    pub fn resolve\u003cT: Any + Send + Sync + 'static\u003e(\u0026self) -\u003e Result\u003cArc\u003cT\u003e, DIError\u003e {\n        // Check scoped instances first\n        if let Some(instance) = self.scoped_instances.get(\u0026TypeId::of::\u003cT\u003e()) {\n            if let Some(arc) = instance.downcast_ref::\u003cArc\u003cT\u003e\u003e() {\n                return Ok(arc.clone());\n            }\n        }\n        \n        // Fall back to container\n        self.container.resolve::\u003cT\u003e()\n    }\n}\n\nimpl ContainerBuilder {\n    pub fn new() -\u003e Self {\n        Self {\n            container: Container::new(),\n        }\n    }\n    \n    pub fn register_singleton\u003cT: Any + Send + Sync + 'static\u003e(\n        mut self,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) -\u003e Self {\n        self.container.register_singleton(factory);\n        self\n    }\n    \n    pub fn register_factory\u003cT: Any + Send + Sync + 'static\u003e(\n        mut self,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) -\u003e Self {\n        self.container.register_factory(factory);\n        self\n    }\n    \n    pub fn build(self) -\u003e Container {\n        self.container\n    }\n}\n\n// Trait for resolving dependencies\npub trait ResolveDependencies {\n    fn resolve(container: \u0026Container) -\u003e Self;\n}\n\n// Implement for tuples of dependencies\nimpl\u003cT1: Any + Send + Sync + 'static\u003e ResolveDependencies for (Arc\u003cT1\u003e,) {\n    fn resolve(container: \u0026Container) -\u003e Self {\n        (container.resolve::\u003cT1\u003e().unwrap(),)\n    }\n}","traces":[{"line":411,"address":[],"length":0,"stats":{"Line":0}},{"line":412,"address":[],"length":0,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":428,"address":[],"length":0,"stats":{"Line":0}},{"line":429,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":0}},{"line":443,"address":[],"length":0,"stats":{"Line":0}},{"line":444,"address":[],"length":0,"stats":{"Line":0}},{"line":454,"address":[],"length":0,"stats":{"Line":0}},{"line":456,"address":[],"length":0,"stats":{"Line":0}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":458,"address":[],"length":0,"stats":{"Line":0}},{"line":463,"address":[],"length":0,"stats":{"Line":0}},{"line":464,"address":[],"length":0,"stats":{"Line":0}},{"line":465,"address":[],"length":0,"stats":{"Line":0}},{"line":469,"address":[],"length":0,"stats":{"Line":0}},{"line":476,"address":[],"length":0,"stats":{"Line":0}},{"line":477,"address":[],"length":0,"stats":{"Line":0}},{"line":478,"address":[],"length":0,"stats":{"Line":0}},{"line":482,"address":[],"length":0,"stats":{"Line":0}},{"line":485,"address":[],"length":0,"stats":{"Line":0}},{"line":487,"address":[],"length":0,"stats":{"Line":0}},{"line":499,"address":[],"length":0,"stats":{"Line":0}},{"line":500,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":505,"address":[],"length":0,"stats":{"Line":0}},{"line":506,"address":[],"length":0,"stats":{"Line":0}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":512,"address":[],"length":0,"stats":{"Line":0}},{"line":527,"address":[],"length":0,"stats":{"Line":0}},{"line":528,"address":[],"length":0,"stats":{"Line":0}},{"line":535,"address":[],"length":0,"stats":{"Line":0}},{"line":536,"address":[],"length":0,"stats":{"Line":0}},{"line":551,"address":[],"length":0,"stats":{"Line":0}},{"line":552,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":36},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","opencode-core","src","error","tests.rs"],"content":"//! Tests for error handling improvements\n//! \n//! This test suite defines the expected behavior for comprehensive error handling,\n//! error context, and error recovery following TDD principles.\n\nuse super::*;\nuse std::io;\nuse std::sync::Arc;\n\n#[cfg(test)]\nmod error_handling_tests {\n    use super::*;\n\n    #[test]\n    fn test_error_creation_and_display() {\n        // GIVEN: Various error scenarios\n        \n        // WHEN: We create different error types\n        let api_error = OpenCodeError::Provider(ProviderError::ApiError {\n            status: 429,\n            message: \"Rate limit exceeded\".to_string(),\n        });\n        \n        let config_error = OpenCodeError::Configuration(ConfigError::Validation(\n            \"Invalid provider configuration\".to_string()\n        ));\n        \n        let network_error = OpenCodeError::Network(NetworkError::Timeout {\n            operation: \"API call\".to_string(),\n            duration: std::time::Duration::from_secs(30),\n        });\n        \n        // THEN: Error messages should be properly formatted\n        assert_eq!(api_error.to_string(), \"Provider error: API error: Rate limit exceeded (status: 429)\");\n        assert_eq!(config_error.to_string(), \"Configuration error: Validation error: Invalid provider configuration\");\n        assert_eq!(network_error.to_string(), \"Network error: Operation 'API call' timed out after 30s\");\n    }\n\n    #[test]\n    fn test_error_context_chain() {\n        // GIVEN: An error with context chain\n        let base_error = io::Error::new(io::ErrorKind::NotFound, \"File not found\");\n        \n        // WHEN: We add context to the error\n        let error = OpenCodeError::Io(base_error)\n            .with_context(\"Loading configuration\")\n            .with_context(\"Initializing application\");\n        \n        // THEN: Context should be preserved in order\n        let contexts = error.contexts();\n        assert_eq!(contexts.len(), 2);\n        assert_eq!(contexts[0], \"Loading configuration\");\n        assert_eq!(contexts[1], \"Initializing application\");\n        \n        // Full error message should include context\n        let full_message = error.full_message();\n        assert!(full_message.contains(\"Initializing application\"));\n        assert!(full_message.contains(\"Loading configuration\"));\n        assert!(full_message.contains(\"File not found\"));\n    }\n\n    #[test]\n    fn test_error_recovery_suggestions() {\n        // GIVEN: Errors with recovery suggestions\n        \n        // WHEN: We create errors with recovery hints\n        let rate_limit_error = OpenCodeError::Provider(ProviderError::RateLimitExceeded)\n            .with_recovery(Recovery::Retry {\n                after: std::time::Duration::from_secs(60),\n                max_attempts: 3,\n            });\n        \n        let auth_error = OpenCodeError::Provider(ProviderError::AuthenticationError(\n            \"Invalid API key\".to_string()\n        ))\n            .with_recovery(Recovery::Manual(\n                \"Please check your API key in the configuration file\".to_string()\n            ));\n        \n        // THEN: Recovery suggestions should be accessible\n        match rate_limit_error.recovery() {\n            Some(Recovery::Retry { after, max_attempts }) =\u003e {\n                assert_eq!(after.as_secs(), 60);\n                assert_eq!(*max_attempts, 3);\n            }\n            _ =\u003e panic!(\"Expected Retry recovery\"),\n        }\n        \n        match auth_error.recovery() {\n            Some(Recovery::Manual(msg)) =\u003e {\n                assert!(msg.contains(\"API key\"));\n            }\n            _ =\u003e panic!(\"Expected Manual recovery\"),\n        }\n    }\n\n    #[test]\n    fn test_error_source_chain() {\n        // GIVEN: Nested errors with source chain\n        let io_error = io::Error::new(io::ErrorKind::PermissionDenied, \"Access denied\");\n        let config_error = ConfigError::Io(io_error);\n        let app_error = OpenCodeError::Configuration(config_error);\n        \n        // WHEN: We traverse the error source chain\n        let mut sources = vec![];\n        let mut current: Option\u003c\u0026dyn std::error::Error\u003e = Some(\u0026app_error);\n        \n        while let Some(err) = current {\n            sources.push(err.to_string());\n            current = err.source();\n        }\n        \n        // THEN: We should see the full error chain\n        assert_eq!(sources.len(), 3);\n        assert!(sources[0].contains(\"Configuration error\"));\n        assert!(sources[1].contains(\"IO error\"));\n        assert!(sources[2].contains(\"Access denied\"));\n    }\n\n    #[test]\n    fn test_error_categorization() {\n        // GIVEN: Various errors\n        let errors = vec![\n            OpenCodeError::Provider(ProviderError::RateLimitExceeded),\n            OpenCodeError::Network(NetworkError::ConnectionRefused),\n            OpenCodeError::Configuration(ConfigError::NotFound),\n            OpenCodeError::Internal(\"Unexpected state\".to_string()),\n        ];\n        \n        // WHEN: We categorize errors\n        for error in errors {\n            let category = error.category();\n            \n            // THEN: Each error should have appropriate category\n            match \u0026error {\n                OpenCodeError::Provider(ProviderError::RateLimitExceeded) =\u003e {\n                    assert_eq!(category, ErrorCategory::Transient);\n                }\n                OpenCodeError::Network(_) =\u003e {\n                    assert_eq!(category, ErrorCategory::Transient);\n                }\n                OpenCodeError::Configuration(_) =\u003e {\n                    assert_eq!(category, ErrorCategory::Configuration);\n                }\n                OpenCodeError::Internal(_) =\u003e {\n                    assert_eq!(category, ErrorCategory::Internal);\n                }\n                _ =\u003e {}\n            }\n        }\n    }\n\n    #[test]\n    fn test_error_retry_policy() {\n        // GIVEN: Errors with different retry policies\n        let transient_error = OpenCodeError::Network(NetworkError::Timeout {\n            operation: \"Request\".to_string(),\n            duration: std::time::Duration::from_secs(30),\n        });\n        \n        let permanent_error = OpenCodeError::Configuration(ConfigError::Validation(\n            \"Invalid setting\".to_string()\n        ));\n        \n        // WHEN: We check retry policies\n        let transient_policy = transient_error.retry_policy();\n        let permanent_policy = permanent_error.retry_policy();\n        \n        // THEN: Appropriate policies should be returned\n        match transient_policy {\n            RetryPolicy::Exponential { max_attempts, base_delay, .. } =\u003e {\n                assert_eq!(max_attempts, 3);\n                assert_eq!(base_delay.as_secs(), 1);\n            }\n            _ =\u003e panic!(\"Expected exponential retry for transient error\"),\n        }\n        \n        assert_eq!(permanent_policy, RetryPolicy::None);\n    }\n\n    #[test]\n    fn test_error_telemetry() {\n        // GIVEN: An error with telemetry data\n        let error = OpenCodeError::Provider(ProviderError::ApiError {\n            status: 500,\n            message: \"Internal server error\".to_string(),\n        })\n        .with_telemetry(ErrorTelemetry {\n            timestamp: std::time::SystemTime::now(),\n            request_id: Some(\"req-123\".to_string()),\n            user_id: Some(\"user-456\".to_string()),\n            additional_data: {\n                let mut map = std::collections::HashMap::new();\n                map.insert(\"provider\".to_string(), \"openai\".to_string());\n                map.insert(\"model\".to_string(), \"gpt-4\".to_string());\n                map\n            },\n        });\n        \n        // WHEN: We access telemetry data\n        let telemetry = error.telemetry().unwrap();\n        \n        // THEN: All telemetry fields should be accessible\n        assert_eq!(telemetry.request_id, Some(\"req-123\".to_string()));\n        assert_eq!(telemetry.user_id, Some(\"user-456\".to_string()));\n        assert_eq!(telemetry.additional_data.get(\"provider\"), Some(\u0026\"openai\".to_string()));\n        assert_eq!(telemetry.additional_data.get(\"model\"), Some(\u0026\"gpt-4\".to_string()));\n    }\n\n    #[test]\n    fn test_error_serialization() {\n        // GIVEN: An error that needs to be serialized\n        let error = OpenCodeError::Provider(ProviderError::ApiError {\n            status: 404,\n            message: \"Model not found\".to_string(),\n        })\n        .with_context(\"Calling OpenAI API\")\n        .with_recovery(Recovery::Fallback {\n            alternative: \"Use gpt-3.5-turbo instead\".to_string(),\n        });\n        \n        // WHEN: We serialize the error\n        let serialized = error.to_json();\n        \n        // THEN: JSON should contain all error information\n        let json: serde_json::Value = serde_json::from_str(\u0026serialized).unwrap();\n        assert_eq!(json[\"type\"], \"Provider\");\n        assert_eq!(json[\"message\"], \"Provider error: API error: Model not found (status: 404)\");\n        assert_eq!(json[\"contexts\"][0], \"Calling OpenAI API\");\n        assert_eq!(json[\"recovery\"][\"type\"], \"Fallback\");\n        assert_eq!(json[\"recovery\"][\"alternative\"], \"Use gpt-3.5-turbo instead\");\n    }\n\n    #[test]\n    fn test_error_aggregation() {\n        // GIVEN: Multiple errors that need to be aggregated\n        let errors = vec![\n            OpenCodeError::Provider(ProviderError::RateLimitExceeded),\n            OpenCodeError::Network(NetworkError::ConnectionRefused),\n            OpenCodeError::Provider(ProviderError::ApiError {\n                status: 500,\n                message: \"Server error\".to_string(),\n            }),\n        ];\n        \n        // WHEN: We aggregate errors\n        let aggregated = OpenCodeError::Multiple(errors);\n        \n        // THEN: All errors should be accessible\n        match \u0026aggregated {\n            OpenCodeError::Multiple(errs) =\u003e {\n                assert_eq!(errs.len(), 3);\n                // Check that we can iterate and handle each error\n                for (i, err) in errs.iter().enumerate() {\n                    match i {\n                        0 =\u003e assert!(matches!(err, OpenCodeError::Provider(ProviderError::RateLimitExceeded))),\n                        1 =\u003e assert!(matches!(err, OpenCodeError::Network(_))),\n                        2 =\u003e assert!(matches!(err, OpenCodeError::Provider(ProviderError::ApiError { .. }))),\n                        _ =\u003e panic!(\"Unexpected error count\"),\n                    }\n                }\n            }\n            _ =\u003e panic!(\"Expected Multiple error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_async_error_handling() {\n        // GIVEN: An async operation that might fail\n        async fn risky_operation() -\u003e Result\u003cString, OpenCodeError\u003e {\n            // Simulate async work\n            tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;\n            \n            Err(OpenCodeError::Network(NetworkError::Timeout {\n                operation: \"Async operation\".to_string(),\n                duration: std::time::Duration::from_secs(10),\n            }))\n        }\n        \n        // WHEN: We handle the error with async context\n        let result = risky_operation()\n            .await\n            .map_err(|e| e.with_context(\"Performing background task\"));\n        \n        // THEN: Error context should be preserved\n        assert!(result.is_err());\n        let error = result.unwrap_err();\n        assert!(error.contexts().contains(\u0026\"Performing background task\".to_string()));\n    }\n\n    #[test]\n    fn test_error_conversion() {\n        // GIVEN: Errors from external libraries\n        let io_error = io::Error::new(io::ErrorKind::NotFound, \"File not found\");\n        let parse_error = \"invalid digit found in string\".parse::\u003ci32\u003e().unwrap_err();\n        \n        // WHEN: We convert them to our error type\n        let our_io_error: OpenCodeError = io_error.into();\n        let our_parse_error: OpenCodeError = parse_error.into();\n        \n        // THEN: They should be properly wrapped\n        assert!(matches!(our_io_error, OpenCodeError::Io(_)));\n        assert!(matches!(our_parse_error, OpenCodeError::Parse(_)));\n    }\n}\n\n// Type definitions that will be moved to the actual implementation\nuse serde::{Serialize, Deserialize};\nuse std::collections::HashMap;\nuse thiserror::Error;\n\n#[derive(Debug, Error)]\npub enum OpenCodeError {\n    #[error(\"Provider error: {0}\")]\n    Provider(#[from] ProviderError),\n    \n    #[error(\"Configuration error: {0}\")]\n    Configuration(#[from] ConfigError),\n    \n    #[error(\"Network error: {0}\")]\n    Network(#[from] NetworkError),\n    \n    #[error(\"IO error: {0}\")]\n    Io(#[from] io::Error),\n    \n    #[error(\"Parse error: {0}\")]\n    Parse(#[from] std::num::ParseIntError),\n    \n    #[error(\"Internal error: {0}\")]\n    Internal(String),\n    \n    #[error(\"Multiple errors occurred: {0:?}\")]\n    Multiple(Vec\u003cOpenCodeError\u003e),\n}\n\n#[derive(Debug, Error)]\npub enum NetworkError {\n    #[error(\"Connection refused\")]\n    ConnectionRefused,\n    \n    #[error(\"Operation '{operation}' timed out after {duration:?}\")]\n    Timeout {\n        operation: String,\n        duration: std::time::Duration,\n    },\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub enum ErrorCategory {\n    Transient,\n    Configuration,\n    Internal,\n    External,\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub enum RetryPolicy {\n    None,\n    Exponential {\n        max_attempts: u32,\n        base_delay: std::time::Duration,\n        max_delay: std::time::Duration,\n    },\n    Fixed {\n        attempts: u32,\n        delay: std::time::Duration,\n    },\n}\n\n#[derive(Debug, Clone)]\npub enum Recovery {\n    Retry {\n        after: std::time::Duration,\n        max_attempts: u32,\n    },\n    Fallback {\n        alternative: String,\n    },\n    Manual(String),\n}\n\n#[derive(Debug, Clone)]\npub struct ErrorTelemetry {\n    pub timestamp: std::time::SystemTime,\n    pub request_id: Option\u003cString\u003e,\n    pub user_id: Option\u003cString\u003e,\n    pub additional_data: HashMap\u003cString, String\u003e,\n}\n\n#[derive(Serialize, Deserialize)]\nstruct SerializedError {\n    #[serde(rename = \"type\")]\n    error_type: String,\n    message: String,\n    contexts: Vec\u003cString\u003e,\n    recovery: Option\u003cSerializedRecovery\u003e,\n}\n\n#[derive(Serialize, Deserialize)]\nstruct SerializedRecovery {\n    #[serde(rename = \"type\")]\n    recovery_type: String,\n    #[serde(flatten)]\n    data: serde_json::Value,\n}\n\n// Error enhancement implementation\nstruct ErrorEnhancement {\n    contexts: Vec\u003cString\u003e,\n    recovery: Option\u003cRecovery\u003e,\n    telemetry: Option\u003cErrorTelemetry\u003e,\n}\n\n// Extension trait for error enhancement\nimpl OpenCodeError {\n    pub fn with_context(self, context: impl Into\u003cString\u003e) -\u003e Self {\n        // Implementation would store context\n        self\n    }\n    \n    pub fn with_recovery(self, recovery: Recovery) -\u003e Self {\n        // Implementation would store recovery\n        self\n    }\n    \n    pub fn with_telemetry(self, telemetry: ErrorTelemetry) -\u003e Self {\n        // Implementation would store telemetry\n        self\n    }\n    \n    pub fn contexts(\u0026self) -\u003e Vec\u003cString\u003e {\n        // Implementation would return stored contexts\n        vec![]\n    }\n    \n    pub fn recovery(\u0026self) -\u003e Option\u003c\u0026Recovery\u003e {\n        // Implementation would return stored recovery\n        None\n    }\n    \n    pub fn telemetry(\u0026self) -\u003e Option\u003c\u0026ErrorTelemetry\u003e {\n        // Implementation would return stored telemetry\n        None\n    }\n    \n    pub fn full_message(\u0026self) -\u003e String {\n        // Implementation would build full error message with context\n        self.to_string()\n    }\n    \n    pub fn category(\u0026self) -\u003e ErrorCategory {\n        match self {\n            Self::Provider(ProviderError::RateLimitExceeded) =\u003e ErrorCategory::Transient,\n            Self::Network(_) =\u003e ErrorCategory::Transient,\n            Self::Configuration(_) =\u003e ErrorCategory::Configuration,\n            Self::Internal(_) =\u003e ErrorCategory::Internal,\n            _ =\u003e ErrorCategory::External,\n        }\n    }\n    \n    pub fn retry_policy(\u0026self) -\u003e RetryPolicy {\n        match self.category() {\n            ErrorCategory::Transient =\u003e RetryPolicy::Exponential {\n                max_attempts: 3,\n                base_delay: std::time::Duration::from_secs(1),\n                max_delay: std::time::Duration::from_secs(60),\n            },\n            _ =\u003e RetryPolicy::None,\n        }\n    }\n    \n    pub fn to_json(\u0026self) -\u003e String {\n        // Implementation would serialize to JSON\n        serde_json::to_string_pretty(\u0026SerializedError {\n            error_type: \"Provider\".to_string(),\n            message: self.to_string(),\n            contexts: self.contexts(),\n            recovery: None,\n        }).unwrap()\n    }\n}","traces":[{"line":416,"address":[],"length":0,"stats":{"Line":0}},{"line":418,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":2},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","opencode-core","src","provider","tests.rs"],"content":"//! Tests for the Provider abstraction trait\n//! \n//! This test suite defines the expected behavior for AI provider implementations\n//! following TDD principles.\n\nuse super::*;\nuse async_trait::async_trait;\nuse mockall::*;\n\n#[cfg(test)]\nmod provider_trait_tests {\n    use super::*;\n\n    #[test]\n    fn test_provider_trait_requirements() {\n        // Verify that Provider trait has all required methods\n        fn assert_provider_trait\u003cT: Provider\u003e() {}\n        \n        // This test ensures the trait has the right shape\n        // Compilation will fail if trait requirements change\n    }\n\n    #[tokio::test]\n    async fn test_provider_completion() {\n        // GIVEN: A mock provider implementation\n        let mut mock_provider = MockProvider::new();\n        \n        // WHEN: We set up expectations for a completion request\n        mock_provider\n            .expect_complete()\n            .with(mockall::predicate::function(|req: \u0026CompletionRequest| {\n                req.messages.len() == 1 \u0026\u0026 \n                req.messages[0].role == MessageRole::User \u0026\u0026\n                req.messages[0].content == \"Hello, AI!\"\n            }))\n            .times(1)\n            .returning(|_| {\n                Ok(CompletionResponse {\n                    id: \"test-123\".to_string(),\n                    model: \"test-model\".to_string(),\n                    choices: vec![\n                        Choice {\n                            message: Message {\n                                role: MessageRole::Assistant,\n                                content: \"Hello! How can I help you?\".to_string(),\n                            },\n                            finish_reason: FinishReason::Stop,\n                            index: 0,\n                        }\n                    ],\n                    usage: Usage {\n                        prompt_tokens: 10,\n                        completion_tokens: 8,\n                        total_tokens: 18,\n                    },\n                    created: 1234567890,\n                })\n            });\n\n        // THEN: The provider should return the expected response\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![\n                Message {\n                    role: MessageRole::User,\n                    content: \"Hello, AI!\".to_string(),\n                }\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = mock_provider.complete(request).await.unwrap();\n        assert_eq!(response.id, \"test-123\");\n        assert_eq!(response.choices.len(), 1);\n        assert_eq!(response.choices[0].message.content, \"Hello! How can I help you?\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_streaming() {\n        // GIVEN: A mock provider that supports streaming\n        let mut mock_provider = MockProvider::new();\n        \n        // WHEN: We request a streaming completion\n        mock_provider\n            .expect_complete_stream()\n            .times(1)\n            .returning(|_| {\n                let (tx, rx) = tokio::sync::mpsc::channel(10);\n                \n                tokio::spawn(async move {\n                    // Simulate streaming chunks\n                    for chunk in [\"Hello\", \" from\", \" streaming\", \" AI!\"] {\n                        let _ = tx.send(Ok(StreamChunk {\n                            id: \"stream-123\".to_string(),\n                            choices: vec![\n                                StreamChoice {\n                                    delta: Delta {\n                                        content: Some(chunk.to_string()),\n                                        role: None,\n                                    },\n                                    index: 0,\n                                    finish_reason: None,\n                                }\n                            ],\n                            created: 1234567890,\n                        })).await;\n                    }\n                });\n                \n                Ok(Box::pin(tokio_stream::wrappers::ReceiverStream::new(rx)))\n            });\n\n        // THEN: We should receive all streaming chunks\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: MessageRole::User,\n                content: \"Stream this!\".to_string(),\n            }],\n            temperature: None,\n            max_tokens: None,\n            stream: true,\n        };\n\n        let mut stream = mock_provider.complete_stream(request).await.unwrap();\n        let mut full_response = String::new();\n        \n        while let Some(chunk_result) = stream.next().await {\n            let chunk = chunk_result.unwrap();\n            if let Some(content) = \u0026chunk.choices[0].delta.content {\n                full_response.push_str(content);\n            }\n        }\n        \n        assert_eq!(full_response, \"Hello from streaming AI!\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_error_handling() {\n        // GIVEN: A mock provider that returns errors\n        let mut mock_provider = MockProvider::new();\n        \n        // WHEN: The provider encounters an API error\n        mock_provider\n            .expect_complete()\n            .times(1)\n            .returning(|_| {\n                Err(ProviderError::ApiError {\n                    status: 429,\n                    message: \"Rate limit exceeded\".to_string(),\n                })\n            });\n\n        // THEN: The error should be properly propagated\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: MessageRole::User,\n                content: \"Test\".to_string(),\n            }],\n            temperature: None,\n            max_tokens: None,\n            stream: false,\n        };\n\n        let result = mock_provider.complete(request).await;\n        assert!(result.is_err());\n        \n        match result.unwrap_err() {\n            ProviderError::ApiError { status, message } =\u003e {\n                assert_eq!(status, 429);\n                assert_eq!(message, \"Rate limit exceeded\");\n            }\n            _ =\u003e panic!(\"Expected ApiError\"),\n        }\n    }\n\n    #[test]\n    fn test_provider_capabilities() {\n        // GIVEN: Different provider implementations\n        let mut mock_provider = MockProvider::new();\n        \n        // WHEN: We query provider capabilities\n        mock_provider\n            .expect_capabilities()\n            .times(1)\n            .returning(|| {\n                ProviderCapabilities {\n                    supports_streaming: true,\n                    supports_function_calling: true,\n                    supports_vision: false,\n                    max_tokens: 4096,\n                    models: vec![\n                        ModelInfo {\n                            id: \"gpt-4\".to_string(),\n                            display_name: \"GPT-4\".to_string(),\n                            context_window: 8192,\n                            max_output_tokens: 4096,\n                        }\n                    ],\n                }\n            });\n\n        // THEN: Capabilities should be correctly reported\n        let caps = mock_provider.capabilities();\n        assert!(caps.supports_streaming);\n        assert!(caps.supports_function_calling);\n        assert!(!caps.supports_vision);\n        assert_eq!(caps.max_tokens, 4096);\n        assert_eq!(caps.models.len(), 1);\n    }\n\n    #[test]\n    fn test_provider_configuration() {\n        // Test that providers can be configured with different settings\n        // This will be implemented based on the configuration trait\n    }\n}\n\n// Mock implementations for testing\n#[cfg(test)]\nmockall::mock! {\n    Provider {}\n    \n    #[async_trait]\n    impl Provider for Provider {\n        async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse, ProviderError\u003e;\n        async fn complete_stream(\u0026self, request: CompletionRequest) -\u003e Result\u003cPin\u003cBox\u003cdyn Stream\u003cItem = Result\u003cStreamChunk, ProviderError\u003e\u003e + Send\u003e\u003e, ProviderError\u003e;\n        fn capabilities(\u0026self) -\u003e ProviderCapabilities;\n        fn name(\u0026self) -\u003e \u0026str;\n    }\n}\n\n// Type definitions that will be moved to the actual implementation\n#[derive(Debug, Clone, PartialEq)]\npub enum MessageRole {\n    System,\n    User,\n    Assistant,\n}\n\n#[derive(Debug, Clone)]\npub struct Message {\n    pub role: MessageRole,\n    pub content: String,\n}\n\n#[derive(Debug, Clone)]\npub struct CompletionRequest {\n    pub model: String,\n    pub messages: Vec\u003cMessage\u003e,\n    pub temperature: Option\u003cf32\u003e,\n    pub max_tokens: Option\u003cu32\u003e,\n    pub stream: bool,\n}\n\n#[derive(Debug, Clone)]\npub struct CompletionResponse {\n    pub id: String,\n    pub model: String,\n    pub choices: Vec\u003cChoice\u003e,\n    pub usage: Usage,\n    pub created: i64,\n}\n\n#[derive(Debug, Clone)]\npub struct Choice {\n    pub message: Message,\n    pub finish_reason: FinishReason,\n    pub index: u32,\n}\n\n#[derive(Debug, Clone)]\npub enum FinishReason {\n    Stop,\n    Length,\n    FunctionCall,\n}\n\n#[derive(Debug, Clone)]\npub struct Usage {\n    pub prompt_tokens: u32,\n    pub completion_tokens: u32,\n    pub total_tokens: u32,\n}\n\n#[derive(Debug, Clone)]\npub struct StreamChunk {\n    pub id: String,\n    pub choices: Vec\u003cStreamChoice\u003e,\n    pub created: i64,\n}\n\n#[derive(Debug, Clone)]\npub struct StreamChoice {\n    pub delta: Delta,\n    pub index: u32,\n    pub finish_reason: Option\u003cFinishReason\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct Delta {\n    pub content: Option\u003cString\u003e,\n    pub role: Option\u003cMessageRole\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct ProviderCapabilities {\n    pub supports_streaming: bool,\n    pub supports_function_calling: bool,\n    pub supports_vision: bool,\n    pub max_tokens: u32,\n    pub models: Vec\u003cModelInfo\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct ModelInfo {\n    pub id: String,\n    pub display_name: String,\n    pub context_window: u32,\n    pub max_output_tokens: u32,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum ProviderError {\n    #[error(\"API error: {message} (status: {status})\")]\n    ApiError { status: u16, message: String },\n    \n    #[error(\"Network error: {0}\")]\n    NetworkError(String),\n    \n    #[error(\"Invalid request: {0}\")]\n    InvalidRequest(String),\n    \n    #[error(\"Rate limit exceeded\")]\n    RateLimitExceeded,\n    \n    #[error(\"Authentication failed: {0}\")]\n    AuthenticationError(String),\n}\n\n#[async_trait]\npub trait Provider: Send + Sync {\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse, ProviderError\u003e;\n    async fn complete_stream(\u0026self, request: CompletionRequest) -\u003e Result\u003cPin\u003cBox\u003cdyn Stream\u003cItem = Result\u003cStreamChunk, ProviderError\u003e\u003e + Send\u003e\u003e, ProviderError\u003e;\n    fn capabilities(\u0026self) -\u003e ProviderCapabilities;\n    fn name(\u0026self) -\u003e \u0026str;\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","config","mod.rs"],"content":"use crate::error::{Error, Result};\nuse serde::{Deserialize, Serialize};\nuse std::env;\nuse std::fs;\nuse std::path::Path;\n\n#[cfg(test)]\nmod tests;\n\n/// OpenAI configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OpenAIConfig {\n    pub default_model: String,\n    pub api_base: String,\n    pub max_retries: u32,\n    pub timeout_seconds: u32,\n}\n\nimpl Default for OpenAIConfig {\n    fn default() -\u003e Self {\n        Self {\n            default_model: \"gpt-4\".to_string(),\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        }\n    }\n}\n\n/// Main configuration structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Config {\n    pub openai: OpenAIConfig,\n}\n\nimpl Default for Config {\n    fn default() -\u003e Self {\n        Self {\n            openai: OpenAIConfig::default(),\n        }\n    }\n}\n\nimpl Config {\n    /// Load configuration from file and environment variables\n    /// Environment variables take precedence over file values\n    pub fn load\u003cP: AsRef\u003cPath\u003e\u003e(config_path: Option\u003cP\u003e) -\u003e Result\u003cSelf\u003e {\n        let mut config = if let Some(path) = config_path {\n            Self::from_file(path)?\n        } else {\n            Self::default()\n        };\n\n        // Override with environment variables\n        let env_config = Self::from_env()?;\n        config.merge_env(env_config);\n\n        Ok(config)\n    }\n\n    /// Load configuration from a TOML file\n    pub fn from_file\u003cP: AsRef\u003cPath\u003e\u003e(path: P) -\u003e Result\u003cSelf\u003e {\n        let content = fs::read_to_string(path)?;\n        let config: Config = toml::from_str(\u0026content)?;\n        Ok(config)\n    }\n\n    /// Load configuration from environment variables\n    pub fn from_env() -\u003e Result\u003cSelf\u003e {\n        let mut config = Self::default();\n\n        // OpenAI configuration\n        if let Ok(model) = env::var(\"OPENAI_MODEL\") {\n            config.openai.default_model = model;\n        }\n\n        if let Ok(api_base) = env::var(\"OPENAI_API_BASE\") {\n            config.openai.api_base = api_base;\n        }\n\n        if let Ok(max_retries) = env::var(\"OPENAI_MAX_RETRIES\") {\n            config.openai.max_retries = max_retries\n                .parse()\n                .map_err(|e| Error::Config(format!(\"Invalid OPENAI_MAX_RETRIES: {}\", e)))?;\n        }\n\n        if let Ok(timeout) = env::var(\"OPENAI_TIMEOUT\") {\n            config.openai.timeout_seconds = timeout\n                .parse()\n                .map_err(|e| Error::Config(format!(\"Invalid OPENAI_TIMEOUT: {}\", e)))?;\n        }\n\n        Ok(config)\n    }\n\n    /// Merge environment configuration into this config\n    /// Environment values take precedence\n    fn merge_env(\u0026mut self, env_config: Config) {\n        // Only update values that were actually set in environment\n        if env::var(\"OPENAI_MODEL\").is_ok() {\n            self.openai.default_model = env_config.openai.default_model;\n        }\n        if env::var(\"OPENAI_API_BASE\").is_ok() {\n            self.openai.api_base = env_config.openai.api_base;\n        }\n        if env::var(\"OPENAI_MAX_RETRIES\").is_ok() {\n            self.openai.max_retries = env_config.openai.max_retries;\n        }\n        if env::var(\"OPENAI_TIMEOUT\").is_ok() {\n            self.openai.timeout_seconds = env_config.openai.timeout_seconds;\n        }\n    }\n\n    /// Save configuration to a TOML file\n    pub fn save\u003cP: AsRef\u003cPath\u003e\u003e(\u0026self, path: P) -\u003e Result\u003c()\u003e {\n        let content = toml::to_string_pretty(self)\n            .map_err(|e| Error::Config(format!(\"Failed to serialize config: {}\", e)))?;\n        fs::write(path, content)?;\n        Ok(())\n    }\n}","traces":[{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":16},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","config","tests.rs"],"content":"use super::*;\nuse std::env;\nuse tempfile::NamedTempFile;\nuse std::io::Write;\n\n#[test]\nfn test_config_defaults() {\n    let config = Config::default();\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 3);\n    assert_eq!(config.openai.timeout_seconds, 30);\n}\n\n#[test]\nfn test_openai_config_defaults() {\n    let config = OpenAIConfig::default();\n    assert_eq!(config.default_model, \"gpt-4\");\n    assert_eq!(config.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.max_retries, 3);\n    assert_eq!(config.timeout_seconds, 30);\n}\n\n#[test]\nfn test_config_from_toml() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-3.5-turbo\"\napi_base = \"https://custom.openai.com/v1\"\nmax_retries = 5\ntimeout_seconds = 60\n\"#;\n\n    let config: Config = toml::from_str(toml_content).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-3.5-turbo\");\n    assert_eq!(config.openai.api_base, \"https://custom.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 5);\n    assert_eq!(config.openai.timeout_seconds, 60);\n}\n\n#[test]\nfn test_config_from_file() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4-turbo\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 2\ntimeout_seconds = 45\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    let config = Config::from_file(temp_file.path()).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4-turbo\");\n    assert_eq!(config.openai.max_retries, 2);\n    assert_eq!(config.openai.timeout_seconds, 45);\n}\n\n#[test]\nfn test_config_from_file_not_found() {\n    let result = Config::from_file(\"non_existent_file.toml\");\n    assert!(result.is_err());\n    match result {\n        Err(Error::Io(_)) =\u003e {}\n        _ =\u003e panic!(\"Expected IO error\"),\n    }\n}\n\n#[test]\nfn test_config_from_env() {\n    // Set environment variables\n    env::set_var(\"OPENAI_MODEL\", \"gpt-4-vision\");\n    env::set_var(\"OPENAI_API_BASE\", \"https://custom-api.com/v1\");\n    env::set_var(\"OPENAI_MAX_RETRIES\", \"7\");\n    env::set_var(\"OPENAI_TIMEOUT\", \"90\");\n\n    let config = Config::from_env().unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4-vision\");\n    assert_eq!(config.openai.api_base, \"https://custom-api.com/v1\");\n    assert_eq!(config.openai.max_retries, 7);\n    assert_eq!(config.openai.timeout_seconds, 90);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n}\n\n#[test]\nfn test_config_from_env_partial() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Only set some environment variables\n    env::set_var(\"OPENAI_MODEL\", \"gpt-3.5-turbo-16k\");\n\n    let config = Config::from_env().unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-3.5-turbo-16k\");\n    // Should use defaults for other values\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 3);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n}\n\n#[test]\nfn test_config_load_priority() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Test that environment variables override file values\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 3\ntimeout_seconds = 30\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    // Set environment variable\n    env::set_var(\"OPENAI_MODEL\", \"gpt-4-turbo\");\n\n    let config = Config::load(Some(temp_file.path())).unwrap();\n    // Environment variable should override file value\n    assert_eq!(config.openai.default_model, \"gpt-4-turbo\");\n    // File value should be used for non-overridden values\n    assert_eq!(config.openai.max_retries, 3);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n}\n\n#[test]\nfn test_config_load_file_only() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 4\ntimeout_seconds = 25\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    let config = Config::load(Some(temp_file.path())).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.max_retries, 4);\n    assert_eq!(config.openai.timeout_seconds, 25);\n}\n\n#[test]\nfn test_config_load_no_file() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Load with no file specified - should use defaults + env\n    env::set_var(\"OPENAI_MAX_RETRIES\", \"10\");\n\n    let config = Config::load::\u003c\u0026str\u003e(None).unwrap();\n    // Should use default for most values\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    // But use env var where set\n    assert_eq!(config.openai.max_retries, 10);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n}\n\n#[test]\nfn test_invalid_toml() {\n    let invalid_toml = r#\"\n[openai\ndefault_model = \"gpt-4\"\n\"#;\n\n    let result: std::result::Result\u003cConfig, toml::de::Error\u003e = toml::from_str(invalid_toml);\n    assert!(result.is_err());\n}\n\n#[test]\nfn test_config_serialization() {\n    let config = Config {\n        openai: OpenAIConfig {\n            default_model: \"gpt-4\".to_string(),\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        },\n    };\n\n    let toml_str = toml::to_string(\u0026config).unwrap();\n    assert!(toml_str.contains(\"default_model = \\\"gpt-4\\\"\"));\n    assert!(toml_str.contains(\"max_retries = 3\"));\n\n    // Round trip test\n    let parsed: Config = toml::from_str(\u0026toml_str).unwrap();\n    assert_eq!(parsed.openai.default_model, config.openai.default_model);\n    assert_eq!(parsed.openai.max_retries, config.openai.max_retries);\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","error.rs"],"content":"use std::fmt;\n\n/// Custom error type for the application\n#[derive(Debug)]\npub enum Error {\n    /// Configuration errors\n    Config(String),\n    /// Provider errors (API calls, network, etc.)\n    Provider(String),\n    /// Service container errors\n    Service(String),\n    /// IO errors\n    Io(std::io::Error),\n    /// Other errors\n    Other(String),\n}\n\nimpl fmt::Display for Error {\n    fn fmt(\u0026self, f: \u0026mut fmt::Formatter\u003c'_\u003e) -\u003e fmt::Result {\n        match self {\n            Error::Config(msg) =\u003e write!(f, \"Configuration error: {}\", msg),\n            Error::Provider(msg) =\u003e write!(f, \"Provider error: {}\", msg),\n            Error::Service(msg) =\u003e write!(f, \"Service error: {}\", msg),\n            Error::Io(err) =\u003e write!(f, \"IO error: {}\", err),\n            Error::Other(msg) =\u003e write!(f, \"Error: {}\", msg),\n        }\n    }\n}\n\nimpl std::error::Error for Error {\n    fn source(\u0026self) -\u003e Option\u003c\u0026(dyn std::error::Error + 'static)\u003e {\n        match self {\n            Error::Io(err) =\u003e Some(err),\n            _ =\u003e None,\n        }\n    }\n}\n\nimpl From\u003cstd::io::Error\u003e for Error {\n    fn from(err: std::io::Error) -\u003e Self {\n        Error::Io(err)\n    }\n}\n\nimpl From\u003ctoml::de::Error\u003e for Error {\n    fn from(err: toml::de::Error) -\u003e Self {\n        Error::Config(format!(\"TOML parsing error: {}\", err))\n    }\n}\n\nimpl From\u003cstd::env::VarError\u003e for Error {\n    fn from(err: std::env::VarError) -\u003e Self {\n        Error::Config(format!(\"Environment variable error: {}\", err))\n    }\n}\n\n/// Result type alias\npub type Result\u003cT\u003e = std::result::Result\u003cT, Error\u003e;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::error::Error as StdError;\n\n    #[test]\n    fn test_error_display() {\n        let err = Error::Config(\"Invalid API key\".to_string());\n        assert_eq!(err.to_string(), \"Configuration error: Invalid API key\");\n\n        let err = Error::Provider(\"API rate limit exceeded\".to_string());\n        assert_eq!(err.to_string(), \"Provider error: API rate limit exceeded\");\n\n        let err = Error::Service(\"Service not found\".to_string());\n        assert_eq!(err.to_string(), \"Service error: Service not found\");\n\n        let err = Error::Other(\"Unknown error\".to_string());\n        assert_eq!(err.to_string(), \"Error: Unknown error\");\n    }\n\n    #[test]\n    fn test_error_from_io() {\n        let io_err = std::io::Error::new(std::io::ErrorKind::NotFound, \"File not found\");\n        let err: Error = io_err.into();\n        assert!(matches!(err, Error::Io(_)));\n    }\n\n    #[test]\n    fn test_error_source() {\n        let io_err = std::io::Error::new(std::io::ErrorKind::PermissionDenied, \"Access denied\");\n        let err = Error::Io(io_err);\n        assert!(StdError::source(\u0026err).is_some());\n\n        let err = Error::Config(\"Bad config\".to_string());\n        assert!(StdError::source(\u0026err).is_none());\n    }\n\n    #[test]\n    fn test_error_from_env_var() {\n        let env_err = std::env::VarError::NotPresent;\n        let err: Error = env_err.into();\n        match err {\n            Error::Config(msg) =\u003e assert!(msg.contains(\"Environment variable error\")),\n            _ =\u003e panic!(\"Expected Config error\"),\n        }\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","lib.rs"],"content":"pub mod config;\npub mod error;\npub mod provider;\npub mod service;\n\nuse config::Config;\nuse error::Result;\nuse provider::{CompletionRequest, Message};\nuse service::ServiceContainer;\nuse std::sync::OnceLock;\n\nstatic SERVICE_CONTAINER: OnceLock\u003cServiceContainer\u003e = OnceLock::new();\n\n/// Initialize the global service container\npub fn init(config: Config) -\u003e Result\u003c()\u003e {\n    let container = ServiceContainer::new(config)?;\n    SERVICE_CONTAINER\n        .set(container)\n        .map_err(|_| error::Error::Service(\"Service container already initialized\".into()))?;\n    Ok(())\n}\n\n/// Get the global service container\npub fn get_service_container() -\u003e Result\u003c\u0026'static ServiceContainer\u003e {\n    SERVICE_CONTAINER\n        .get()\n        .ok_or_else(|| error::Error::Service(\"Service container not initialized\".into()))\n}\n\n/// Backward compatible ask function\npub async fn ask(prompt: \u0026str) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: container.config().openai.default_model.clone(),\n        messages: vec![Message {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }],\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n/// Ask with a specific model\npub async fn ask_with_model(prompt: \u0026str, model: \u0026str) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: model.to_string(),\n        messages: vec![Message {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }],\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n/// Ask with messages (conversation context)\npub async fn ask_with_messages(messages: Vec\u003cMessage\u003e) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: container.config().openai.default_model.clone(),\n        messages,\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::provider::tests::MockProvider;\n    use std::sync::Arc;\n\n    fn setup_test_container() -\u003e ServiceContainer {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n        \n        let mock_provider = Arc::new(MockProvider {\n            response: \"Test response from global\".to_string(),\n            should_fail: false,\n        });\n        \n        container.register_provider(\"mock\", mock_provider);\n        container\n    }\n\n    #[test]\n    fn test_init_and_get_container() {\n        // Reset for test\n        let config = Config::default();\n        \n        // This might fail if already initialized, but that's okay for tests\n        let _ = init(config);\n        \n        // Should be able to get the container\n        let result = get_service_container();\n        // In a real test environment, this might be initialized already\n        // so we just check it doesn't panic\n        assert!(result.is_ok() || result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_ask_backward_compatibility() {\n        // For this test, we'll test the ask function logic without global state\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_model_logic() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Test with specific model\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_messages_logic() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let messages = vec![\n            Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            },\n            Message {\n                role: \"assistant\".to_string(),\n                content: \"Hi there!\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"How are you?\".to_string(),\n            },\n        ];\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages,\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[test]\n    fn test_service_not_initialized() {\n        // Clear any existing container (this is a limitation of using OnceLock in tests)\n        // In practice, we'd use a different pattern for testability\n        \n        // This test verifies the error when service is not initialized\n        // The actual behavior depends on whether init() was called previously\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","provider","mod.rs"],"content":"use crate::error::{Error, Result};\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse serde::{Deserialize, Serialize};\n\n#[cfg(test)]\npub mod tests;\n\n/// Message in a conversation\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Message {\n    pub role: String,\n    pub content: String,\n}\n\n/// Request for LLM completion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompletionRequest {\n    pub model: String,\n    pub messages: Vec\u003cMessage\u003e,\n    pub temperature: Option\u003cf32\u003e,\n    pub max_tokens: Option\u003cu32\u003e,\n    pub stream: bool,\n}\n\n/// Response from LLM completion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompletionResponse {\n    pub content: String,\n    pub model: String,\n    pub usage: Usage,\n}\n\n/// Token usage information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Usage {\n    pub prompt_tokens: u32,\n    pub completion_tokens: u32,\n    pub total_tokens: u32,\n}\n\n/// Streaming chunk from LLM\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct StreamChunk {\n    pub delta: String,\n    pub finish_reason: Option\u003cString\u003e,\n}\n\n/// Trait for LLM providers\n#[async_trait]\npub trait LLMProvider: Send + Sync {\n    /// Get the name of the provider\n    fn name(\u0026self) -\u003e \u0026str;\n\n    /// Complete a request and return the full response\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e;\n\n    /// Stream a response\n    async fn stream(\n        \u0026self,\n        request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e;\n}\n\npub mod openai;\n\npub use openai::OpenAIProvider;","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","provider","openai.rs"],"content":"use super::*;\nuse crate::config::OpenAIConfig;\nuse async_openai::{\n    types::{\n        ChatCompletionRequestMessage, ChatCompletionRequestSystemMessageArgs,\n        ChatCompletionRequestUserMessageArgs, ChatCompletionRequestAssistantMessageArgs,\n        CreateChatCompletionRequestArgs, CreateChatCompletionStreamResponse,\n    },\n    Client,\n};\nuse futures::StreamExt;\n\n/// OpenAI provider implementation\npub struct OpenAIProvider {\n    client: Client\u003casync_openai::config::OpenAIConfig\u003e,\n    config: OpenAIConfig,\n}\n\nimpl OpenAIProvider {\n    /// Create a new OpenAI provider\n    pub fn new(api_key: String, config: OpenAIConfig) -\u003e Self {\n        let openai_config = async_openai::config::OpenAIConfig::new()\n            .with_api_key(api_key)\n            .with_api_base(config.api_base.clone());\n\n        Self {\n            client: Client::with_config(openai_config),\n            config,\n        }\n    }\n\n    fn convert_messages(\u0026self, messages: Vec\u003cMessage\u003e) -\u003e Vec\u003cChatCompletionRequestMessage\u003e {\n        messages\n            .into_iter()\n            .map(|msg| match msg.role.as_str() {\n                \"system\" =\u003e ChatCompletionRequestSystemMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n                \"assistant\" =\u003e ChatCompletionRequestAssistantMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n                _ =\u003e ChatCompletionRequestUserMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n            })\n            .collect()\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for OpenAIProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"openai\"\n    }\n\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e {\n        let mut builder = CreateChatCompletionRequestArgs::default();\n        builder\n            .model(\u0026request.model)\n            .messages(self.convert_messages(request.messages));\n\n        if let Some(temp) = request.temperature {\n            builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = request.max_tokens {\n            builder.max_tokens(max_tokens as u16);\n        }\n\n        let openai_request = builder\n            .build()\n            .map_err(|e| Error::Provider(format!(\"Failed to build request: {}\", e)))?;\n\n        let response = self\n            .client\n            .chat()\n            .create(openai_request)\n            .await\n            .map_err(|e| Error::Provider(format!(\"OpenAI API error: {}\", e)))?;\n\n        let content = response\n            .choices\n            .first()\n            .and_then(|c| c.message.content.as_ref())\n            .ok_or_else(|| Error::Provider(\"No content in response\".into()))?\n            .clone();\n\n        Ok(CompletionResponse {\n            content,\n            model: response.model,\n            usage: Usage {\n                prompt_tokens: response.usage.as_ref().map(|u| u.prompt_tokens).unwrap_or(0) as u32,\n                completion_tokens: response\n                    .usage\n                    .as_ref()\n                    .map(|u| u.completion_tokens)\n                    .unwrap_or(0) as u32,\n                total_tokens: response.usage.as_ref().map(|u| u.total_tokens).unwrap_or(0) as u32,\n            },\n        })\n    }\n\n    async fn stream(\n        \u0026self,\n        request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e {\n        let mut builder = CreateChatCompletionRequestArgs::default();\n        builder\n            .model(\u0026request.model)\n            .messages(self.convert_messages(request.messages))\n            .stream(true);\n\n        if let Some(temp) = request.temperature {\n            builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = request.max_tokens {\n            builder.max_tokens(max_tokens as u16);\n        }\n\n        let openai_request = builder\n            .build()\n            .map_err(|e| Error::Provider(format!(\"Failed to build request: {}\", e)))?;\n\n        let stream = self\n            .client\n            .chat()\n            .create_stream(openai_request)\n            .await\n            .map_err(|e| Error::Provider(format!(\"OpenAI API error: {}\", e)))?;\n\n        let mapped_stream = stream.map(|result| match result {\n            Ok(response) =\u003e {\n                let chunk = extract_chunk(response);\n                Ok(chunk)\n            }\n            Err(e) =\u003e Err(Error::Provider(format!(\"Stream error: {}\", e))),\n        });\n\n        Ok(Box::pin(mapped_stream))\n    }\n}\n\nfn extract_chunk(response: CreateChatCompletionStreamResponse) -\u003e StreamChunk {\n    let delta = response\n        .choices\n        .first()\n        .and_then(|c| c.delta.content.as_ref())\n        .map(|s| s.clone())\n        .unwrap_or_default();\n\n    let finish_reason = response\n        .choices\n        .first()\n        .and_then(|c| c.finish_reason.as_ref())\n        .map(|r| format!(\"{:?}\", r));\n\n    StreamChunk {\n        delta,\n        finish_reason,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_openai_provider_creation() {\n        let config = OpenAIConfig {\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            default_model: \"gpt-4\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n\n        let provider = OpenAIProvider::new(\"test-key\".to_string(), config.clone());\n        assert_eq!(provider.name(), \"openai\");\n        assert_eq!(provider.config.default_model, \"gpt-4\");\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let config = OpenAIConfig {\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            default_model: \"gpt-4\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n\n        let provider = OpenAIProvider::new(\"test-key\".to_string(), config);\n\n        let messages = vec![\n            Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            },\n            Message {\n                role: \"assistant\".to_string(),\n                content: \"Hi there!\".to_string(),\n            },\n        ];\n\n        let converted = provider.convert_messages(messages);\n        assert_eq!(converted.len(), 3);\n    }\n\n    #[test]\n    fn test_extract_chunk() {\n        // This would require mocking CreateChatCompletionStreamResponse\n        // which is complex due to the async-openai types\n        // For now, we'll focus on the integration tests\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","provider","tests.rs"],"content":"use super::*;\nuse async_trait::async_trait;\nuse tokio_stream::StreamExt;\n\n#[derive(Debug, Clone)]\npub struct MockProvider {\n    pub response: String,\n    pub should_fail: bool,\n}\n\n#[async_trait]\nimpl LLMProvider for MockProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"mock\"\n    }\n\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e {\n        if self.should_fail {\n            return Err(Error::Provider(\"Mock provider error\".into()));\n        }\n\n        Ok(CompletionResponse {\n            content: self.response.clone(),\n            model: request.model,\n            usage: Usage {\n                prompt_tokens: 10,\n                completion_tokens: 20,\n                total_tokens: 30,\n            },\n        })\n    }\n\n    async fn stream(\n        \u0026self,\n        _request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e {\n        if self.should_fail {\n            return Err(Error::Provider(\"Mock provider error\".into()));\n        }\n\n        let chunks = vec![\n            StreamChunk {\n                delta: self.response.clone(),\n                finish_reason: None,\n            },\n            StreamChunk {\n                delta: String::new(),\n                finish_reason: Some(\"stop\".to_string()),\n            },\n        ];\n\n        Ok(Box::pin(tokio_stream::iter(chunks.into_iter().map(Ok))))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_mock_provider_complete() {\n        let provider = MockProvider {\n            response: \"Test response\".to_string(),\n            should_fail: false,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = provider.complete(request.clone()).await.unwrap();\n        assert_eq!(response.content, \"Test response\");\n        assert_eq!(response.model, \"gpt-4\");\n        assert_eq!(response.usage.total_tokens, 30);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_complete_error() {\n        let provider = MockProvider {\n            response: String::new(),\n            should_fail: true,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: None,\n            max_tokens: None,\n            stream: false,\n        };\n\n        let result = provider.complete(request).await;\n        assert!(result.is_err());\n        match result {\n            Err(Error::Provider(msg)) =\u003e assert_eq!(msg, \"Mock provider error\"),\n            _ =\u003e panic!(\"Expected Provider error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_stream() {\n        let provider = MockProvider {\n            response: \"Streaming response\".to_string(),\n            should_fail: false,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            }],\n            temperature: Some(0.5),\n            max_tokens: Some(200),\n            stream: true,\n        };\n\n        let mut stream = provider.stream(request).await.unwrap();\n        \n        let mut chunks = Vec::new();\n        while let Some(chunk) = stream.next().await {\n            chunks.push(chunk.unwrap());\n        }\n\n        assert_eq!(chunks.len(), 2);\n        assert_eq!(chunks[0].delta, \"Streaming response\");\n        assert_eq!(chunks[0].finish_reason, None);\n        assert_eq!(chunks[1].delta, \"\");\n        assert_eq!(chunks[1].finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[tokio::test]\n    async fn test_provider_trait_methods() {\n        let provider = MockProvider {\n            response: \"Test\".to_string(),\n            should_fail: false,\n        };\n\n        assert_eq!(provider.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_message_construction() {\n        let msg = Message {\n            role: \"assistant\".to_string(),\n            content: \"I can help with that\".to_string(),\n        };\n\n        assert_eq!(msg.role, \"assistant\");\n        assert_eq!(msg.content, \"I can help with that\");\n    }\n\n    #[test]\n    fn test_completion_request_builder() {\n        let request = CompletionRequest {\n            model: \"gpt-3.5-turbo\".to_string(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are a coding assistant\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Write a hello world program\".to_string(),\n                },\n            ],\n            temperature: Some(0.8),\n            max_tokens: Some(1000),\n            stream: true,\n        };\n\n        assert_eq!(request.model, \"gpt-3.5-turbo\");\n        assert_eq!(request.messages.len(), 2);\n        assert_eq!(request.temperature, Some(0.8));\n        assert_eq!(request.max_tokens, Some(1000));\n        assert!(request.stream);\n    }\n\n    #[test]\n    fn test_usage_calculation() {\n        let usage = Usage {\n            prompt_tokens: 50,\n            completion_tokens: 100,\n            total_tokens: 150,\n        };\n\n        assert_eq!(usage.prompt_tokens, 50);\n        assert_eq!(usage.completion_tokens, 100);\n        assert_eq!(usage.total_tokens, 150);\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","service.rs"],"content":"use crate::config::Config;\nuse crate::error::{Error, Result};\nuse crate::provider::{LLMProvider, OpenAIProvider};\nuse std::collections::HashMap;\nuse std::sync::Arc;\n\n/// Service container for dependency injection\npub struct ServiceContainer {\n    providers: HashMap\u003cString, Arc\u003cdyn LLMProvider\u003e\u003e,\n    config: Config,\n}\n\nimpl ServiceContainer {\n    /// Create a new service container\n    pub fn new(config: Config) -\u003e Result\u003cSelf\u003e {\n        let mut container = Self {\n            providers: HashMap::new(),\n            config,\n        };\n\n        // Register default providers\n        container.register_default_providers()?;\n\n        Ok(container)\n    }\n\n    /// Register default providers based on configuration\n    fn register_default_providers(\u0026mut self) -\u003e Result\u003c()\u003e {\n        // Register OpenAI provider if API key is available\n        if let Ok(api_key) = std::env::var(\"OPENAI_API_KEY\") {\n            let provider = OpenAIProvider::new(api_key, self.config.openai.clone());\n            self.register_provider(\"openai\", Arc::new(provider));\n        }\n\n        Ok(())\n    }\n\n    /// Register a provider with the container\n    pub fn register_provider(\u0026mut self, name: \u0026str, provider: Arc\u003cdyn LLMProvider\u003e) {\n        self.providers.insert(name.to_string(), provider);\n    }\n\n    /// Get a provider by name\n    pub fn get_provider(\u0026self, name: \u0026str) -\u003e Result\u003cArc\u003cdyn LLMProvider\u003e\u003e {\n        self.providers\n            .get(name)\n            .cloned()\n            .ok_or_else(|| Error::Service(format!(\"Provider '{}' not found\", name)))\n    }\n\n    /// Get the default provider (first available)\n    pub fn get_default_provider(\u0026self) -\u003e Result\u003cArc\u003cdyn LLMProvider\u003e\u003e {\n        // Try OpenAI first as the default\n        if let Ok(provider) = self.get_provider(\"openai\") {\n            return Ok(provider);\n        }\n\n        // If no specific provider, return the first available\n        self.providers\n            .values()\n            .next()\n            .cloned()\n            .ok_or_else(|| Error::Service(\"No providers available\".into()))\n    }\n\n    /// List all registered provider names\n    pub fn list_providers(\u0026self) -\u003e Vec\u003cString\u003e {\n        self.providers.keys().cloned().collect()\n    }\n\n    /// Get the configuration\n    pub fn config(\u0026self) -\u003e \u0026Config {\n        \u0026self.config\n    }\n\n    /// Update the configuration and re-register providers\n    pub fn update_config(\u0026mut self, config: Config) -\u003e Result\u003c()\u003e {\n        self.config = config;\n        self.providers.clear();\n        self.register_default_providers()?;\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::provider::tests::MockProvider;\n\n    #[test]\n    fn test_service_container_creation() {\n        let config = Config::default();\n        let container = ServiceContainer::new(config).unwrap();\n        \n        // Should create without error\n        assert!(container.providers.is_empty() || !container.providers.is_empty());\n    }\n\n    #[test]\n    fn test_register_and_get_provider() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Test response\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"mock\", mock_provider.clone());\n\n        let retrieved = container.get_provider(\"mock\").unwrap();\n        assert_eq!(retrieved.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_get_provider_not_found() {\n        let config = Config::default();\n        let container = ServiceContainer::new(config).unwrap();\n\n        let result = container.get_provider(\"nonexistent\");\n        assert!(result.is_err());\n        match result {\n            Err(Error::Service(msg)) =\u003e assert!(msg.contains(\"not found\")),\n            _ =\u003e panic!(\"Expected Service error\"),\n        }\n    }\n\n    #[test]\n    fn test_list_providers() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Clear any existing providers first\n        container.providers.clear();\n\n        let mock1 = Arc::new(MockProvider {\n            response: \"Test1\".to_string(),\n            should_fail: false,\n        });\n        let mock2 = Arc::new(MockProvider {\n            response: \"Test2\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"mock1\", mock1);\n        container.register_provider(\"mock2\", mock2);\n\n        let providers = container.list_providers();\n        assert_eq!(providers.len(), 2);\n        assert!(providers.contains(\u0026\"mock1\".to_string()));\n        assert!(providers.contains(\u0026\"mock2\".to_string()));\n    }\n\n    #[test]\n    fn test_get_default_provider() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Clear any existing providers first\n        container.providers.clear();\n\n        // With no providers registered, should fail\n        let result = container.get_default_provider();\n        assert!(result.is_err());\n\n        // Register a provider\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Default\".to_string(),\n            should_fail: false,\n        });\n        container.register_provider(\"default\", mock_provider);\n\n        let default = container.get_default_provider().unwrap();\n        assert_eq!(default.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_config_access() {\n        let config = Config::default();\n        let original_model = config.openai.default_model.clone();\n        \n        let container = ServiceContainer::new(config).unwrap();\n        assert_eq!(container.config().openai.default_model, original_model);\n    }\n\n    #[test]\n    fn test_update_config() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mut new_config = Config::default();\n        new_config.openai.default_model = \"gpt-3.5-turbo\".to_string();\n\n        container.update_config(new_config).unwrap();\n        assert_eq!(container.config().openai.default_model, \"gpt-3.5-turbo\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_functionality() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Hello from service container\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"test\", mock_provider);\n\n        let provider = container.get_provider(\"test\").unwrap();\n        \n        let request = crate::provider::CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![crate::provider::Message {\n                role: \"user\".to_string(),\n                content: \"Test message\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Hello from service container\");\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","target","debug","build","mime_guess-9cec288f4dc4b10d","out","mime_types_generated.rs"],"content":"","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","tests","integration","end_to_end.rs"],"content":"use std::process::Command;\nuse std::time::Duration;\nuse tempfile::TempDir;\nuse tokio::time::timeout;\n\n/// End-to-end integration tests for the entire OpenCode system\n#[tokio::test]\nasync fn test_full_workflow() {\n    // Create a temporary directory for testing\n    let temp_dir = TempDir::new().expect(\"Failed to create temp directory\");\n    let temp_path = temp_dir.path();\n\n    // Test CLI basic functionality\n    let output = Command::new(\"cargo\")\n        .args(\u0026[\"run\", \"--bin\", \"opencode\", \"--\", \"--version\"])\n        .output()\n        .expect(\"Failed to execute opencode CLI\");\n\n    assert!(output.status.success(), \"CLI version command failed\");\n    assert!(\n        String::from_utf8_lossy(\u0026output.stdout).contains(\"opencode\"),\n        \"Version output doesn't contain 'opencode'\"\n    );\n\n    // Test help command\n    let output = Command::new(\"cargo\")\n        .args(\u0026[\"run\", \"--bin\", \"opencode\", \"--\", \"--help\"])\n        .output()\n        .expect(\"Failed to execute opencode CLI help\");\n\n    assert!(output.status.success(), \"CLI help command failed\");\n    let help_output = String::from_utf8_lossy(\u0026output.stdout);\n    assert!(help_output.contains(\"USAGE\"), \"Help output doesn't contain usage information\");\n}\n\n#[tokio::test]\nasync fn test_cli_core_integration() {\n    // Test that CLI can properly initialize the core library\n    let output = Command::new(\"cargo\")\n        .args(\u0026[\"run\", \"--bin\", \"opencode\", \"--\", \"init\", \"--dry-run\"])\n        .output()\n        .expect(\"Failed to execute opencode CLI init\");\n\n    // Should succeed even in dry-run mode\n    assert!(output.status.success() || output.status.code() == Some(1), \n           \"CLI init command failed unexpectedly: {:?}\", \n           String::from_utf8_lossy(\u0026output.stderr));\n}\n\n#[tokio::test]\nasync fn test_provider_integration() {\n    use opencode_core::provider::{Provider, ProviderManager};\n    use opencode_core::config::Config;\n\n    // Test provider initialization and basic functionality\n    let config = Config::default();\n    let manager = ProviderManager::new(config);\n    \n    // Verify provider registration works\n    assert!(manager.is_healthy().await, \"Provider manager should be healthy\");\n}\n\n#[tokio::test]\nasync fn test_supervisor_agent_coordination() {\n    use opencode_core::supervisor::Supervisor;\n    use opencode_core::config::Config;\n    use tokio::time::Duration;\n\n    let config = Config::default();\n    let supervisor = Supervisor::new(config);\n\n    // Test basic supervisor functionality\n    let health_check = timeout(Duration::from_secs(5), supervisor.health_check()).await;\n    assert!(health_check.is_ok(), \"Supervisor health check timed out\");\n    assert!(health_check.unwrap().is_ok(), \"Supervisor health check failed\");\n}\n\n#[tokio::test]\nasync fn test_swarm_orchestration() {\n    use opencode_core::swarm::SwarmOrchestrator;\n    use opencode_core::config::Config;\n\n    let config = Config::default();\n    let orchestrator = SwarmOrchestrator::new(config);\n\n    // Test swarm initialization\n    assert!(orchestrator.is_healthy().await, \"Swarm orchestrator should be healthy\");\n}\n\n#[cfg(feature = \"container-tests\")]\n#[tokio::test]\nasync fn test_container_isolation() {\n    // This test would require Docker to be available\n    // Test container creation, isolation, and cleanup\n    \n    // Note: This is a placeholder for container integration tests\n    // In a real implementation, this would:\n    // 1. Start a container\n    // 2. Execute code inside it\n    // 3. Verify isolation\n    // 4. Clean up resources\n    \n    println!(\"Container isolation tests require Docker runtime\");\n}\n\n#[tokio::test]\nasync fn test_error_handling_and_recovery() {\n    use opencode_core::error::OpenCodeError;\n    \n    // Test various error conditions and recovery mechanisms\n    \n    // Test invalid configuration\n    let result = std::panic::catch_unwind(|| {\n        // This should handle errors gracefully\n        let _config = opencode_core::config::Config::from_file(\"nonexistent.toml\");\n    });\n    assert!(result.is_ok(), \"Error handling should not panic\");\n}\n\n#[tokio::test]\nasync fn test_concurrent_operations() {\n    use opencode_core::config::Config;\n    use std::sync::Arc;\n    use tokio::task::JoinSet;\n\n    let config = Arc::new(Config::default());\n    let mut join_set = JoinSet::new();\n\n    // Spawn multiple concurrent operations\n    for i in 0..5 {\n        let config_clone = Arc::clone(\u0026config);\n        join_set.spawn(async move {\n            // Simulate concurrent work\n            tokio::time::sleep(Duration::from_millis(100 * i)).await;\n            format!(\"Operation {} completed\", i)\n        });\n    }\n\n    // Wait for all operations to complete\n    let mut results = Vec::new();\n    while let Some(result) = join_set.join_next().await {\n        results.push(result.expect(\"Task should complete successfully\"));\n    }\n\n    assert_eq!(results.len(), 5, \"All concurrent operations should complete\");\n}\n\n#[tokio::test]\nasync fn test_performance_benchmarks() {\n    use std::time::Instant;\n    use opencode_core::config::Config;\n\n    let config = Config::default();\n    \n    // Basic performance benchmark\n    let start = Instant::now();\n    \n    // Simulate some work\n    for _ in 0..1000 {\n        let _ = serde_json::to_string(\u0026config);\n    }\n    \n    let duration = start.elapsed();\n    assert!(duration.as_millis() \u003c 1000, \"Performance test should complete within 1 second\");\n}\n\n#[tokio::test]\nasync fn test_memory_usage() {\n    // Test for memory leaks and excessive memory usage\n    let initial_memory = get_memory_usage();\n    \n    // Perform operations that might leak memory\n    for _ in 0..100 {\n        let config = opencode_core::config::Config::default();\n        drop(config);\n    }\n    \n    // Force garbage collection\n    #[cfg(feature = \"jemalloc\")]\n    {\n        // If using jemalloc, we could trigger collection here\n    }\n    \n    let final_memory = get_memory_usage();\n    let memory_growth = final_memory.saturating_sub(initial_memory);\n    \n    // Allow some memory growth but flag excessive growth\n    assert!(memory_growth \u003c 10_000_000, \"Memory usage grew too much: {} bytes\", memory_growth);\n}\n\nfn get_memory_usage() -\u003e usize {\n    // Simplified memory usage check\n    // In a real implementation, this would use proper memory profiling\n    std::mem::size_of::\u003copencode_core::config::Config\u003e()\n}\n\n#[tokio::test]\nasync fn test_cross_platform_compatibility() {\n    // Test platform-specific functionality\n    \n    #[cfg(target_os = \"windows\")]\n    {\n        // Windows-specific tests\n        test_windows_specific_features().await;\n    }\n    \n    #[cfg(target_os = \"macos\")]\n    {\n        // macOS-specific tests\n        test_macos_specific_features().await;\n    }\n    \n    #[cfg(target_os = \"linux\")]\n    {\n        // Linux-specific tests\n        test_linux_specific_features().await;\n    }\n}\n\n#[cfg(target_os = \"windows\")]\nasync fn test_windows_specific_features() {\n    // Test Windows path handling, permissions, etc.\n    use std::path::Path;\n    let path = Path::new(\"C:\\\\Windows\\\\System32\");\n    assert!(path.is_absolute(), \"Windows absolute path should be recognized\");\n}\n\n#[cfg(target_os = \"macos\")]\nasync fn test_macos_specific_features() {\n    // Test macOS-specific functionality\n    use std::path::Path;\n    let path = Path::new(\"/Applications\");\n    // This path should exist on macOS\n}\n\n#[cfg(target_os = \"linux\")]\nasync fn test_linux_specific_features() {\n    // Test Linux-specific functionality\n    use std::path::Path;\n    let path = Path::new(\"/usr/bin\");\n    assert!(path.exists() || !path.exists(), \"Path check should not panic\");\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","tests","integration","gui_backend_ipc.rs"],"content":"use crate::integration::utils;\nuse serde_json::json;\nuse std::time::Duration;\nuse tokio::time::timeout;\n\n/// Test GUI ↔ Backend IPC communication\n#[tokio::test]\nasync fn test_gui_backend_communication() {\n    crate::integration::init_test_env();\n    \n    // Skip this test in CI if GUI dependencies aren't available\n    if utils::is_ci_environment() {\n        println!(\"Skipping GUI tests in CI environment\");\n        return;\n    }\n    \n    // Test basic IPC message passing\n    test_ipc_message_serialization().await;\n    test_ipc_command_handling().await;\n    test_ipc_event_streaming().await;\n}\n\nasync fn test_ipc_message_serialization() {\n    // Test that GUI messages can be properly serialized/deserialized\n    \n    let test_message = json!({\n        \"type\": \"command\",\n        \"payload\": {\n            \"action\": \"start_agent\",\n            \"params\": {\n                \"name\": \"test-agent\",\n                \"provider\": \"openai\"\n            }\n        }\n    });\n    \n    // Test serialization\n    let serialized = serde_json::to_string(\u0026test_message)\n        .expect(\"Should serialize message\");\n    \n    // Test deserialization\n    let deserialized: serde_json::Value = serde_json::from_str(\u0026serialized)\n        .expect(\"Should deserialize message\");\n    \n    assert_eq!(test_message, deserialized, \"Message should roundtrip correctly\");\n}\n\nasync fn test_ipc_command_handling() {\n    // Test that commands sent from GUI are properly handled by backend\n    \n    // This would typically involve:\n    // 1. Starting the backend service\n    // 2. Connecting from GUI component\n    // 3. Sending commands\n    // 4. Verifying responses\n    \n    // Placeholder test - in real implementation would test actual IPC\n    let command = json!({\n        \"type\": \"get_status\",\n        \"id\": \"test-123\"\n    });\n    \n    // Simulate command processing\n    let response = process_command(command).await;\n    \n    assert!(response.is_ok(), \"Command processing should succeed\");\n}\n\nasync fn test_ipc_event_streaming() {\n    // Test real-time event streaming from backend to GUI\n    \n    let events = vec![\n        json!({\"type\": \"agent_started\", \"agent_id\": \"agent-1\"}),\n        json!({\"type\": \"agent_completed\", \"agent_id\": \"agent-1\", \"result\": \"success\"}),\n    ];\n    \n    for event in events {\n        let processed = process_event(event.clone()).await;\n        assert!(processed.is_ok(), \"Event processing should succeed for: {:?}\", event);\n    }\n}\n\n#[tokio::test]\nasync fn test_gui_state_synchronization() {\n    // Test that GUI state stays synchronized with backend state\n    \n    // Simulate backend state changes\n    let initial_state = json!({\n        \"agents\": [],\n        \"containers\": [],\n        \"status\": \"idle\"\n    });\n    \n    let updated_state = json!({\n        \"agents\": [{\"id\": \"agent-1\", \"status\": \"running\"}],\n        \"containers\": [{\"id\": \"container-1\", \"status\": \"active\"}],\n        \"status\": \"busy\"\n    });\n    \n    // Test state updates\n    assert_ne!(initial_state, updated_state, \"States should be different\");\n    \n    // In real implementation, would verify GUI reflects these changes\n}\n\n#[tokio::test]\nasync fn test_error_handling_in_ipc() {\n    // Test error handling in IPC communication\n    \n    let invalid_command = json!({\n        \"type\": \"invalid_command\",\n        \"malformed\": true\n    });\n    \n    let result = process_command(invalid_command).await;\n    assert!(result.is_err(), \"Invalid commands should return errors\");\n}\n\n#[tokio::test] \nasync fn test_ipc_performance() {\n    // Test IPC performance under load\n    \n    let start = std::time::Instant::now();\n    let message_count = 1000;\n    \n    for i in 0..message_count {\n        let message = json!({\n            \"type\": \"ping\",\n            \"id\": i,\n            \"timestamp\": chrono::Utc::now().timestamp()\n        });\n        \n        let result = timeout(Duration::from_millis(10), process_command(message)).await;\n        assert!(result.is_ok(), \"Command {} should complete within timeout\", i);\n    }\n    \n    let duration = start.elapsed();\n    let messages_per_sec = message_count as f64 / duration.as_secs_f64();\n    \n    assert!(messages_per_sec \u003e 100.0, \"Should process at least 100 messages/sec, got {:.2}\", messages_per_sec);\n}\n\n// Helper functions for testing (would be replaced with actual IPC in real implementation)\n\nasync fn process_command(command: serde_json::Value) -\u003e Result\u003cserde_json::Value, Box\u003cdyn std::error::Error\u003e\u003e {\n    // Simulate command processing\n    tokio::time::sleep(Duration::from_millis(1)).await;\n    \n    match command.get(\"type\").and_then(|t| t.as_str()) {\n        Some(\"get_status\") =\u003e Ok(json!({\n            \"status\": \"ok\",\n            \"response\": {\n                \"agents\": [],\n                \"uptime\": 3600\n            }\n        })),\n        Some(\"ping\") =\u003e Ok(json!({\n            \"type\": \"pong\",\n            \"timestamp\": chrono::Utc::now().timestamp()\n        })),\n        Some(\"invalid_command\") =\u003e Err(\"Invalid command type\".into()),\n        _ =\u003e Ok(json!({\"status\": \"unknown\"}))\n    }\n}\n\nasync fn process_event(event: serde_json::Value) -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {\n    // Simulate event processing\n    tokio::time::sleep(Duration::from_millis(1)).await;\n    \n    if event.get(\"type\").is_some() {\n        Ok(())\n    } else {\n        Err(\"Invalid event format\".into())\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","tests","integration","mod.rs"],"content":"//! Integration tests for OpenCode-RS\n//! \n//! This module contains comprehensive integration tests that verify\n//! the interaction between different components of the system.\n\npub mod end_to_end;\npub mod gui_backend_ipc;\npub mod multi_agent_coordination;\npub mod container_isolation;\n\nuse std::sync::Once;\n\nstatic INIT: Once = Once::new();\n\n/// Initialize test environment\npub fn init_test_env() {\n    INIT.call_once(|| {\n        // Set up logging for tests\n        env_logger::init();\n        \n        // Set test-specific environment variables\n        std::env::set_var(\"RUST_LOG\", \"debug\");\n        std::env::set_var(\"OPENCODE_TEST_MODE\", \"true\");\n    });\n}\n\n/// Common test utilities\npub mod utils {\n    use std::path::PathBuf;\n    use tempfile::TempDir;\n    \n    pub fn create_test_workspace() -\u003e (TempDir, PathBuf) {\n        let temp_dir = TempDir::new().expect(\"Failed to create temp directory\");\n        let workspace_path = temp_dir.path().to_path_buf();\n        (temp_dir, workspace_path)\n    }\n    \n    pub fn is_ci_environment() -\u003e bool {\n        std::env::var(\"CI\").is_ok() || std::env::var(\"GITHUB_ACTIONS\").is_ok()\n    }\n    \n    pub async fn wait_for_service(port: u16, timeout_secs: u64) -\u003e bool {\n        use tokio::time::{timeout, Duration};\n        use tokio::net::TcpStream;\n        \n        let result = timeout(\n            Duration::from_secs(timeout_secs),\n            async {\n                loop {\n                    if TcpStream::connect(format!(\"127.0.0.1:{}\", port)).await.is_ok() {\n                        return true;\n                    }\n                    tokio::time::sleep(Duration::from_millis(100)).await;\n                }\n            }\n        ).await;\n        \n        result.unwrap_or(false)\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","tests","integration","multi_agent_coordination.rs"],"content":"use crate::integration::utils;\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::Duration;\nuse tokio::sync::RwLock;\nuse tokio::time::timeout;\n\n/// Test multi-agent coordination and orchestration\n#[tokio::test]\nasync fn test_multi_agent_workflow() {\n    crate::integration::init_test_env();\n    \n    let coordinator = AgentCoordinator::new();\n    \n    // Test agent registration\n    test_agent_registration(\u0026coordinator).await;\n    \n    // Test task distribution\n    test_task_distribution(\u0026coordinator).await;\n    \n    // Test agent communication\n    test_agent_communication(\u0026coordinator).await;\n    \n    // Test failure handling\n    test_failure_handling(\u0026coordinator).await;\n}\n\nasync fn test_agent_registration(coordinator: \u0026AgentCoordinator) {\n    // Test registering multiple agents\n    let agent_ids = vec![\"agent-1\", \"agent-2\", \"agent-3\"];\n    \n    for agent_id in \u0026agent_ids {\n        let result = coordinator.register_agent(agent_id, AgentType::Worker).await;\n        assert!(result.is_ok(), \"Agent registration should succeed for {}\", agent_id);\n    }\n    \n    let registered_agents = coordinator.list_agents().await;\n    assert_eq!(registered_agents.len(), 3, \"Should have 3 registered agents\");\n    \n    for agent_id in \u0026agent_ids {\n        assert!(registered_agents.contains_key(*agent_id), \"Agent {} should be registered\", agent_id);\n    }\n}\n\nasync fn test_task_distribution(coordinator: \u0026AgentCoordinator) {\n    // Create some tasks\n    let tasks = vec![\n        Task::new(\"task-1\", TaskType::Compute, \"echo 'hello'\"),\n        Task::new(\"task-2\", TaskType::Compute, \"echo 'world'\"),\n        Task::new(\"task-3\", TaskType::IO, \"ls -la\"),\n    ];\n    \n    // Distribute tasks to agents\n    for task in tasks {\n        let result = coordinator.distribute_task(task).await;\n        assert!(result.is_ok(), \"Task distribution should succeed\");\n    }\n    \n    // Wait for tasks to complete\n    let completion_result = timeout(\n        Duration::from_secs(30),\n        coordinator.wait_for_completion()\n    ).await;\n    \n    assert!(completion_result.is_ok(), \"Tasks should complete within timeout\");\n}\n\nasync fn test_agent_communication(coordinator: \u0026AgentCoordinator) {\n    // Test agent-to-agent communication\n    let sender = \"agent-1\";\n    let receiver = \"agent-2\";\n    let message = \"test message\";\n    \n    let result = coordinator.send_message(sender, receiver, message).await;\n    assert!(result.is_ok(), \"Agent communication should succeed\");\n    \n    // Verify message was received\n    let messages = coordinator.get_messages_for_agent(receiver).await;\n    assert!(!messages.is_empty(), \"Receiver should have messages\");\n    assert_eq!(messages[0].content, message, \"Message content should match\");\n}\n\nasync fn test_failure_handling(coordinator: \u0026AgentCoordinator) {\n    // Simulate agent failure\n    let failing_agent = \"agent-3\";\n    coordinator.simulate_agent_failure(failing_agent).await;\n    \n    // Create a task that would be assigned to the failed agent\n    let task = Task::new(\"recovery-task\", TaskType::Compute, \"echo 'recovery'\");\n    let result = coordinator.distribute_task(task).await;\n    \n    // Should succeed by reassigning to healthy agent\n    assert!(result.is_ok(), \"Task should be reassigned to healthy agent\");\n    \n    // Test agent recovery\n    let recovery_result = coordinator.recover_agent(failing_agent).await;\n    assert!(recovery_result.is_ok(), \"Agent recovery should succeed\");\n}\n\n#[tokio::test]\nasync fn test_load_balancing() {\n    crate::integration::init_test_env();\n    \n    let coordinator = AgentCoordinator::new();\n    \n    // Register multiple agents\n    for i in 1..=5 {\n        coordinator.register_agent(\u0026format!(\"agent-{}\", i), AgentType::Worker).await.unwrap();\n    }\n    \n    // Create many tasks\n    let mut tasks = Vec::new();\n    for i in 1..=20 {\n        tasks.push(Task::new(\u0026format!(\"task-{}\", i), TaskType::Compute, \"sleep 1\"));\n    }\n    \n    // Distribute all tasks\n    let start = std::time::Instant::now();\n    for task in tasks {\n        coordinator.distribute_task(task).await.unwrap();\n    }\n    \n    // Wait for completion\n    coordinator.wait_for_completion().await.unwrap();\n    let duration = start.elapsed();\n    \n    // With 5 agents and 20 tasks, should complete in ~4 seconds (assuming 1 sec per task)\n    assert!(duration.as_secs() \u003c 10, \"Load balancing should distribute work efficiently\");\n    \n    // Verify load was distributed\n    let agent_loads = coordinator.get_agent_loads().await;\n    let max_load = agent_loads.values().max().unwrap_or(\u00260);\n    let min_load = agent_loads.values().min().unwrap_or(\u00260);\n    \n    // Load should be reasonably balanced (within 2 tasks)\n    assert!(max_load - min_load \u003c= 2, \"Load should be balanced across agents\");\n}\n\n#[tokio::test]\nasync fn test_hierarchical_coordination() {\n    crate::integration::init_test_env();\n    \n    let coordinator = AgentCoordinator::new();\n    \n    // Create hierarchical agent structure\n    coordinator.register_agent(\"supervisor\", AgentType::Supervisor).await.unwrap();\n    coordinator.register_agent(\"worker-1\", AgentType::Worker).await.unwrap();\n    coordinator.register_agent(\"worker-2\", AgentType::Worker).await.unwrap();\n    \n    // Set up hierarchy\n    coordinator.set_supervisor(\"supervisor\", vec![\"worker-1\", \"worker-2\"]).await.unwrap();\n    \n    // Create a complex task that requires coordination\n    let complex_task = Task::new(\"complex-task\", TaskType::Coordinated, \"multi-step-process\");\n    \n    let result = coordinator.distribute_task(complex_task).await;\n    assert!(result.is_ok(), \"Complex task should be handled by supervisor\");\n    \n    // Verify subtasks were created and distributed\n    let subtasks = coordinator.get_subtasks(\"complex-task\").await;\n    assert!(!subtasks.is_empty(), \"Complex task should generate subtasks\");\n}\n\n// Mock implementations for testing\n\n#[derive(Clone)]\nstruct AgentCoordinator {\n    agents: Arc\u003cRwLock\u003cHashMap\u003cString, Agent\u003e\u003e\u003e,\n    tasks: Arc\u003cRwLock\u003cVec\u003cTask\u003e\u003e\u003e,\n    messages: Arc\u003cRwLock\u003cHashMap\u003cString, Vec\u003cMessage\u003e\u003e\u003e\u003e,\n}\n\nimpl AgentCoordinator {\n    fn new() -\u003e Self {\n        Self {\n            agents: Arc::new(RwLock::new(HashMap::new())),\n            tasks: Arc::new(RwLock::new(Vec::new())),\n            messages: Arc::new(RwLock::new(HashMap::new())),\n        }\n    }\n    \n    async fn register_agent(\u0026self, id: \u0026str, agent_type: AgentType) -\u003e Result\u003c(), String\u003e {\n        let mut agents = self.agents.write().await;\n        agents.insert(id.to_string(), Agent::new(id, agent_type));\n        Ok(())\n    }\n    \n    async fn list_agents(\u0026self) -\u003e HashMap\u003cString, Agent\u003e {\n        self.agents.read().await.clone()\n    }\n    \n    async fn distribute_task(\u0026self, task: Task) -\u003e Result\u003c(), String\u003e {\n        let mut tasks = self.tasks.write().await;\n        tasks.push(task);\n        \n        // Simulate task processing\n        tokio::time::sleep(Duration::from_millis(10)).await;\n        Ok(())\n    }\n    \n    async fn wait_for_completion(\u0026self) -\u003e Result\u003c(), String\u003e {\n        // Simulate waiting for all tasks to complete\n        tokio::time::sleep(Duration::from_millis(100)).await;\n        Ok(())\n    }\n    \n    async fn send_message(\u0026self, from: \u0026str, to: \u0026str, content: \u0026str) -\u003e Result\u003c(), String\u003e {\n        let mut messages = self.messages.write().await;\n        let agent_messages = messages.entry(to.to_string()).or_insert_with(Vec::new);\n        agent_messages.push(Message {\n            from: from.to_string(),\n            to: to.to_string(),\n            content: content.to_string(),\n            timestamp: chrono::Utc::now(),\n        });\n        Ok(())\n    }\n    \n    async fn get_messages_for_agent(\u0026self, agent_id: \u0026str) -\u003e Vec\u003cMessage\u003e {\n        let messages = self.messages.read().await;\n        messages.get(agent_id).cloned().unwrap_or_default()\n    }\n    \n    async fn simulate_agent_failure(\u0026self, agent_id: \u0026str) {\n        let mut agents = self.agents.write().await;\n        if let Some(agent) = agents.get_mut(agent_id) {\n            agent.status = AgentStatus::Failed;\n        }\n    }\n    \n    async fn recover_agent(\u0026self, agent_id: \u0026str) -\u003e Result\u003c(), String\u003e {\n        let mut agents = self.agents.write().await;\n        if let Some(agent) = agents.get_mut(agent_id) {\n            agent.status = AgentStatus::Ready;\n            Ok(())\n        } else {\n            Err(\"Agent not found\".to_string())\n        }\n    }\n    \n    async fn get_agent_loads(\u0026self) -\u003e HashMap\u003cString, usize\u003e {\n        let agents = self.agents.read().await;\n        agents.iter()\n            .map(|(id, agent)| (id.clone(), agent.load))\n            .collect()\n    }\n    \n    async fn set_supervisor(\u0026self, supervisor_id: \u0026str, worker_ids: Vec\u003c\u0026str\u003e) -\u003e Result\u003c(), String\u003e {\n        // Simulate setting up hierarchical relationships\n        Ok(())\n    }\n    \n    async fn get_subtasks(\u0026self, task_id: \u0026str) -\u003e Vec\u003cTask\u003e {\n        // Simulate getting subtasks for a complex task\n        vec![\n            Task::new(\u0026format!(\"{}-subtask-1\", task_id), TaskType::Compute, \"step 1\"),\n            Task::new(\u0026format!(\"{}-subtask-2\", task_id), TaskType::Compute, \"step 2\"),\n        ]\n    }\n}\n\n#[derive(Clone, Debug)]\nstruct Agent {\n    id: String,\n    agent_type: AgentType,\n    status: AgentStatus,\n    load: usize,\n}\n\nimpl Agent {\n    fn new(id: \u0026str, agent_type: AgentType) -\u003e Self {\n        Self {\n            id: id.to_string(),\n            agent_type,\n            status: AgentStatus::Ready,\n            load: 0,\n        }\n    }\n}\n\n#[derive(Clone, Debug)]\nenum AgentType {\n    Worker,\n    Supervisor,\n}\n\n#[derive(Clone, Debug)]\nenum AgentStatus {\n    Ready,\n    Busy,\n    Failed,\n}\n\n#[derive(Clone, Debug)]\nstruct Task {\n    id: String,\n    task_type: TaskType,\n    command: String,\n    status: TaskStatus,\n}\n\nimpl Task {\n    fn new(id: \u0026str, task_type: TaskType, command: \u0026str) -\u003e Self {\n        Self {\n            id: id.to_string(),\n            task_type,\n            command: command.to_string(),\n            status: TaskStatus::Pending,\n        }\n    }\n}\n\n#[derive(Clone, Debug)]\nenum TaskType {\n    Compute,\n    IO,\n    Coordinated,\n}\n\n#[derive(Clone, Debug)]\nenum TaskStatus {\n    Pending,\n    Running,\n    Completed,\n    Failed,\n}\n\n#[derive(Clone, Debug)]\nstruct Message {\n    from: String,\n    to: String,\n    content: String,\n    timestamp: chrono::DateTime\u003cchrono::Utc\u003e,\n}","traces":[],"covered":0,"coverable":0}]};
        var previousData = {"files":[{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","cli","src","cli.rs"],"content":"use anyhow::{Context, Result};\nuse clap::{Parser, Subcommand};\nuse opencode_core::supervisor::AgentSupervisor;\nuse tracing::{info, warn, error};\n\n#[derive(Parser, Debug, Clone)]\n#[command(author, version, about, long_about = None)]\npub struct Cli {\n    #[command(subcommand)]\n    pub command: Option\u003cCommands\u003e,\n    \n    /// Enable verbose logging\n    #[arg(short, long)]\n    pub verbose: bool,\n    \n    /// Configuration file path\n    #[arg(short, long)]\n    pub config: Option\u003cString\u003e,\n}\n\n#[derive(Subcommand, Debug, Clone)]\npub enum Commands {\n    /// Agent management commands\n    #[command(subcommand)]\n    Agent(AgentCommands),\n    \n    /// Ask a question directly\n    Ask {\n        /// The question to ask\n        question: String,\n        \n        /// Persona to use for the response\n        #[arg(short, long, default_value = \"default\")]\n        persona: String,\n    },\n    \n    /// Start interactive REPL mode\n    Repl,\n    \n    /// Show version information\n    Version,\n}\n\n#[derive(Subcommand, Debug, Clone)]\npub enum AgentCommands {\n    /// List all running agents\n    Ls,\n    \n    /// Spawn a new agent\n    Spawn {\n        /// Agent identifier\n        id: String,\n        \n        /// Agent persona\n        #[arg(short, long, default_value = \"rusty\")]\n        persona: String,\n    },\n    \n    /// Stop an agent\n    Stop {\n        /// Agent identifier\n        id: String,\n    },\n    \n    /// Get agent status\n    Status {\n        /// Agent identifier\n        id: String,\n    },\n}\n\npub async fn execute_command(command: Commands) -\u003e Result\u003c()\u003e {\n    match command {\n        Commands::Agent(agent_cmd) =\u003e execute_agent_command(agent_cmd).await,\n        Commands::Ask { question, persona } =\u003e execute_ask_command(\u0026question, \u0026persona).await,\n        Commands::Repl =\u003e {\n            // This should not happen in practice since None case goes to REPL\n            // But we handle it for completeness\n            crate::repl::start().await\n        },\n        Commands::Version =\u003e {\n            execute_version_command().await\n        },\n    }\n}\n\nasync fn execute_agent_command(command: AgentCommands) -\u003e Result\u003c()\u003e {\n    let mut supervisor = AgentSupervisor::new();\n    \n    match command {\n        AgentCommands::Ls =\u003e {\n            info!(\"Listing all agents\");\n            let agents = supervisor.list().await;\n            if agents.is_empty() {\n                println!(\"No agents running.\");\n            } else {\n                println!(\"Running agents:\");\n                for agent in agents {\n                    println!(\"  {} ({}): {:?}\", agent.id, agent.persona, agent.status);\n                }\n            }\n        }\n        AgentCommands::Spawn { id, persona } =\u003e {\n            info!(\"Spawning agent '{}' with persona '{}'\", id, persona);\n            supervisor.spawn(\u0026id, \u0026persona).await\n                .with_context(|| format!(\"Failed to spawn agent '{}' with persona '{}'\", id, persona))?;\n            println!(\"Spawned agent '{}' with persona '{}'\", id, persona);\n        }\n        AgentCommands::Stop { id } =\u003e {\n            info!(\"Stopping agent '{}'\", id);\n            supervisor.stop(\u0026id).await\n                .with_context(|| format!(\"Failed to stop agent '{}'\", id))?;\n            println!(\"Stopped agent '{}'\", id);\n        }\n        AgentCommands::Status { id } =\u003e {\n            info!(\"Getting status for agent '{}'\", id);\n            match supervisor.get_status(\u0026id).await {\n                Ok(status) =\u003e println!(\"Agent '{}' status: {:?}\", id, status),\n                Err(e) =\u003e {\n                    error!(\"Failed to get status for agent '{}': {}\", id, e);\n                    return Err(e.into());\n                }\n            }\n        }\n    }\n    \n    Ok(())\n}\n\nasync fn execute_ask_command(question: \u0026str, persona: \u0026str) -\u003e Result\u003c()\u003e {\n    info!(\"Asking question with persona '{}'\", persona);\n    \n    // Import ask function from core\n    use opencode_core::ask_with_persona;\n    \n    match ask_with_persona(question, persona).await {\n        Ok(response) =\u003e {\n            println!(\"{}\", response);\n        }\n        Err(e) =\u003e {\n            error!(\"Failed to get response: {}\", e);\n            return Err(e.into());\n        }\n    }\n    \n    Ok(())\n}\n\nasync fn execute_version_command() -\u003e Result\u003c()\u003e {\n    println!(\"OpenCode-RS CLI v{}\", env!(\"CARGO_PKG_VERSION\"));\n    println!(\"Built with Rust {}\", env!(\"RUSTC_VERBOSE_VERSION\"));\n    Ok(())\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use clap::CommandFactory;\n    use pretty_assertions::assert_eq;\n    use test_case::test_case;\n\n    #[test]\n    fn test_cli_structure() {\n        // Test that CLI can be built without errors\n        let _cmd = Cli::command();\n    }\n\n    #[test]\n    fn test_cli_help() {\n        let cmd = Cli::command();\n        let help = cmd.render_help();\n        assert!(help.to_string().contains(\"OpenCode\"));\n    }\n\n    #[test_case(\"agent\", \"ls\"; \"agent ls command\")]\n    #[test_case(\"ask\", \"What is Rust?\"; \"ask command\")]\n    #[test_case(\"version\"; \"version command\")]\n    fn test_command_parsing(command: \u0026str, args: \u0026str) {\n        let mut cmd_args = vec![\"opencode\", command];\n        if !args.is_empty() {\n            cmd_args.extend(args.split_whitespace());\n        }\n        \n        let result = Cli::try_parse_from(cmd_args);\n        assert!(result.is_ok(), \"Failed to parse command: {} {}\", command, args);\n    }\n\n    #[test]\n    fn test_agent_spawn_parsing() {\n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"spawn\", \"test-agent\", \"--persona\", \"test-persona\"]).unwrap();\n        \n        match cli.command {\n            Some(Commands::Agent(AgentCommands::Spawn { id, persona })) =\u003e {\n                assert_eq!(id, \"test-agent\");\n                assert_eq!(persona, \"test-persona\");\n            }\n            _ =\u003e panic!(\"Expected agent spawn command\"),\n        }\n    }\n\n    #[test]\n    fn test_ask_command_parsing() {\n        let cli = Cli::try_parse_from([\"opencode\", \"ask\", \"What is Rust?\", \"--persona\", \"expert\"]).unwrap();\n        \n        match cli.command {\n            Some(Commands::Ask { question, persona }) =\u003e {\n                assert_eq!(question, \"What is Rust?\");\n                assert_eq!(persona, \"expert\");\n            }\n            _ =\u003e panic!(\"Expected ask command\"),\n        }\n    }\n\n    #[test]\n    fn test_default_persona() {\n        let cli = Cli::try_parse_from([\"opencode\", \"ask\", \"What is Rust?\"]).unwrap();\n        \n        match cli.command {\n            Some(Commands::Ask { persona, .. }) =\u003e {\n                assert_eq!(persona, \"default\");\n            }\n            _ =\u003e panic!(\"Expected ask command\"),\n        }\n    }\n\n    #[test]\n    fn test_verbose_flag() {\n        let cli = Cli::try_parse_from([\"opencode\", \"--verbose\", \"version\"]).unwrap();\n        assert!(cli.verbose);\n    }\n\n    #[test]\n    fn test_config_option() {\n        let cli = Cli::try_parse_from([\"opencode\", \"--config\", \"test.toml\", \"version\"]).unwrap();\n        assert_eq!(cli.config, Some(\"test.toml\".to_string()));\n    }\n\n    #[test]\n    fn test_invalid_command() {\n        let result = Cli::try_parse_from([\"opencode\", \"invalid\"]);\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_agent_commands_completeness() {\n        // Ensure all agent commands are tested\n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"ls\"]).unwrap();\n        assert!(matches!(cli.command, Some(Commands::Agent(AgentCommands::Ls))));\n        \n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"spawn\", \"test\"]).unwrap();\n        assert!(matches!(cli.command, Some(Commands::Agent(AgentCommands::Spawn { .. }))));\n        \n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"stop\", \"test\"]).unwrap();\n        assert!(matches!(cli.command, Some(Commands::Agent(AgentCommands::Stop { .. }))));\n        \n        let cli = Cli::try_parse_from([\"opencode\", \"agent\", \"status\", \"test\"]).unwrap();\n        assert!(matches!(cli.command, Some(Commands::Agent(AgentCommands::Status { .. }))));\n    }\n\n    #[tokio::test]\n    async fn test_version_command_execution() {\n        let result = execute_version_command().await;\n        assert!(result.is_ok());\n    }\n\n    // Property-based testing for command parsing\n    #[cfg(feature = \"proptest\")]\n    mod property_tests {\n        use super::*;\n        use proptest::prelude::*;\n\n        proptest! {\n            #[test]\n            fn test_agent_id_parsing(id in \"[a-zA-Z][a-zA-Z0-9_-]*\") {\n                let args = vec![\"opencode\", \"agent\", \"spawn\", \u0026id];\n                let result = Cli::try_parse_from(args);\n                prop_assert!(result.is_ok());\n            }\n\n            #[test]\n            fn test_ask_question_parsing(question in \".{1,100}\") {\n                let args = vec![\"opencode\", \"ask\", \u0026question];\n                let result = Cli::try_parse_from(args);\n                prop_assert!(result.is_ok());\n            }\n        }\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","cli","src","main.rs"],"content":"mod cli;\nmod repl;\n\nuse anyhow::Result;\nuse clap::Parser;\n\n#[tokio::main]\nasync fn main() -\u003e Result\u003c()\u003e {\n    let cli = cli::Cli::parse();\n    \n    match cli.command {\n        Some(cmd) =\u003e {\n            // Single-shot command mode\n            cli::execute_command(cmd).await\n        }\n        None =\u003e {\n            // Interactive REPL mode\n            repl::start().await\n        }\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","config","mod.rs"],"content":"use crate::error::{Error, Result};\nuse serde::{Deserialize, Serialize};\nuse std::env;\nuse std::fs;\nuse std::path::Path;\n\n#[cfg(test)]\nmod tests;\n\n/// OpenAI configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OpenAIConfig {\n    pub default_model: String,\n    pub api_base: String,\n    pub max_retries: u32,\n    pub timeout_seconds: u32,\n}\n\nimpl Default for OpenAIConfig {\n    fn default() -\u003e Self {\n        Self {\n            default_model: \"gpt-4\".to_string(),\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        }\n    }\n}\n\n/// Main configuration structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Config {\n    pub openai: OpenAIConfig,\n}\n\nimpl Default for Config {\n    fn default() -\u003e Self {\n        Self {\n            openai: OpenAIConfig::default(),\n        }\n    }\n}\n\nimpl Config {\n    /// Load configuration from file and environment variables\n    /// Environment variables take precedence over file values\n    pub fn load\u003cP: AsRef\u003cPath\u003e\u003e(config_path: Option\u003cP\u003e) -\u003e Result\u003cSelf\u003e {\n        let mut config = if let Some(path) = config_path {\n            Self::from_file(path)?\n        } else {\n            Self::default()\n        };\n\n        // Override with environment variables\n        let env_config = Self::from_env()?;\n        config.merge_env(env_config);\n\n        Ok(config)\n    }\n\n    /// Load configuration from a TOML file\n    pub fn from_file\u003cP: AsRef\u003cPath\u003e\u003e(path: P) -\u003e Result\u003cSelf\u003e {\n        let content = fs::read_to_string(path)?;\n        let config: Config = toml::from_str(\u0026content)?;\n        Ok(config)\n    }\n\n    /// Load configuration from environment variables\n    pub fn from_env() -\u003e Result\u003cSelf\u003e {\n        let mut config = Self::default();\n\n        // OpenAI configuration\n        if let Ok(model) = env::var(\"OPENAI_MODEL\") {\n            config.openai.default_model = model;\n        }\n\n        if let Ok(api_base) = env::var(\"OPENAI_API_BASE\") {\n            config.openai.api_base = api_base;\n        }\n\n        if let Ok(max_retries) = env::var(\"OPENAI_MAX_RETRIES\") {\n            config.openai.max_retries = max_retries\n                .parse()\n                .map_err(|e| Error::Config(format!(\"Invalid OPENAI_MAX_RETRIES: {}\", e)))?;\n        }\n\n        if let Ok(timeout) = env::var(\"OPENAI_TIMEOUT\") {\n            config.openai.timeout_seconds = timeout\n                .parse()\n                .map_err(|e| Error::Config(format!(\"Invalid OPENAI_TIMEOUT: {}\", e)))?;\n        }\n\n        Ok(config)\n    }\n\n    /// Merge environment configuration into this config\n    /// Environment values take precedence\n    fn merge_env(\u0026mut self, env_config: Config) {\n        // Only update values that were actually set in environment\n        if env::var(\"OPENAI_MODEL\").is_ok() {\n            self.openai.default_model = env_config.openai.default_model;\n        }\n        if env::var(\"OPENAI_API_BASE\").is_ok() {\n            self.openai.api_base = env_config.openai.api_base;\n        }\n        if env::var(\"OPENAI_MAX_RETRIES\").is_ok() {\n            self.openai.max_retries = env_config.openai.max_retries;\n        }\n        if env::var(\"OPENAI_TIMEOUT\").is_ok() {\n            self.openai.timeout_seconds = env_config.openai.timeout_seconds;\n        }\n    }\n\n    /// Save configuration to a TOML file\n    pub fn save\u003cP: AsRef\u003cPath\u003e\u003e(\u0026self, path: P) -\u003e Result\u003c()\u003e {\n        let content = toml::to_string_pretty(self)\n            .map_err(|e| Error::Config(format!(\"Failed to serialize config: {}\", e)))?;\n        fs::write(path, content)?;\n        Ok(())\n    }\n}","traces":[{"line":20,"address":[],"length":0,"stats":{"Line":21}},{"line":22,"address":[],"length":0,"stats":{"Line":63}},{"line":23,"address":[],"length":0,"stats":{"Line":21}},{"line":37,"address":[],"length":0,"stats":{"Line":20}},{"line":39,"address":[],"length":0,"stats":{"Line":20}},{"line":47,"address":[],"length":0,"stats":{"Line":3}},{"line":48,"address":[],"length":0,"stats":{"Line":8}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":1}},{"line":55,"address":[],"length":0,"stats":{"Line":3}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":4}},{"line":63,"address":[],"length":0,"stats":{"Line":12}},{"line":64,"address":[],"length":0,"stats":{"Line":3}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":5}},{"line":70,"address":[],"length":0,"stats":{"Line":10}},{"line":73,"address":[],"length":0,"stats":{"Line":8}},{"line":77,"address":[],"length":0,"stats":{"Line":6}},{"line":81,"address":[],"length":0,"stats":{"Line":7}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":6}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":5}},{"line":98,"address":[],"length":0,"stats":{"Line":3}},{"line":100,"address":[],"length":0,"stats":{"Line":7}},{"line":101,"address":[],"length":0,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":6}},{"line":104,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":7}},{"line":107,"address":[],"length":0,"stats":{"Line":1}},{"line":109,"address":[],"length":0,"stats":{"Line":6}},{"line":110,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}}],"covered":26,"coverable":39},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","config","tests.rs"],"content":"use super::*;\nuse std::env;\nuse tempfile::NamedTempFile;\nuse std::io::Write;\n\n#[test]\nfn test_config_defaults() {\n    let config = Config::default();\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 3);\n    assert_eq!(config.openai.timeout_seconds, 30);\n}\n\n#[test]\nfn test_openai_config_defaults() {\n    let config = OpenAIConfig::default();\n    assert_eq!(config.default_model, \"gpt-4\");\n    assert_eq!(config.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.max_retries, 3);\n    assert_eq!(config.timeout_seconds, 30);\n}\n\n#[test]\nfn test_config_from_toml() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-3.5-turbo\"\napi_base = \"https://custom.openai.com/v1\"\nmax_retries = 5\ntimeout_seconds = 60\n\"#;\n\n    let config: Config = toml::from_str(toml_content).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-3.5-turbo\");\n    assert_eq!(config.openai.api_base, \"https://custom.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 5);\n    assert_eq!(config.openai.timeout_seconds, 60);\n}\n\n#[test]\nfn test_config_from_file() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4-turbo\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 2\ntimeout_seconds = 45\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    let config = Config::from_file(temp_file.path()).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4-turbo\");\n    assert_eq!(config.openai.max_retries, 2);\n    assert_eq!(config.openai.timeout_seconds, 45);\n}\n\n#[test]\nfn test_config_from_file_not_found() {\n    let result = Config::from_file(\"non_existent_file.toml\");\n    assert!(result.is_err());\n    match result {\n        Err(Error::Io(_)) =\u003e {}\n        _ =\u003e panic!(\"Expected IO error\"),\n    }\n}\n\n#[test]\nfn test_config_from_env() {\n    // Set environment variables\n    env::set_var(\"OPENAI_MODEL\", \"gpt-4-vision\");\n    env::set_var(\"OPENAI_API_BASE\", \"https://custom-api.com/v1\");\n    env::set_var(\"OPENAI_MAX_RETRIES\", \"7\");\n    env::set_var(\"OPENAI_TIMEOUT\", \"90\");\n\n    let config = Config::from_env().unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4-vision\");\n    assert_eq!(config.openai.api_base, \"https://custom-api.com/v1\");\n    assert_eq!(config.openai.max_retries, 7);\n    assert_eq!(config.openai.timeout_seconds, 90);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n}\n\n#[test]\nfn test_config_from_env_partial() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Only set some environment variables\n    env::set_var(\"OPENAI_MODEL\", \"gpt-3.5-turbo-16k\");\n\n    let config = Config::from_env().unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-3.5-turbo-16k\");\n    // Should use defaults for other values\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 3);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n}\n\n#[test]\nfn test_config_load_priority() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Test that environment variables override file values\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 3\ntimeout_seconds = 30\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    // Set environment variable\n    env::set_var(\"OPENAI_MODEL\", \"gpt-4-turbo\");\n\n    let config = Config::load(Some(temp_file.path())).unwrap();\n    // Environment variable should override file value\n    assert_eq!(config.openai.default_model, \"gpt-4-turbo\");\n    // File value should be used for non-overridden values\n    assert_eq!(config.openai.max_retries, 3);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n}\n\n#[test]\nfn test_config_load_file_only() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 4\ntimeout_seconds = 25\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    let config = Config::load(Some(temp_file.path())).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.max_retries, 4);\n    assert_eq!(config.openai.timeout_seconds, 25);\n}\n\n#[test]\nfn test_config_load_no_file() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Load with no file specified - should use defaults + env\n    env::set_var(\"OPENAI_MAX_RETRIES\", \"10\");\n\n    let config = Config::load::\u003c\u0026str\u003e(None).unwrap();\n    // Should use default for most values\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    // But use env var where set\n    assert_eq!(config.openai.max_retries, 10);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n}\n\n#[test]\nfn test_invalid_toml() {\n    let invalid_toml = r#\"\n[openai\ndefault_model = \"gpt-4\"\n\"#;\n\n    let result: std::result::Result\u003cConfig, toml::de::Error\u003e = toml::from_str(invalid_toml);\n    assert!(result.is_err());\n}\n\n#[test]\nfn test_config_serialization() {\n    let config = Config {\n        openai: OpenAIConfig {\n            default_model: \"gpt-4\".to_string(),\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        },\n    };\n\n    let toml_str = toml::to_string(\u0026config).unwrap();\n    assert!(toml_str.contains(\"default_model = \\\"gpt-4\\\"\"));\n    assert!(toml_str.contains(\"max_retries = 3\"));\n\n    // Round trip test\n    let parsed: Config = toml::from_str(\u0026toml_str).unwrap();\n    assert_eq!(parsed.openai.default_model, config.openai.default_model);\n    assert_eq!(parsed.openai.max_retries, config.openai.max_retries);\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","error.rs"],"content":"use std::fmt;\n\n/// Custom error type for the application\n#[derive(Debug)]\npub enum Error {\n    /// Configuration errors\n    Config(String),\n    /// Provider errors (API calls, network, etc.)\n    Provider(String),\n    /// Service container errors\n    Service(String),\n    /// IO errors\n    Io(std::io::Error),\n    /// Other errors\n    Other(String),\n}\n\nimpl fmt::Display for Error {\n    fn fmt(\u0026self, f: \u0026mut fmt::Formatter\u003c'_\u003e) -\u003e fmt::Result {\n        match self {\n            Error::Config(msg) =\u003e write!(f, \"Configuration error: {}\", msg),\n            Error::Provider(msg) =\u003e write!(f, \"Provider error: {}\", msg),\n            Error::Service(msg) =\u003e write!(f, \"Service error: {}\", msg),\n            Error::Io(err) =\u003e write!(f, \"IO error: {}\", err),\n            Error::Other(msg) =\u003e write!(f, \"Error: {}\", msg),\n        }\n    }\n}\n\nimpl std::error::Error for Error {\n    fn source(\u0026self) -\u003e Option\u003c\u0026(dyn std::error::Error + 'static)\u003e {\n        match self {\n            Error::Io(err) =\u003e Some(err),\n            _ =\u003e None,\n        }\n    }\n}\n\nimpl From\u003cstd::io::Error\u003e for Error {\n    fn from(err: std::io::Error) -\u003e Self {\n        Error::Io(err)\n    }\n}\n\nimpl From\u003ctoml::de::Error\u003e for Error {\n    fn from(err: toml::de::Error) -\u003e Self {\n        Error::Config(format!(\"TOML parsing error: {}\", err))\n    }\n}\n\nimpl From\u003cstd::env::VarError\u003e for Error {\n    fn from(err: std::env::VarError) -\u003e Self {\n        Error::Config(format!(\"Environment variable error: {}\", err))\n    }\n}\n\n/// Result type alias\npub type Result\u003cT\u003e = std::result::Result\u003cT, Error\u003e;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::error::Error as StdError;\n\n    #[test]\n    fn test_error_display() {\n        let err = Error::Config(\"Invalid API key\".to_string());\n        assert_eq!(err.to_string(), \"Configuration error: Invalid API key\");\n\n        let err = Error::Provider(\"API rate limit exceeded\".to_string());\n        assert_eq!(err.to_string(), \"Provider error: API rate limit exceeded\");\n\n        let err = Error::Service(\"Service not found\".to_string());\n        assert_eq!(err.to_string(), \"Service error: Service not found\");\n\n        let err = Error::Other(\"Unknown error\".to_string());\n        assert_eq!(err.to_string(), \"Error: Unknown error\");\n    }\n\n    #[test]\n    fn test_error_from_io() {\n        let io_err = std::io::Error::new(std::io::ErrorKind::NotFound, \"File not found\");\n        let err: Error = io_err.into();\n        assert!(matches!(err, Error::Io(_)));\n    }\n\n    #[test]\n    fn test_error_source() {\n        let io_err = std::io::Error::new(std::io::ErrorKind::PermissionDenied, \"Access denied\");\n        let err = Error::Io(io_err);\n        assert!(StdError::source(\u0026err).is_some());\n\n        let err = Error::Config(\"Bad config\".to_string());\n        assert!(StdError::source(\u0026err).is_none());\n    }\n\n    #[test]\n    fn test_error_from_env_var() {\n        let env_err = std::env::VarError::NotPresent;\n        let err: Error = env_err.into();\n        match err {\n            Error::Config(msg) =\u003e assert!(msg.contains(\"Environment variable error\")),\n            _ =\u003e panic!(\"Expected Config error\"),\n        }\n    }\n}","traces":[{"line":19,"address":[],"length":0,"stats":{"Line":4}},{"line":20,"address":[],"length":0,"stats":{"Line":4}},{"line":21,"address":[],"length":0,"stats":{"Line":1}},{"line":22,"address":[],"length":0,"stats":{"Line":4}},{"line":23,"address":[],"length":0,"stats":{"Line":4}},{"line":24,"address":[],"length":0,"stats":{"Line":0}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":31,"address":[],"length":0,"stats":{"Line":2}},{"line":32,"address":[],"length":0,"stats":{"Line":2}},{"line":33,"address":[],"length":0,"stats":{"Line":1}},{"line":34,"address":[],"length":0,"stats":{"Line":1}},{"line":40,"address":[],"length":0,"stats":{"Line":2}},{"line":41,"address":[],"length":0,"stats":{"Line":2}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":1}},{"line":53,"address":[],"length":0,"stats":{"Line":1}}],"covered":14,"coverable":17},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","lib.rs"],"content":"pub mod config;\npub mod config;\npub mod error;\npub mod provider;\npub mod service;\n\n#[cfg(test)]\nmod comprehensive_tests;\n\n#[cfg(test)]\nmod property_tests;\n\n\nuse config::Config;\nuse error::Result;\nuse provider::{CompletionRequest, Message};\nuse service::ServiceContainer;\nuse std::sync::OnceLock;\n\nstatic SERVICE_CONTAINER: OnceLock\u003cServiceContainer\u003e = OnceLock::new();\n\n/// Initialize the global service container\npub fn init(config: Config) -\u003e Result\u003c()\u003e {\n    let container = ServiceContainer::new(config)?;\n    SERVICE_CONTAINER\n        .set(container)\n        .map_err(|_| error::Error::Service(\"Service container already initialized\".into()))?;\n    Ok(())\n}\n\n/// Get the global service container\npub fn get_service_container() -\u003e Result\u003c\u0026'static ServiceContainer\u003e {\n    SERVICE_CONTAINER\n        .get()\n        .ok_or_else(|| error::Error::Service(\"Service container not initialized\".into()))\n}\n\n/// Backward compatible ask function\npub async fn ask(prompt: \u0026str) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: container.config().openai.default_model.clone(),\n        messages: vec![Message {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }],\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n/// Ask with a specific model\npub async fn ask_with_model(prompt: \u0026str, model: \u0026str) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: model.to_string(),\n        messages: vec![Message {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }],\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n/// Ask with messages (conversation context)\npub async fn ask_with_messages(messages: Vec\u003cMessage\u003e) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: container.config().openai.default_model.clone(),\n        messages,\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::provider::tests::MockProvider;\n    use std::sync::Arc;\n\n    fn setup_test_container() -\u003e ServiceContainer {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n        \n        let mock_provider = Arc::new(MockProvider {\n            response: \"Test response from global\".to_string(),\n            should_fail: false,\n        });\n        \n        container.register_provider(\"mock\", mock_provider);\n        container\n    }\n\n    #[test]\n    fn test_init_and_get_container() {\n        // Reset for test\n        let config = Config::default();\n        \n        // This might fail if already initialized, but that's okay for tests\n        let _ = init(config);\n        \n        // Should be able to get the container\n        let result = get_service_container();\n        // In a real test environment, this might be initialized already\n        // so we just check it doesn't panic\n        assert!(result.is_ok() || result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_ask_backward_compatibility() {\n        // For this test, we'll test the ask function logic without global state\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_model_logic() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Test with specific model\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_messages_logic() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let messages = vec![\n            Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            },\n            Message {\n                role: \"assistant\".to_string(),\n                content: \"Hi there!\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"How are you?\".to_string(),\n            },\n        ];\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages,\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[test]\n    fn test_service_not_initialized() {\n        // Clear any existing container (this is a limitation of using OnceLock in tests)\n        // In practice, we'd use a different pattern for testability\n        \n        // This test verifies the error when service is not initialized\n        // The actual behavior depends on whether init() was called previously\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_persona_default() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are a helpful assistant.\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Hello\".to_string(),\n                },\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_persona_expert() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are an expert software developer with deep knowledge of programming languages, best practices, and system design.\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Test expert persona\".to_string(),\n                },\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_persona_custom() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are a helpful assistant with the personality of a custom expert.\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Test custom persona\".to_string(),\n                },\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n}","traces":[{"line":15,"address":[],"length":0,"stats":{"Line":1}},{"line":16,"address":[],"length":0,"stats":{"Line":3}},{"line":19,"address":[],"length":0,"stats":{"Line":0}},{"line":20,"address":[],"length":0,"stats":{"Line":1}},{"line":24,"address":[],"length":0,"stats":{"Line":1}},{"line":25,"address":[],"length":0,"stats":{"Line":1}},{"line":27,"address":[],"length":0,"stats":{"Line":1}},{"line":31,"address":[],"length":0,"stats":{"Line":0}},{"line":32,"address":[],"length":0,"stats":{"Line":0}},{"line":33,"address":[],"length":0,"stats":{"Line":0}},{"line":36,"address":[],"length":0,"stats":{"Line":0}},{"line":37,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":42,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":83,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}}],"covered":6,"coverable":33},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","provider","mod.rs"],"content":"use crate::error::{Error, Result};\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse serde::{Deserialize, Serialize};\n\n#[cfg(test)]\npub mod tests;\n\n/// Message in a conversation\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Message {\n    pub role: String,\n    pub content: String,\n}\n\n/// Request for LLM completion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompletionRequest {\n    pub model: String,\n    pub messages: Vec\u003cMessage\u003e,\n    pub temperature: Option\u003cf32\u003e,\n    pub max_tokens: Option\u003cu32\u003e,\n    pub stream: bool,\n}\n\n/// Response from LLM completion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompletionResponse {\n    pub content: String,\n    pub model: String,\n    pub usage: Usage,\n}\n\n/// Token usage information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Usage {\n    pub prompt_tokens: u32,\n    pub completion_tokens: u32,\n    pub total_tokens: u32,\n}\n\n/// Streaming chunk from LLM\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct StreamChunk {\n    pub delta: String,\n    pub finish_reason: Option\u003cString\u003e,\n}\n\n/// Trait for LLM providers\n#[async_trait]\npub trait LLMProvider: Send + Sync {\n    /// Get the name of the provider\n    fn name(\u0026self) -\u003e \u0026str;\n\n    /// Complete a request and return the full response\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e;\n\n    /// Stream a response\n    async fn stream(\n        \u0026self,\n        request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e;\n}\n\npub mod openai;\n\npub use openai::OpenAIProvider;","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","provider","openai.rs"],"content":"use super::*;\nuse crate::config::OpenAIConfig;\nuse async_openai::{\n    types::{\n        ChatCompletionRequestMessage, ChatCompletionRequestSystemMessageArgs,\n        ChatCompletionRequestUserMessageArgs, ChatCompletionRequestAssistantMessageArgs,\n        CreateChatCompletionRequestArgs, CreateChatCompletionStreamResponse,\n    },\n    Client,\n};\nuse futures::StreamExt;\n\n/// OpenAI provider implementation\npub struct OpenAIProvider {\n    client: Client\u003casync_openai::config::OpenAIConfig\u003e,\n    config: OpenAIConfig,\n}\n\nimpl OpenAIProvider {\n    /// Create a new OpenAI provider\n    pub fn new(api_key: String, config: OpenAIConfig) -\u003e Self {\n        let openai_config = async_openai::config::OpenAIConfig::new()\n            .with_api_key(api_key)\n            .with_api_base(config.api_base.clone());\n\n        Self {\n            client: Client::with_config(openai_config),\n            config,\n        }\n    }\n\n    fn convert_messages(\u0026self, messages: Vec\u003cMessage\u003e) -\u003e Vec\u003cChatCompletionRequestMessage\u003e {\n        messages\n            .into_iter()\n            .map(|msg| match msg.role.as_str() {\n                \"system\" =\u003e ChatCompletionRequestSystemMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n                \"assistant\" =\u003e ChatCompletionRequestAssistantMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n                _ =\u003e ChatCompletionRequestUserMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n            })\n            .collect()\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for OpenAIProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"openai\"\n    }\n\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e {\n        let mut builder = CreateChatCompletionRequestArgs::default();\n        builder\n            .model(\u0026request.model)\n            .messages(self.convert_messages(request.messages));\n\n        if let Some(temp) = request.temperature {\n            builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = request.max_tokens {\n            builder.max_tokens(max_tokens as u16);\n        }\n\n        let openai_request = builder\n            .build()\n            .map_err(|e| Error::Provider(format!(\"Failed to build request: {}\", e)))?;\n\n        let response = self\n            .client\n            .chat()\n            .create(openai_request)\n            .await\n            .map_err(|e| Error::Provider(format!(\"OpenAI API error: {}\", e)))?;\n\n        let content = response\n            .choices\n            .first()\n            .and_then(|c| c.message.content.as_ref())\n            .ok_or_else(|| Error::Provider(\"No content in response\".into()))?\n            .clone();\n\n        Ok(CompletionResponse {\n            content,\n            model: response.model,\n            usage: Usage {\n                prompt_tokens: response.usage.as_ref().map(|u| u.prompt_tokens).unwrap_or(0) as u32,\n                completion_tokens: response\n                    .usage\n                    .as_ref()\n                    .map(|u| u.completion_tokens)\n                    .unwrap_or(0) as u32,\n                total_tokens: response.usage.as_ref().map(|u| u.total_tokens).unwrap_or(0) as u32,\n            },\n        })\n    }\n\n    async fn stream(\n        \u0026self,\n        request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e {\n        let mut builder = CreateChatCompletionRequestArgs::default();\n        builder\n            .model(\u0026request.model)\n            .messages(self.convert_messages(request.messages))\n            .stream(true);\n\n        if let Some(temp) = request.temperature {\n            builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = request.max_tokens {\n            builder.max_tokens(max_tokens as u16);\n        }\n\n        let openai_request = builder\n            .build()\n            .map_err(|e| Error::Provider(format!(\"Failed to build request: {}\", e)))?;\n\n        let stream = self\n            .client\n            .chat()\n            .create_stream(openai_request)\n            .await\n            .map_err(|e| Error::Provider(format!(\"OpenAI API error: {}\", e)))?;\n\n        let mapped_stream = stream.map(|result| match result {\n            Ok(response) =\u003e {\n                let chunk = extract_chunk(response);\n                Ok(chunk)\n            }\n            Err(e) =\u003e Err(Error::Provider(format!(\"Stream error: {}\", e))),\n        });\n\n        Ok(Box::pin(mapped_stream))\n    }\n}\n\nfn extract_chunk(response: CreateChatCompletionStreamResponse) -\u003e StreamChunk {\n    let delta = response\n        .choices\n        .first()\n        .and_then(|c| c.delta.content.as_ref())\n        .map(|s| s.clone())\n        .unwrap_or_default();\n\n    let finish_reason = response\n        .choices\n        .first()\n        .and_then(|c| c.finish_reason.as_ref())\n        .map(|r| format!(\"{:?}\", r));\n\n    StreamChunk {\n        delta,\n        finish_reason,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_openai_provider_creation() {\n        let config = OpenAIConfig {\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            default_model: \"gpt-4\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n\n        let provider = OpenAIProvider::new(\"test-key\".to_string(), config.clone());\n        assert_eq!(provider.name(), \"openai\");\n        assert_eq!(provider.config.default_model, \"gpt-4\");\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let config = OpenAIConfig {\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            default_model: \"gpt-4\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n\n        let provider = OpenAIProvider::new(\"test-key\".to_string(), config);\n\n        let messages = vec![\n            Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            },\n            Message {\n                role: \"assistant\".to_string(),\n                content: \"Hi there!\".to_string(),\n            },\n        ];\n\n        let converted = provider.convert_messages(messages);\n        assert_eq!(converted.len(), 3);\n    }\n\n    #[test]\n    fn test_extract_chunk() {\n        // This would require mocking CreateChatCompletionStreamResponse\n        // which is complex due to the async-openai types\n        // For now, we'll focus on the integration tests\n    }\n}","traces":[{"line":21,"address":[],"length":0,"stats":{"Line":15}},{"line":22,"address":[],"length":0,"stats":{"Line":30}},{"line":23,"address":[],"length":0,"stats":{"Line":30}},{"line":24,"address":[],"length":0,"stats":{"Line":45}},{"line":27,"address":[],"length":0,"stats":{"Line":30}},{"line":32,"address":[],"length":0,"stats":{"Line":1}},{"line":33,"address":[],"length":0,"stats":{"Line":1}},{"line":35,"address":[],"length":0,"stats":{"Line":4}},{"line":36,"address":[],"length":0,"stats":{"Line":5}},{"line":37,"address":[],"length":0,"stats":{"Line":1}},{"line":38,"address":[],"length":0,"stats":{"Line":1}},{"line":39,"address":[],"length":0,"stats":{"Line":1}},{"line":40,"address":[],"length":0,"stats":{"Line":1}},{"line":41,"address":[],"length":0,"stats":{"Line":4}},{"line":42,"address":[],"length":0,"stats":{"Line":1}},{"line":43,"address":[],"length":0,"stats":{"Line":1}},{"line":44,"address":[],"length":0,"stats":{"Line":1}},{"line":45,"address":[],"length":0,"stats":{"Line":1}},{"line":46,"address":[],"length":0,"stats":{"Line":1}},{"line":58,"address":[],"length":0,"stats":{"Line":1}},{"line":59,"address":[],"length":0,"stats":{"Line":1}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":0}},{"line":127,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}}],"covered":21,"coverable":57},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","provider","tests.rs"],"content":"use super::*;\nuse async_trait::async_trait;\nuse tokio_stream::StreamExt;\n\n#[derive(Debug, Clone)]\npub struct MockProvider {\n    pub response: String,\n    pub should_fail: bool,\n}\n\n#[async_trait]\nimpl LLMProvider for MockProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"mock\"\n    }\n\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e {\n        if self.should_fail {\n            return Err(Error::Provider(\"Mock provider error\".into()));\n        }\n\n        Ok(CompletionResponse {\n            content: self.response.clone(),\n            model: request.model,\n            usage: Usage {\n                prompt_tokens: 10,\n                completion_tokens: 20,\n                total_tokens: 30,\n            },\n        })\n    }\n\n    async fn stream(\n        \u0026self,\n        _request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e {\n        if self.should_fail {\n            return Err(Error::Provider(\"Mock provider error\".into()));\n        }\n\n        let chunks = vec![\n            StreamChunk {\n                delta: self.response.clone(),\n                finish_reason: None,\n            },\n            StreamChunk {\n                delta: String::new(),\n                finish_reason: Some(\"stop\".to_string()),\n            },\n        ];\n\n        Ok(Box::pin(tokio_stream::iter(chunks.into_iter().map(Ok))))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_mock_provider_complete() {\n        let provider = MockProvider {\n            response: \"Test response\".to_string(),\n            should_fail: false,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = provider.complete(request.clone()).await.unwrap();\n        assert_eq!(response.content, \"Test response\");\n        assert_eq!(response.model, \"gpt-4\");\n        assert_eq!(response.usage.total_tokens, 30);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_complete_error() {\n        let provider = MockProvider {\n            response: String::new(),\n            should_fail: true,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: None,\n            max_tokens: None,\n            stream: false,\n        };\n\n        let result = provider.complete(request).await;\n        assert!(result.is_err());\n        match result {\n            Err(Error::Provider(msg)) =\u003e assert_eq!(msg, \"Mock provider error\"),\n            _ =\u003e panic!(\"Expected Provider error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_stream() {\n        let provider = MockProvider {\n            response: \"Streaming response\".to_string(),\n            should_fail: false,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            }],\n            temperature: Some(0.5),\n            max_tokens: Some(200),\n            stream: true,\n        };\n\n        let mut stream = provider.stream(request).await.unwrap();\n        \n        let mut chunks = Vec::new();\n        while let Some(chunk) = stream.next().await {\n            chunks.push(chunk.unwrap());\n        }\n\n        assert_eq!(chunks.len(), 2);\n        assert_eq!(chunks[0].delta, \"Streaming response\");\n        assert_eq!(chunks[0].finish_reason, None);\n        assert_eq!(chunks[1].delta, \"\");\n        assert_eq!(chunks[1].finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[tokio::test]\n    async fn test_provider_trait_methods() {\n        let provider = MockProvider {\n            response: \"Test\".to_string(),\n            should_fail: false,\n        };\n\n        assert_eq!(provider.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_message_construction() {\n        let msg = Message {\n            role: \"assistant\".to_string(),\n            content: \"I can help with that\".to_string(),\n        };\n\n        assert_eq!(msg.role, \"assistant\");\n        assert_eq!(msg.content, \"I can help with that\");\n    }\n\n    #[test]\n    fn test_completion_request_builder() {\n        let request = CompletionRequest {\n            model: \"gpt-3.5-turbo\".to_string(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are a coding assistant\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Write a hello world program\".to_string(),\n                },\n            ],\n            temperature: Some(0.8),\n            max_tokens: Some(1000),\n            stream: true,\n        };\n\n        assert_eq!(request.model, \"gpt-3.5-turbo\");\n        assert_eq!(request.messages.len(), 2);\n        assert_eq!(request.temperature, Some(0.8));\n        assert_eq!(request.max_tokens, Some(1000));\n        assert!(request.stream);\n    }\n\n    #[test]\n    fn test_usage_calculation() {\n        let usage = Usage {\n            prompt_tokens: 50,\n            completion_tokens: 100,\n            total_tokens: 150,\n        };\n\n        assert_eq!(usage.prompt_tokens, 50);\n        assert_eq!(usage.completion_tokens, 100);\n        assert_eq!(usage.total_tokens, 150);\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","service.rs"],"content":"use crate::config::Config;\nuse crate::error::{Error, Result};\nuse crate::provider::{LLMProvider, OpenAIProvider};\nuse std::collections::HashMap;\nuse std::sync::Arc;\n\n/// Service container for dependency injection\npub struct ServiceContainer {\n    providers: HashMap\u003cString, Arc\u003cdyn LLMProvider\u003e\u003e,\n    config: Config,\n}\n\nimpl ServiceContainer {\n    /// Create a new service container\n    pub fn new(config: Config) -\u003e Result\u003cSelf\u003e {\n        let mut container = Self {\n            providers: HashMap::new(),\n            config,\n        };\n\n        // Register default providers\n        container.register_default_providers()?;\n\n        Ok(container)\n    }\n\n    /// Register default providers based on configuration\n    fn register_default_providers(\u0026mut self) -\u003e Result\u003c()\u003e {\n        // Register OpenAI provider if API key is available\n        if let Ok(api_key) = std::env::var(\"OPENAI_API_KEY\") {\n            let provider = OpenAIProvider::new(api_key, self.config.openai.clone());\n            self.register_provider(\"openai\", Arc::new(provider));\n        }\n\n        Ok(())\n    }\n\n    /// Register a provider with the container\n    pub fn register_provider(\u0026mut self, name: \u0026str, provider: Arc\u003cdyn LLMProvider\u003e) {\n        self.providers.insert(name.to_string(), provider);\n    }\n\n    /// Get a provider by name\n    pub fn get_provider(\u0026self, name: \u0026str) -\u003e Result\u003cArc\u003cdyn LLMProvider\u003e\u003e {\n        self.providers\n            .get(name)\n            .cloned()\n            .ok_or_else(|| Error::Service(format!(\"Provider '{}' not found\", name)))\n    }\n\n    /// Get the default provider (first available)\n    pub fn get_default_provider(\u0026self) -\u003e Result\u003cArc\u003cdyn LLMProvider\u003e\u003e {\n        // Try OpenAI first as the default\n        if let Ok(provider) = self.get_provider(\"openai\") {\n            return Ok(provider);\n        }\n\n        // If no specific provider, return the first available\n        self.providers\n            .values()\n            .next()\n            .cloned()\n            .ok_or_else(|| Error::Service(\"No providers available\".into()))\n    }\n\n    /// List all registered provider names\n    pub fn list_providers(\u0026self) -\u003e Vec\u003cString\u003e {\n        self.providers.keys().cloned().collect()\n    }\n\n    /// Get the configuration\n    pub fn config(\u0026self) -\u003e \u0026Config {\n        \u0026self.config\n    }\n\n    /// Update the configuration and re-register providers\n    pub fn update_config(\u0026mut self, config: Config) -\u003e Result\u003c()\u003e {\n        self.config = config;\n        self.providers.clear();\n        self.register_default_providers()?;\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::provider::tests::MockProvider;\n\n    #[test]\n    fn test_service_container_creation() {\n        let config = Config::default();\n        let container = ServiceContainer::new(config).unwrap();\n        \n        // Should create without error\n        assert!(container.providers.is_empty() || !container.providers.is_empty());\n    }\n\n    #[test]\n    fn test_register_and_get_provider() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Test response\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"mock\", mock_provider.clone());\n\n        let retrieved = container.get_provider(\"mock\").unwrap();\n        assert_eq!(retrieved.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_get_provider_not_found() {\n        let config = Config::default();\n        let container = ServiceContainer::new(config).unwrap();\n\n        let result = container.get_provider(\"nonexistent\");\n        assert!(result.is_err());\n        match result {\n            Err(Error::Service(msg)) =\u003e assert!(msg.contains(\"not found\")),\n            _ =\u003e panic!(\"Expected Service error\"),\n        }\n    }\n\n    #[test]\n    fn test_list_providers() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Clear any existing providers first\n        container.providers.clear();\n\n        let mock1 = Arc::new(MockProvider {\n            response: \"Test1\".to_string(),\n            should_fail: false,\n        });\n        let mock2 = Arc::new(MockProvider {\n            response: \"Test2\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"mock1\", mock1);\n        container.register_provider(\"mock2\", mock2);\n\n        let providers = container.list_providers();\n        assert_eq!(providers.len(), 2);\n        assert!(providers.contains(\u0026\"mock1\".to_string()));\n        assert!(providers.contains(\u0026\"mock2\".to_string()));\n    }\n\n    #[test]\n    fn test_get_default_provider() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Clear any existing providers first\n        container.providers.clear();\n\n        // With no providers registered, should fail\n        let result = container.get_default_provider();\n        assert!(result.is_err());\n\n        // Register a provider\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Default\".to_string(),\n            should_fail: false,\n        });\n        container.register_provider(\"default\", mock_provider);\n\n        let default = container.get_default_provider().unwrap();\n        assert_eq!(default.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_config_access() {\n        let config = Config::default();\n        let original_model = config.openai.default_model.clone();\n        \n        let container = ServiceContainer::new(config).unwrap();\n        assert_eq!(container.config().openai.default_model, original_model);\n    }\n\n    #[test]\n    fn test_update_config() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mut new_config = Config::default();\n        new_config.openai.default_model = \"gpt-3.5-turbo\".to_string();\n\n        container.update_config(new_config).unwrap();\n        assert_eq!(container.config().openai.default_model, \"gpt-3.5-turbo\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_functionality() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Hello from service container\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"test\", mock_provider);\n\n        let provider = container.get_provider(\"test\").unwrap();\n        \n        let request = crate::provider::CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![crate::provider::Message {\n                role: \"user\".to_string(),\n                content: \"Test message\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Hello from service container\");\n    }\n}","traces":[{"line":15,"address":[],"length":0,"stats":{"Line":12}},{"line":17,"address":[],"length":0,"stats":{"Line":12}},{"line":22,"address":[],"length":0,"stats":{"Line":24}},{"line":24,"address":[],"length":0,"stats":{"Line":12}},{"line":28,"address":[],"length":0,"stats":{"Line":13}},{"line":30,"address":[],"length":0,"stats":{"Line":26}},{"line":35,"address":[],"length":0,"stats":{"Line":13}},{"line":39,"address":[],"length":0,"stats":{"Line":21}},{"line":40,"address":[],"length":0,"stats":{"Line":105}},{"line":44,"address":[],"length":0,"stats":{"Line":8}},{"line":45,"address":[],"length":0,"stats":{"Line":8}},{"line":46,"address":[],"length":0,"stats":{"Line":16}},{"line":48,"address":[],"length":0,"stats":{"Line":14}},{"line":52,"address":[],"length":0,"stats":{"Line":2}},{"line":54,"address":[],"length":0,"stats":{"Line":4}},{"line":59,"address":[],"length":0,"stats":{"Line":2}},{"line":63,"address":[],"length":0,"stats":{"Line":4}},{"line":67,"address":[],"length":0,"stats":{"Line":1}},{"line":68,"address":[],"length":0,"stats":{"Line":4}},{"line":72,"address":[],"length":0,"stats":{"Line":3}},{"line":73,"address":[],"length":0,"stats":{"Line":3}},{"line":77,"address":[],"length":0,"stats":{"Line":1}},{"line":78,"address":[],"length":0,"stats":{"Line":2}},{"line":79,"address":[],"length":0,"stats":{"Line":2}},{"line":80,"address":[],"length":0,"stats":{"Line":2}},{"line":81,"address":[],"length":0,"stats":{"Line":1}}],"covered":26,"coverable":26},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","core","src","supervisor.rs"],"content":"use anyhow::{Context, Result};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tokio::sync::Mutex;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Agent {\n    pub id: String,\n    pub persona: String,\n    pub status: AgentStatus,\n    pub branch_name: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"PascalCase\")]\npub enum AgentStatus {\n    Running,\n    Stopped,\n    Error(String),\n}\n\npub struct AgentSupervisor {\n    agents: Arc\u003cMutex\u003cHashMap\u003cString, Agent\u003e\u003e\u003e,\n}\n\nimpl AgentSupervisor {\n    pub fn new() -\u003e Self {\n        Self {\n            agents: Arc::new(Mutex::new(HashMap::new())),\n        }\n    }\n\n    pub async fn spawn(\u0026mut self, id: \u0026str, persona: \u0026str) -\u003e Result\u003c()\u003e {\n        let mut agents = self.agents.lock().await;\n        \n        if agents.contains_key(id) {\n            return Err(anyhow::anyhow!(\"Agent with id '{}' already exists\", id));\n        }\n\n        let agent = Agent {\n            id: id.to_string(),\n            persona: persona.to_string(),\n            status: AgentStatus::Running,\n            branch_name: format!(\"agent-{}\", id),\n        };\n\n        agents.insert(id.to_string(), agent);\n        Ok(())\n    }\n\n    pub async fn list(\u0026self) -\u003e Vec\u003cAgent\u003e {\n        let agents = self.agents.lock().await;\n        agents.values().cloned().collect()\n    }\n\n    pub async fn stop(\u0026mut self, id: \u0026str) -\u003e Result\u003c()\u003e {\n        let mut agents = self.agents.lock().await;\n        \n        let agent = agents.get_mut(id)\n            .context(format!(\"Agent '{}' not found\", id))?;\n        \n        agent.status = AgentStatus::Stopped;\n        Ok(())\n    }\n\n    pub async fn get_status(\u0026self, id: \u0026str) -\u003e Result\u003cAgentStatus\u003e {\n        let agents = self.agents.lock().await;\n        \n        let agent = agents.get(id)\n            .context(format!(\"Agent '{}' not found\", id))?;\n        \n        Ok(agent.status.clone())\n    }\n}\n\nimpl Default for AgentSupervisor {\n    fn default() -\u003e Self {\n        Self::new()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_supervisor_new() {\n        let supervisor = AgentSupervisor::new();\n        let agents = supervisor.list().await;\n        assert_eq!(agents.len(), 0);\n    }\n\n    #[tokio::test]\n    async fn test_spawn_agent() {\n        let mut supervisor = AgentSupervisor::new();\n        let result = supervisor.spawn(\"test-agent\", \"rusty\").await;\n        assert!(result.is_ok());\n\n        let agents = supervisor.list().await;\n        assert_eq!(agents.len(), 1);\n        assert_eq!(agents[0].id, \"test-agent\");\n        assert_eq!(agents[0].persona, \"rusty\");\n    }\n\n    #[tokio::test]\n    async fn test_spawn_duplicate_agent() {\n        let mut supervisor = AgentSupervisor::new();\n        supervisor.spawn(\"test-agent\", \"rusty\").await.unwrap();\n        \n        let result = supervisor.spawn(\"test-agent\", \"pythonic\").await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_stop_agent() {\n        let mut supervisor = AgentSupervisor::new();\n        supervisor.spawn(\"test-agent\", \"rusty\").await.unwrap();\n        \n        let result = supervisor.stop(\"test-agent\").await;\n        assert!(result.is_ok());\n\n        let agents = supervisor.list().await;\n        assert!(matches!(agents[0].status, AgentStatus::Stopped));\n    }\n\n    #[tokio::test]\n    async fn test_get_status() {\n        let mut supervisor = AgentSupervisor::new();\n        supervisor.spawn(\"test-agent\", \"rusty\").await.unwrap();\n        \n        let status = supervisor.get_status(\"test-agent\").await.unwrap();\n        assert!(matches!(status, AgentStatus::Running));\n        \n        supervisor.stop(\"test-agent\").await.unwrap();\n        let status = supervisor.get_status(\"test-agent\").await.unwrap();\n        assert!(matches!(status, AgentStatus::Stopped));\n    }\n\n    #[tokio::test]\n    async fn test_get_status_nonexistent() {\n        let supervisor = AgentSupervisor::new();\n        let result = supervisor.get_status(\"nonexistent\").await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_spawn_multiple_agents() {\n        let mut supervisor = AgentSupervisor::new();\n        \n        supervisor.spawn(\"agent1\", \"rusty\").await.unwrap();\n        supervisor.spawn(\"agent2\", \"pythonic\").await.unwrap();\n        \n        let agents = supervisor.list().await;\n        assert_eq!(agents.len(), 2);\n        \n        let agent1 = agents.iter().find(|a| a.id == \"agent1\").unwrap();\n        let agent2 = agents.iter().find(|a| a.id == \"agent2\").unwrap();\n        \n        assert_eq!(agent1.persona, \"rusty\");\n        assert_eq!(agent2.persona, \"pythonic\");\n        assert!(matches!(agent1.status, AgentStatus::Running));\n        assert!(matches!(agent2.status, AgentStatus::Running));\n    }\n\n    #[tokio::test]\n    async fn test_stop_nonexistent_agent() {\n        let mut supervisor = AgentSupervisor::new();\n        let result = supervisor.stop(\"nonexistent\").await;\n        assert!(result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_agent_status_serialization() {\n        let running = AgentStatus::Running;\n        let stopped = AgentStatus::Stopped;\n        let error = AgentStatus::Error(\"test error\".to_string());\n        \n        let running_json = serde_json::to_string(\u0026running).unwrap();\n        let stopped_json = serde_json::to_string(\u0026stopped).unwrap();\n        let error_json = serde_json::to_string(\u0026error).unwrap();\n        \n        assert_eq!(running_json, \"\\\"Running\\\"\");\n        assert_eq!(stopped_json, \"\\\"Stopped\\\"\");\n        assert!(error_json.contains(\"test error\"));\n    }\n\n    #[tokio::test]\n    async fn test_concurrent_agent_operations() {\n        use std::sync::Arc;\n        use tokio::sync::Mutex;\n        \n        let supervisor = Arc::new(Mutex::new(AgentSupervisor::new()));\n        let mut handles = vec![];\n        \n        // Spawn 10 agents concurrently\n        for i in 0..10 {\n            let supervisor = supervisor.clone();\n            let handle = tokio::spawn(async move {\n                let mut sup = supervisor.lock().await;\n                sup.spawn(\u0026format!(\"agent{}\", i), \"rusty\").await\n            });\n            handles.push(handle);\n        }\n        \n        // Wait for all spawn operations to complete\n        for handle in handles {\n            handle.await.unwrap().unwrap();\n        }\n        \n        let supervisor = supervisor.lock().await;\n        let agents = supervisor.list().await;\n        assert_eq!(agents.len(), 10);\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","opencode-gui","src-tauri","build.rs"],"content":"fn main() {\n    tauri_build::build()\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","crates","opencode-gui","src-tauri","src","main.rs"],"content":"#![cfg_attr(\n    all(not(debug_assertions), target_os = \"windows\"),\n    windows_subsystem = \"windows\"\n)]\n\nuse opencode_core::supervisor::{Agent, AgentSupervisor};\nuse opencode_core::swarm;\nuse std::path::PathBuf;\nuse std::sync::Arc;\nuse tauri::{AppHandle, Emitter};\nuse tokio::sync::Mutex;\n\n// Create a struct for the application's shared state\npub struct AppState {\n    supervisor: Arc\u003cMutex\u003cAgentSupervisor\u003e\u003e,\n}\n\n// Define the payload for our progress event\n#[derive(Clone, serde::Serialize)]\nstruct SwarmProgressPayload {\n    total: usize,\n    completed: usize,\n    task: String,\n}\n\n#[tauri::command]\nasync fn list_agents(state: tauri::State\u003c'_, AppState\u003e) -\u003e Result\u003cVec\u003cAgent\u003e, String\u003e {\n    let supervisor = state.supervisor.lock().await;\n    Ok(supervisor.list().await)\n}\n\n#[tauri::command]\nasync fn spawn_agent(\n    id: String,\n    persona: String,\n    state: tauri::State\u003c'_, AppState\u003e,\n) -\u003e Result\u003c(), String\u003e {\n    let mut supervisor = state.supervisor.lock().await;\n    supervisor\n        .spawn(\u0026id, \u0026persona)\n        .await\n        .map_err(|e| e.to_string())\n}\n\n#[tauri::command]\nasync fn execute_swarm_build(\n    app_handle: AppHandle,\n    state: tauri::State\u003c'_, AppState\u003e,\n) -\u003e Result\u003c(), String\u003e {\n    let supervisor = state.supervisor.lock().await;\n\n    // For this example, we assume Cargo.toml is in the current directory.\n    let manifest_path = PathBuf::from(\"Cargo.toml\");\n    let plan = swarm::plan_build_from_manifest(\u0026manifest_path).map_err(|e| e.to_string())?;\n\n    let total_tasks = plan.tasks.len();\n    println!(\"Executing swarm build with {} tasks.\", total_tasks);\n\n    // Emit initial event\n    app_handle.emit(\"SWARM_PROGRESS\", SwarmProgressPayload {\n        total: total_tasks,\n        completed: 0,\n        task: \"Starting swarm build...\".into(),\n    }).unwrap();\n\n    // Drop the supervisor lock before spawning tasks\n    drop(supervisor);\n\n    // Spawn an agent for each task\n    for (i, task) in plan.tasks.iter().enumerate() {\n        let agent_id = format!(\"builder-{}\", task.replace('/', \"-\"));\n        let persona = \"rusty\"; // Use a default builder persona\n        \n        // Acquire lock for each spawn operation\n        let mut supervisor = state.supervisor.lock().await;\n        supervisor.spawn(\u0026agent_id, persona).await.map_err(|e| e.to_string())?;\n        drop(supervisor);\n\n        // Simulate work being done\n        tokio::time::sleep(std::time::Duration::from_secs(2)).await;\n\n        // Emit a progress event after each task\n        app_handle.emit(\"SWARM_PROGRESS\", SwarmProgressPayload {\n            total: total_tasks,\n            completed: i + 1,\n            task: format!(\"Completed build for '{}'\", task),\n        }).unwrap();\n    }\n    \n    // Final completion event\n    app_handle.emit(\"SWARM_PROGRESS\", SwarmProgressPayload {\n        total: total_tasks,\n        completed: total_tasks,\n        task: \"Swarm build finished!\".into(),\n    }).unwrap();\n\n    Ok(())\n}\n\nfn main() {\n    // Create the initial state\n    let state = AppState {\n        supervisor: Arc::new(Mutex::new(AgentSupervisor::new())),\n    };\n\n    tauri::Builder::default()\n        .manage(state) // Add the state to be managed by Tauri\n        .invoke_handler(tauri::generate_handler![\n            // Register our commands\n            list_agents,\n            spawn_agent,\n            execute_swarm_build,\n        ])\n        .run(tauri::generate_context!())\n        .expect(\"error while running tauri application\");\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","opencode-core","src","config","tests.rs"],"content":"//! Tests for configuration management\n//! \n//! This test suite defines the expected behavior for configuration loading,\n//! validation, and management following TDD principles.\n\nuse super::*;\nuse std::collections::HashMap;\nuse tempfile::TempDir;\nuse std::fs;\n\n#[cfg(test)]\nmod config_tests {\n    use super::*;\n\n    #[test]\n    fn test_default_configuration() {\n        // GIVEN: No configuration file exists\n        // WHEN: We create a default configuration\n        let config = AppConfig::default();\n        \n        // THEN: It should have sensible defaults\n        assert!(config.providers.is_empty());\n        assert_eq!(config.default_provider, None);\n        assert_eq!(config.server.host, \"127.0.0.1\");\n        assert_eq!(config.server.port, 3000);\n        assert!(!config.features.streaming_enabled);\n        assert!(!config.features.function_calling_enabled);\n    }\n\n    #[test]\n    fn test_config_from_file() {\n        // GIVEN: A configuration file with provider settings\n        let temp_dir = TempDir::new().unwrap();\n        let config_path = temp_dir.path().join(\"config.toml\");\n        \n        let config_content = r#\"\n            default_provider = \"openai\"\n            \n            [server]\n            host = \"0.0.0.0\"\n            port = 8080\n            \n            [features]\n            streaming_enabled = true\n            function_calling_enabled = true\n            \n            [[providers]]\n            name = \"openai\"\n            type = \"openai\"\n            api_key = \"${OPENAI_API_KEY}\"\n            base_url = \"https://api.openai.com/v1\"\n            \n            [[providers]]\n            name = \"anthropic\"\n            type = \"anthropic\"\n            api_key = \"${ANTHROPIC_API_KEY}\"\n            base_url = \"https://api.anthropic.com\"\n        \"#;\n        \n        fs::write(\u0026config_path, config_content).unwrap();\n        \n        // WHEN: We load the configuration\n        let config = AppConfig::from_file(\u0026config_path).unwrap();\n        \n        // THEN: All settings should be loaded correctly\n        assert_eq!(config.default_provider, Some(\"openai\".to_string()));\n        assert_eq!(config.server.host, \"0.0.0.0\");\n        assert_eq!(config.server.port, 8080);\n        assert!(config.features.streaming_enabled);\n        assert!(config.features.function_calling_enabled);\n        assert_eq!(config.providers.len(), 2);\n        \n        let openai = \u0026config.providers[0];\n        assert_eq!(openai.name, \"openai\");\n        assert_eq!(openai.provider_type, ProviderType::OpenAI);\n        assert_eq!(openai.api_key, \"${OPENAI_API_KEY}\");\n    }\n\n    #[test]\n    fn test_environment_variable_expansion() {\n        // GIVEN: Configuration with environment variables\n        std::env::set_var(\"TEST_API_KEY\", \"secret-key-123\");\n        std::env::set_var(\"TEST_BASE_URL\", \"https://test.api.com\");\n        \n        let config_str = r#\"\n            [[providers]]\n            name = \"test\"\n            type = \"openai\"\n            api_key = \"${TEST_API_KEY}\"\n            base_url = \"${TEST_BASE_URL}\"\n        \"#;\n        \n        // WHEN: We parse and expand the configuration\n        let mut config: AppConfig = toml::from_str(config_str).unwrap();\n        config.expand_env_vars();\n        \n        // THEN: Environment variables should be replaced\n        assert_eq!(config.providers[0].api_key, \"secret-key-123\");\n        assert_eq!(config.providers[0].base_url, Some(\"https://test.api.com\".to_string()));\n        \n        // Cleanup\n        std::env::remove_var(\"TEST_API_KEY\");\n        std::env::remove_var(\"TEST_BASE_URL\");\n    }\n\n    #[test]\n    fn test_config_validation() {\n        // GIVEN: Various configuration scenarios\n        \n        // Test 1: Valid configuration\n        let valid_config = AppConfig {\n            providers: vec![\n                ProviderConfig {\n                    name: \"provider1\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key: \"key123\".to_string(),\n                    base_url: None,\n                    models: vec![],\n                    rate_limit: None,\n                }\n            ],\n            default_provider: Some(\"provider1\".to_string()),\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        };\n        \n        assert!(valid_config.validate().is_ok());\n        \n        // Test 2: Invalid - default provider doesn't exist\n        let invalid_config = AppConfig {\n            providers: vec![],\n            default_provider: Some(\"nonexistent\".to_string()),\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        };\n        \n        let result = invalid_config.validate();\n        assert!(result.is_err());\n        assert!(result.unwrap_err().to_string().contains(\"Default provider 'nonexistent' not found\"));\n        \n        // Test 3: Invalid - duplicate provider names\n        let duplicate_config = AppConfig {\n            providers: vec![\n                ProviderConfig {\n                    name: \"same_name\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key: \"key1\".to_string(),\n                    base_url: None,\n                    models: vec![],\n                    rate_limit: None,\n                },\n                ProviderConfig {\n                    name: \"same_name\".to_string(),\n                    provider_type: ProviderType::Anthropic,\n                    api_key: \"key2\".to_string(),\n                    base_url: None,\n                    models: vec![],\n                    rate_limit: None,\n                }\n            ],\n            default_provider: None,\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        };\n        \n        let result = duplicate_config.validate();\n        assert!(result.is_err());\n        assert!(result.unwrap_err().to_string().contains(\"Duplicate provider name\"));\n    }\n\n    #[test]\n    fn test_config_merge() {\n        // GIVEN: A base configuration and override configuration\n        let base_config = AppConfig {\n            providers: vec![\n                ProviderConfig {\n                    name: \"openai\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key: \"base-key\".to_string(),\n                    base_url: None,\n                    models: vec![],\n                    rate_limit: None,\n                }\n            ],\n            default_provider: Some(\"openai\".to_string()),\n            server: ServerConfig {\n                host: \"127.0.0.1\".to_string(),\n                port: 3000,\n            },\n            features: FeaturesConfig {\n                streaming_enabled: false,\n                function_calling_enabled: false,\n            },\n        };\n        \n        let override_config = PartialAppConfig {\n            providers: None,\n            default_provider: Some(Some(\"anthropic\".to_string())),\n            server: Some(PartialServerConfig {\n                host: None,\n                port: Some(8080),\n            }),\n            features: Some(PartialFeaturesConfig {\n                streaming_enabled: Some(true),\n                function_calling_enabled: None,\n            }),\n        };\n        \n        // WHEN: We merge the configurations\n        let merged = base_config.merge(override_config);\n        \n        // THEN: Override values should take precedence\n        assert_eq!(merged.default_provider, Some(\"anthropic\".to_string()));\n        assert_eq!(merged.server.host, \"127.0.0.1\"); // Not overridden\n        assert_eq!(merged.server.port, 8080); // Overridden\n        assert!(merged.features.streaming_enabled); // Overridden\n        assert!(!merged.features.function_calling_enabled); // Not overridden\n    }\n\n    #[test]\n    fn test_provider_specific_config() {\n        // GIVEN: Provider-specific configurations\n        let config = AppConfig {\n            providers: vec![\n                ProviderConfig {\n                    name: \"openai-gpt4\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key: \"key1\".to_string(),\n                    base_url: None,\n                    models: vec![\"gpt-4\".to_string(), \"gpt-4-turbo\".to_string()],\n                    rate_limit: Some(RateLimitConfig {\n                        requests_per_minute: 60,\n                        tokens_per_minute: 90000,\n                    }),\n                },\n                ProviderConfig {\n                    name: \"anthropic-claude\".to_string(),\n                    provider_type: ProviderType::Anthropic,\n                    api_key: \"key2\".to_string(),\n                    base_url: Some(\"https://api.anthropic.com/v1\".to_string()),\n                    models: vec![\"claude-3-opus\".to_string()],\n                    rate_limit: Some(RateLimitConfig {\n                        requests_per_minute: 50,\n                        tokens_per_minute: 100000,\n                    }),\n                },\n            ],\n            default_provider: None,\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        };\n        \n        // WHEN: We access provider configurations\n        let openai_config = config.get_provider(\"openai-gpt4\").unwrap();\n        let anthropic_config = config.get_provider(\"anthropic-claude\").unwrap();\n        \n        // THEN: Each provider should have its specific settings\n        assert_eq!(openai_config.models.len(), 2);\n        assert!(openai_config.models.contains(\u0026\"gpt-4\".to_string()));\n        assert_eq!(openai_config.rate_limit.as_ref().unwrap().requests_per_minute, 60);\n        \n        assert_eq!(anthropic_config.models.len(), 1);\n        assert_eq!(anthropic_config.base_url, Some(\"https://api.anthropic.com/v1\".to_string()));\n        assert_eq!(anthropic_config.rate_limit.as_ref().unwrap().tokens_per_minute, 100000);\n    }\n\n    #[test]\n    fn test_config_hot_reload() {\n        // GIVEN: A configuration that can be watched for changes\n        let temp_dir = TempDir::new().unwrap();\n        let config_path = temp_dir.path().join(\"config.toml\");\n        \n        let initial_config = r#\"\n            default_provider = \"openai\"\n            \n            [[providers]]\n            name = \"openai\"\n            type = \"openai\"\n            api_key = \"initial-key\"\n        \"#;\n        \n        fs::write(\u0026config_path, initial_config).unwrap();\n        \n        // WHEN: We set up configuration with hot reload\n        let (config, mut watcher) = AppConfig::with_hot_reload(\u0026config_path).unwrap();\n        \n        // Initial state check\n        assert_eq!(config.read().unwrap().providers[0].api_key, \"initial-key\");\n        \n        // Update the configuration file\n        let updated_config = r#\"\n            default_provider = \"openai\"\n            \n            [[providers]]\n            name = \"openai\"\n            type = \"openai\"\n            api_key = \"updated-key\"\n        \"#;\n        \n        fs::write(\u0026config_path, updated_config).unwrap();\n        \n        // THEN: The configuration should be automatically reloaded\n        // Note: In real implementation, this would use file system events\n        // For testing, we simulate the reload\n        std::thread::sleep(std::time::Duration::from_millis(100));\n        \n        // In actual implementation, the watcher would trigger this\n        let new_config = AppConfig::from_file(\u0026config_path).unwrap();\n        *config.write().unwrap() = new_config;\n        \n        assert_eq!(config.read().unwrap().providers[0].api_key, \"updated-key\");\n    }\n}\n\n// Type definitions that will be moved to the actual implementation\nuse serde::{Deserialize, Serialize};\nuse std::sync::{Arc, RwLock};\nuse std::path::Path;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AppConfig {\n    pub providers: Vec\u003cProviderConfig\u003e,\n    pub default_provider: Option\u003cString\u003e,\n    pub server: ServerConfig,\n    pub features: FeaturesConfig,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProviderConfig {\n    pub name: String,\n    #[serde(rename = \"type\")]\n    pub provider_type: ProviderType,\n    pub api_key: String,\n    pub base_url: Option\u003cString\u003e,\n    #[serde(default)]\n    pub models: Vec\u003cString\u003e,\n    pub rate_limit: Option\u003cRateLimitConfig\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum ProviderType {\n    OpenAI,\n    Anthropic,\n    Google,\n    Local,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RateLimitConfig {\n    pub requests_per_minute: u32,\n    pub tokens_per_minute: u32,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ServerConfig {\n    pub host: String,\n    pub port: u16,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct FeaturesConfig {\n    pub streaming_enabled: bool,\n    pub function_calling_enabled: bool,\n}\n\n// Partial config structs for merging\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PartialAppConfig {\n    pub providers: Option\u003cVec\u003cProviderConfig\u003e\u003e,\n    pub default_provider: Option\u003cOption\u003cString\u003e\u003e,\n    pub server: Option\u003cPartialServerConfig\u003e,\n    pub features: Option\u003cPartialFeaturesConfig\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PartialServerConfig {\n    pub host: Option\u003cString\u003e,\n    pub port: Option\u003cu16\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PartialFeaturesConfig {\n    pub streaming_enabled: Option\u003cbool\u003e,\n    pub function_calling_enabled: Option\u003cbool\u003e,\n}\n\n// Default implementations\nimpl Default for AppConfig {\n    fn default() -\u003e Self {\n        Self {\n            providers: vec![],\n            default_provider: None,\n            server: ServerConfig::default(),\n            features: FeaturesConfig::default(),\n        }\n    }\n}\n\nimpl Default for ServerConfig {\n    fn default() -\u003e Self {\n        Self {\n            host: \"127.0.0.1\".to_string(),\n            port: 3000,\n        }\n    }\n}\n\nimpl Default for FeaturesConfig {\n    fn default() -\u003e Self {\n        Self {\n            streaming_enabled: false,\n            function_calling_enabled: false,\n        }\n    }\n}\n\n// Implementation stubs\nimpl AppConfig {\n    pub fn from_file(path: \u0026Path) -\u003e Result\u003cSelf, ConfigError\u003e {\n        let content = fs::read_to_string(path)?;\n        let config: Self = toml::from_str(\u0026content)?;\n        Ok(config)\n    }\n    \n    pub fn expand_env_vars(\u0026mut self) {\n        for provider in \u0026mut self.providers {\n            if provider.api_key.starts_with(\"${\") \u0026\u0026 provider.api_key.ends_with(\"}\") {\n                let var_name = \u0026provider.api_key[2..provider.api_key.len()-1];\n                if let Ok(value) = std::env::var(var_name) {\n                    provider.api_key = value;\n                }\n            }\n            \n            if let Some(ref mut base_url) = provider.base_url {\n                if base_url.starts_with(\"${\") \u0026\u0026 base_url.ends_with(\"}\") {\n                    let var_name = \u0026base_url[2..base_url.len()-1];\n                    if let Ok(value) = std::env::var(var_name) {\n                        *base_url = value;\n                    }\n                }\n            }\n        }\n    }\n    \n    pub fn validate(\u0026self) -\u003e Result\u003c(), ConfigError\u003e {\n        // Check for duplicate provider names\n        let mut names = std::collections::HashSet::new();\n        for provider in \u0026self.providers {\n            if !names.insert(\u0026provider.name) {\n                return Err(ConfigError::Validation(format!(\"Duplicate provider name: {}\", provider.name)));\n            }\n        }\n        \n        // Check that default provider exists\n        if let Some(ref default) = self.default_provider {\n            if !self.providers.iter().any(|p| \u0026p.name == default) {\n                return Err(ConfigError::Validation(format!(\"Default provider '{}' not found\", default)));\n            }\n        }\n        \n        Ok(())\n    }\n    \n    pub fn merge(mut self, partial: PartialAppConfig) -\u003e Self {\n        if let Some(providers) = partial.providers {\n            self.providers = providers;\n        }\n        \n        if let Some(default) = partial.default_provider {\n            self.default_provider = default;\n        }\n        \n        if let Some(server) = partial.server {\n            if let Some(host) = server.host {\n                self.server.host = host;\n            }\n            if let Some(port) = server.port {\n                self.server.port = port;\n            }\n        }\n        \n        if let Some(features) = partial.features {\n            if let Some(streaming) = features.streaming_enabled {\n                self.features.streaming_enabled = streaming;\n            }\n            if let Some(function_calling) = features.function_calling_enabled {\n                self.features.function_calling_enabled = function_calling;\n            }\n        }\n        \n        self\n    }\n    \n    pub fn get_provider(\u0026self, name: \u0026str) -\u003e Option\u003c\u0026ProviderConfig\u003e {\n        self.providers.iter().find(|p| p.name == name)\n    }\n    \n    pub fn with_hot_reload(path: \u0026Path) -\u003e Result\u003c(Arc\u003cRwLock\u003cSelf\u003e\u003e, ConfigWatcher), ConfigError\u003e {\n        let config = Self::from_file(path)?;\n        let config = Arc::new(RwLock::new(config));\n        \n        // In real implementation, this would set up file system watching\n        let watcher = ConfigWatcher {};\n        \n        Ok((config, watcher))\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum ConfigError {\n    #[error(\"IO error: {0}\")]\n    Io(#[from] std::io::Error),\n    \n    #[error(\"Parse error: {0}\")]\n    Parse(#[from] toml::de::Error),\n    \n    #[error(\"Validation error: {0}\")]\n    Validation(String),\n}\n\npub struct ConfigWatcher {\n    // File system watcher implementation\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","opencode-core","src","di","tests.rs"],"content":"//! Tests for dependency injection container\n//! \n//! This test suite defines the expected behavior for the DI container\n//! and service registration/resolution following TDD principles.\n\nuse super::*;\nuse std::sync::Arc;\nuse async_trait::async_trait;\n\n#[cfg(test)]\nmod di_container_tests {\n    use super::*;\n\n    #[test]\n    fn test_container_creation() {\n        // GIVEN: A new DI container\n        let container = Container::new();\n        \n        // WHEN: We check its initial state\n        // THEN: It should be empty\n        assert_eq!(container.service_count(), 0);\n    }\n\n    #[test]\n    fn test_singleton_registration_and_resolution() {\n        // GIVEN: A container with a singleton service\n        let mut container = Container::new();\n        \n        // Define a test service\n        #[derive(Clone)]\n        struct TestService {\n            value: String,\n        }\n        \n        impl TestService {\n            fn new() -\u003e Self {\n                Self {\n                    value: \"test\".to_string(),\n                }\n            }\n        }\n        \n        // WHEN: We register and resolve the service\n        container.register_singleton::\u003cTestService\u003e(|| Arc::new(TestService::new()));\n        \n        let service1 = container.resolve::\u003cTestService\u003e().unwrap();\n        let service2 = container.resolve::\u003cTestService\u003e().unwrap();\n        \n        // THEN: Both resolutions should return the same instance\n        assert!(Arc::ptr_eq(\u0026service1, \u0026service2));\n        assert_eq!(service1.value, \"test\");\n    }\n\n    #[test]\n    fn test_factory_registration_and_resolution() {\n        // GIVEN: A container with a factory service\n        let mut container = Container::new();\n        \n        // Counter to track factory invocations\n        let counter = Arc::new(std::sync::Mutex::new(0));\n        let counter_clone = counter.clone();\n        \n        #[derive(Clone)]\n        struct FactoryService {\n            id: u32,\n        }\n        \n        // WHEN: We register a factory\n        container.register_factory::\u003cFactoryService\u003e(move || {\n            let mut count = counter_clone.lock().unwrap();\n            *count += 1;\n            Arc::new(FactoryService { id: *count })\n        });\n        \n        let service1 = container.resolve::\u003cFactoryService\u003e().unwrap();\n        let service2 = container.resolve::\u003cFactoryService\u003e().unwrap();\n        \n        // THEN: Each resolution should create a new instance\n        assert!(!Arc::ptr_eq(\u0026service1, \u0026service2));\n        assert_eq!(service1.id, 1);\n        assert_eq!(service2.id, 2);\n    }\n\n    #[test]\n    fn test_interface_registration() {\n        // GIVEN: An interface and multiple implementations\n        trait Database: Send + Sync {\n            fn name(\u0026self) -\u003e \u0026str;\n        }\n        \n        struct PostgresDB;\n        impl Database for PostgresDB {\n            fn name(\u0026self) -\u003e \u0026str {\n                \"PostgreSQL\"\n            }\n        }\n        \n        struct MySQLDB;\n        impl Database for MySQLDB {\n            fn name(\u0026self) -\u003e \u0026str {\n                \"MySQL\"\n            }\n        }\n        \n        // WHEN: We register implementations for the interface\n        let mut container = Container::new();\n        \n        container.register_interface::\u003cdyn Database, PostgresDB\u003e(\n            \"postgres\",\n            || Arc::new(PostgresDB),\n        );\n        \n        container.register_interface::\u003cdyn Database, MySQLDB\u003e(\n            \"mysql\",\n            || Arc::new(MySQLDB),\n        );\n        \n        // THEN: We can resolve specific implementations\n        let postgres = container.resolve_interface::\u003cdyn Database\u003e(\"postgres\").unwrap();\n        let mysql = container.resolve_interface::\u003cdyn Database\u003e(\"mysql\").unwrap();\n        \n        assert_eq!(postgres.name(), \"PostgreSQL\");\n        assert_eq!(mysql.name(), \"MySQL\");\n    }\n\n    #[test]\n    fn test_dependency_injection_with_dependencies() {\n        // GIVEN: Services with dependencies\n        #[derive(Clone)]\n        struct ConfigService {\n            api_key: String,\n        }\n        \n        #[derive(Clone)]\n        struct ApiClient {\n            config: Arc\u003cConfigService\u003e,\n        }\n        \n        impl ApiClient {\n            fn new(config: Arc\u003cConfigService\u003e) -\u003e Self {\n                Self { config }\n            }\n        }\n        \n        #[derive(Clone)]\n        struct UserService {\n            api_client: Arc\u003cApiClient\u003e,\n        }\n        \n        impl UserService {\n            fn new(api_client: Arc\u003cApiClient\u003e) -\u003e Self {\n                Self { api_client }\n            }\n        }\n        \n        // WHEN: We register services with dependencies\n        let mut container = Container::new();\n        \n        container.register_singleton::\u003cConfigService\u003e(|| {\n            Arc::new(ConfigService {\n                api_key: \"secret123\".to_string(),\n            })\n        });\n        \n        container.register_singleton_with_deps::\u003cApiClient, (Arc\u003cConfigService\u003e,)\u003e(\n            |deps| {\n                let (config,) = deps;\n                Arc::new(ApiClient::new(config))\n            }\n        );\n        \n        container.register_singleton_with_deps::\u003cUserService, (Arc\u003cApiClient\u003e,)\u003e(\n            |deps| {\n                let (api_client,) = deps;\n                Arc::new(UserService::new(api_client))\n            }\n        );\n        \n        // THEN: Dependencies should be resolved correctly\n        let user_service = container.resolve::\u003cUserService\u003e().unwrap();\n        assert_eq!(user_service.api_client.config.api_key, \"secret123\");\n    }\n\n    #[test]\n    fn test_scoped_services() {\n        // GIVEN: A container with scoped services\n        let mut container = Container::new();\n        \n        #[derive(Clone)]\n        struct RequestContext {\n            request_id: String,\n        }\n        \n        // WHEN: We register a scoped service\n        container.register_scoped::\u003cRequestContext\u003e();\n        \n        // Create scope 1\n        let mut scope1 = container.create_scope();\n        scope1.provide::\u003cRequestContext\u003e(Arc::new(RequestContext {\n            request_id: \"req-123\".to_string(),\n        }));\n        \n        // Create scope 2\n        let mut scope2 = container.create_scope();\n        scope2.provide::\u003cRequestContext\u003e(Arc::new(RequestContext {\n            request_id: \"req-456\".to_string(),\n        }));\n        \n        // THEN: Each scope should have its own instance\n        let ctx1 = scope1.resolve::\u003cRequestContext\u003e().unwrap();\n        let ctx2 = scope2.resolve::\u003cRequestContext\u003e().unwrap();\n        \n        assert_eq!(ctx1.request_id, \"req-123\");\n        assert_eq!(ctx2.request_id, \"req-456\");\n    }\n\n    #[test]\n    fn test_circular_dependency_detection() {\n        // GIVEN: Services with circular dependencies\n        let mut container = Container::new();\n        \n        // This should be detected and handled appropriately\n        // Implementation would need cycle detection\n    }\n\n    #[test]\n    fn test_service_not_found() {\n        // GIVEN: A container without a specific service\n        let container = Container::new();\n        \n        struct UnregisteredService;\n        \n        // WHEN: We try to resolve an unregistered service\n        let result = container.resolve::\u003cUnregisteredService\u003e();\n        \n        // THEN: It should return an error\n        assert!(result.is_err());\n        match result {\n            Err(DIError::ServiceNotFound(type_name)) =\u003e {\n                assert!(type_name.contains(\"UnregisteredService\"));\n            }\n            _ =\u003e panic!(\"Expected ServiceNotFound error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_async_initialization() {\n        // GIVEN: Services that require async initialization\n        #[derive(Clone)]\n        struct AsyncService {\n            data: String,\n        }\n        \n        impl AsyncService {\n            async fn new() -\u003e Self {\n                // Simulate async initialization\n                tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;\n                Self {\n                    data: \"async initialized\".to_string(),\n                }\n            }\n        }\n        \n        // WHEN: We register an async service\n        let mut container = Container::new();\n        \n        container.register_async_singleton::\u003cAsyncService\u003e(|| {\n            Box::pin(async {\n                Arc::new(AsyncService::new().await)\n            })\n        });\n        \n        // THEN: We should be able to resolve it\n        let service = container.resolve_async::\u003cAsyncService\u003e().await.unwrap();\n        assert_eq!(service.data, \"async initialized\");\n    }\n\n    #[test]\n    fn test_service_lifetime_management() {\n        // GIVEN: Services with different lifetimes\n        let mut container = Container::new();\n        \n        // Track service creation\n        let singleton_count = Arc::new(std::sync::atomic::AtomicU32::new(0));\n        let transient_count = Arc::new(std::sync::atomic::AtomicU32::new(0));\n        \n        let singleton_count_clone = singleton_count.clone();\n        let transient_count_clone = transient_count.clone();\n        \n        #[derive(Clone)]\n        struct SingletonService {\n            id: u32,\n        }\n        \n        #[derive(Clone)]\n        struct TransientService {\n            id: u32,\n        }\n        \n        // Register singleton\n        container.register_singleton::\u003cSingletonService\u003e(move || {\n            let id = singleton_count_clone.fetch_add(1, std::sync::atomic::Ordering::SeqCst);\n            Arc::new(SingletonService { id })\n        });\n        \n        // Register transient\n        container.register_factory::\u003cTransientService\u003e(move || {\n            let id = transient_count_clone.fetch_add(1, std::sync::atomic::Ordering::SeqCst);\n            Arc::new(TransientService { id })\n        });\n        \n        // WHEN: We resolve services multiple times\n        let singleton1 = container.resolve::\u003cSingletonService\u003e().unwrap();\n        let singleton2 = container.resolve::\u003cSingletonService\u003e().unwrap();\n        let transient1 = container.resolve::\u003cTransientService\u003e().unwrap();\n        let transient2 = container.resolve::\u003cTransientService\u003e().unwrap();\n        \n        // THEN: Singleton should be created once, transient multiple times\n        assert_eq!(singleton1.id, 0);\n        assert_eq!(singleton2.id, 0);\n        assert_eq!(transient1.id, 0);\n        assert_eq!(transient2.id, 1);\n        \n        assert_eq!(singleton_count.load(std::sync::atomic::Ordering::SeqCst), 1);\n        assert_eq!(transient_count.load(std::sync::atomic::Ordering::SeqCst), 2);\n    }\n\n    #[test]\n    fn test_container_builder_pattern() {\n        // GIVEN: A container builder\n        let container = ContainerBuilder::new()\n            .register_singleton::\u003cConfigService\u003e(|| {\n                Arc::new(ConfigService {\n                    api_key: \"test-key\".to_string(),\n                })\n            })\n            .register_factory::\u003cRequestContext\u003e(|| {\n                Arc::new(RequestContext {\n                    request_id: uuid::Uuid::new_v4().to_string(),\n                })\n            })\n            .build();\n        \n        // WHEN: We use the built container\n        let config = container.resolve::\u003cConfigService\u003e().unwrap();\n        let ctx1 = container.resolve::\u003cRequestContext\u003e().unwrap();\n        let ctx2 = container.resolve::\u003cRequestContext\u003e().unwrap();\n        \n        // THEN: Services should be properly registered\n        assert_eq!(config.api_key, \"test-key\");\n        assert_ne!(ctx1.request_id, ctx2.request_id); // Factory creates new instances\n    }\n}\n\n// Type definitions that will be moved to the actual implementation\nuse std::any::{Any, TypeId};\nuse std::collections::HashMap;\nuse std::future::Future;\nuse std::pin::Pin;\n\npub struct Container {\n    services: HashMap\u003cTypeId, Box\u003cdyn Any + Send + Sync\u003e\u003e,\n    factories: HashMap\u003cTypeId, Box\u003cdyn Any + Send + Sync\u003e\u003e,\n    interfaces: HashMap\u003c(TypeId, String), Box\u003cdyn Any + Send + Sync\u003e\u003e,\n    scoped_types: HashMap\u003cTypeId, ()\u003e,\n}\n\npub struct Scope\u003c'a\u003e {\n    container: \u0026'a Container,\n    scoped_instances: HashMap\u003cTypeId, Box\u003cdyn Any + Send + Sync\u003e\u003e,\n}\n\npub struct ContainerBuilder {\n    container: Container,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum DIError {\n    #[error(\"Service not found: {0}\")]\n    ServiceNotFound(String),\n    \n    #[error(\"Circular dependency detected\")]\n    CircularDependency,\n    \n    #[error(\"Service already registered: {0}\")]\n    AlreadyRegistered(String),\n    \n    #[error(\"Invalid service lifetime\")]\n    InvalidLifetime,\n}\n\n// Placeholder implementations\nimpl Container {\n    pub fn new() -\u003e Self {\n        Self {\n            services: HashMap::new(),\n            factories: HashMap::new(),\n            interfaces: HashMap::new(),\n            scoped_types: HashMap::new(),\n        }\n    }\n    \n    pub fn service_count(\u0026self) -\u003e usize {\n        self.services.len() + self.factories.len()\n    }\n    \n    pub fn register_singleton\u003cT: Any + Send + Sync + 'static\u003e(\n        \u0026mut self,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) {\n        let service = factory();\n        self.services.insert(TypeId::of::\u003cT\u003e(), Box::new(service));\n    }\n    \n    pub fn register_factory\u003cT: Any + Send + Sync + 'static\u003e(\n        \u0026mut self,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) {\n        self.factories.insert(TypeId::of::\u003cT\u003e(), Box::new(factory));\n    }\n    \n    pub fn register_interface\u003cI: ?Sized + 'static, T: I + Send + Sync + 'static\u003e(\n        \u0026mut self,\n        name: \u0026str,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) {\n        let service = factory();\n        self.interfaces.insert(\n            (TypeId::of::\u003cI\u003e(), name.to_string()),\n            Box::new(service as Arc\u003cI\u003e),\n        );\n    }\n    \n    pub fn register_singleton_with_deps\u003cT: Any + Send + Sync + 'static, D\u003e(\n        \u0026mut self,\n        factory: impl Fn(D) -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) where\n        D: ResolveDependencies,\n    {\n        // Implementation would resolve dependencies and call factory\n    }\n    \n    pub fn register_scoped\u003cT: Any + Send + Sync + 'static\u003e(\u0026mut self) {\n        self.scoped_types.insert(TypeId::of::\u003cT\u003e(), ());\n    }\n    \n    pub fn register_async_singleton\u003cT: Any + Send + Sync + 'static\u003e(\n        \u0026mut self,\n        factory: impl Fn() -\u003e Pin\u003cBox\u003cdyn Future\u003cOutput = Arc\u003cT\u003e\u003e + Send\u003e\u003e + Send + Sync + 'static,\n    ) {\n        // Implementation would store async factory\n    }\n    \n    pub fn resolve\u003cT: Any + Send + Sync + 'static\u003e(\u0026self) -\u003e Result\u003cArc\u003cT\u003e, DIError\u003e {\n        // Try singletons first\n        if let Some(service) = self.services.get(\u0026TypeId::of::\u003cT\u003e()) {\n            if let Some(arc) = service.downcast_ref::\u003cArc\u003cT\u003e\u003e() {\n                return Ok(arc.clone());\n            }\n        }\n        \n        // Try factories\n        if let Some(factory) = self.factories.get(\u0026TypeId::of::\u003cT\u003e()) {\n            if let Some(f) = factory.downcast_ref::\u003cBox\u003cdyn Fn() -\u003e Arc\u003cT\u003e + Send + Sync\u003e\u003e() {\n                return Ok(f());\n            }\n        }\n        \n        Err(DIError::ServiceNotFound(std::any::type_name::\u003cT\u003e().to_string()))\n    }\n    \n    pub fn resolve_interface\u003cI: ?Sized + 'static\u003e(\n        \u0026self,\n        name: \u0026str,\n    ) -\u003e Result\u003cArc\u003cI\u003e, DIError\u003e {\n        if let Some(service) = self.interfaces.get(\u0026(TypeId::of::\u003cI\u003e(), name.to_string())) {\n            if let Some(arc) = service.downcast_ref::\u003cArc\u003cI\u003e\u003e() {\n                return Ok(arc.clone());\n            }\n        }\n        \n        Err(DIError::ServiceNotFound(format!(\"{} ({})\", std::any::type_name::\u003cI\u003e(), name)))\n    }\n    \n    pub async fn resolve_async\u003cT: Any + Send + Sync + 'static\u003e(\u0026self) -\u003e Result\u003cArc\u003cT\u003e, DIError\u003e {\n        // Implementation would handle async resolution\n        self.resolve::\u003cT\u003e()\n    }\n    \n    pub fn create_scope(\u0026self) -\u003e Scope {\n        Scope {\n            container: self,\n            scoped_instances: HashMap::new(),\n        }\n    }\n}\n\nimpl\u003c'a\u003e Scope\u003c'a\u003e {\n    pub fn provide\u003cT: Any + Send + Sync + 'static\u003e(\u0026mut self, instance: Arc\u003cT\u003e) {\n        self.scoped_instances.insert(TypeId::of::\u003cT\u003e(), Box::new(instance));\n    }\n    \n    pub fn resolve\u003cT: Any + Send + Sync + 'static\u003e(\u0026self) -\u003e Result\u003cArc\u003cT\u003e, DIError\u003e {\n        // Check scoped instances first\n        if let Some(instance) = self.scoped_instances.get(\u0026TypeId::of::\u003cT\u003e()) {\n            if let Some(arc) = instance.downcast_ref::\u003cArc\u003cT\u003e\u003e() {\n                return Ok(arc.clone());\n            }\n        }\n        \n        // Fall back to container\n        self.container.resolve::\u003cT\u003e()\n    }\n}\n\nimpl ContainerBuilder {\n    pub fn new() -\u003e Self {\n        Self {\n            container: Container::new(),\n        }\n    }\n    \n    pub fn register_singleton\u003cT: Any + Send + Sync + 'static\u003e(\n        mut self,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) -\u003e Self {\n        self.container.register_singleton(factory);\n        self\n    }\n    \n    pub fn register_factory\u003cT: Any + Send + Sync + 'static\u003e(\n        mut self,\n        factory: impl Fn() -\u003e Arc\u003cT\u003e + Send + Sync + 'static,\n    ) -\u003e Self {\n        self.container.register_factory(factory);\n        self\n    }\n    \n    pub fn build(self) -\u003e Container {\n        self.container\n    }\n}\n\n// Trait for resolving dependencies\npub trait ResolveDependencies {\n    fn resolve(container: \u0026Container) -\u003e Self;\n}\n\n// Implement for tuples of dependencies\nimpl\u003cT1: Any + Send + Sync + 'static\u003e ResolveDependencies for (Arc\u003cT1\u003e,) {\n    fn resolve(container: \u0026Container) -\u003e Self {\n        (container.resolve::\u003cT1\u003e().unwrap(),)\n    }\n}","traces":[{"line":411,"address":[],"length":0,"stats":{"Line":0}},{"line":412,"address":[],"length":0,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":428,"address":[],"length":0,"stats":{"Line":0}},{"line":429,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":0}},{"line":443,"address":[],"length":0,"stats":{"Line":0}},{"line":444,"address":[],"length":0,"stats":{"Line":0}},{"line":454,"address":[],"length":0,"stats":{"Line":0}},{"line":456,"address":[],"length":0,"stats":{"Line":0}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":458,"address":[],"length":0,"stats":{"Line":0}},{"line":463,"address":[],"length":0,"stats":{"Line":0}},{"line":464,"address":[],"length":0,"stats":{"Line":0}},{"line":465,"address":[],"length":0,"stats":{"Line":0}},{"line":469,"address":[],"length":0,"stats":{"Line":0}},{"line":476,"address":[],"length":0,"stats":{"Line":0}},{"line":477,"address":[],"length":0,"stats":{"Line":0}},{"line":478,"address":[],"length":0,"stats":{"Line":0}},{"line":482,"address":[],"length":0,"stats":{"Line":0}},{"line":485,"address":[],"length":0,"stats":{"Line":0}},{"line":487,"address":[],"length":0,"stats":{"Line":0}},{"line":499,"address":[],"length":0,"stats":{"Line":0}},{"line":500,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":505,"address":[],"length":0,"stats":{"Line":0}},{"line":506,"address":[],"length":0,"stats":{"Line":0}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":512,"address":[],"length":0,"stats":{"Line":0}},{"line":527,"address":[],"length":0,"stats":{"Line":0}},{"line":528,"address":[],"length":0,"stats":{"Line":0}},{"line":535,"address":[],"length":0,"stats":{"Line":0}},{"line":536,"address":[],"length":0,"stats":{"Line":0}},{"line":551,"address":[],"length":0,"stats":{"Line":0}},{"line":552,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":36},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","opencode-core","src","error","tests.rs"],"content":"//! Tests for error handling improvements\n//! \n//! This test suite defines the expected behavior for comprehensive error handling,\n//! error context, and error recovery following TDD principles.\n\nuse super::*;\nuse std::io;\nuse std::sync::Arc;\n\n#[cfg(test)]\nmod error_handling_tests {\n    use super::*;\n\n    #[test]\n    fn test_error_creation_and_display() {\n        // GIVEN: Various error scenarios\n        \n        // WHEN: We create different error types\n        let api_error = OpenCodeError::Provider(ProviderError::ApiError {\n            status: 429,\n            message: \"Rate limit exceeded\".to_string(),\n        });\n        \n        let config_error = OpenCodeError::Configuration(ConfigError::Validation(\n            \"Invalid provider configuration\".to_string()\n        ));\n        \n        let network_error = OpenCodeError::Network(NetworkError::Timeout {\n            operation: \"API call\".to_string(),\n            duration: std::time::Duration::from_secs(30),\n        });\n        \n        // THEN: Error messages should be properly formatted\n        assert_eq!(api_error.to_string(), \"Provider error: API error: Rate limit exceeded (status: 429)\");\n        assert_eq!(config_error.to_string(), \"Configuration error: Validation error: Invalid provider configuration\");\n        assert_eq!(network_error.to_string(), \"Network error: Operation 'API call' timed out after 30s\");\n    }\n\n    #[test]\n    fn test_error_context_chain() {\n        // GIVEN: An error with context chain\n        let base_error = io::Error::new(io::ErrorKind::NotFound, \"File not found\");\n        \n        // WHEN: We add context to the error\n        let error = OpenCodeError::Io(base_error)\n            .with_context(\"Loading configuration\")\n            .with_context(\"Initializing application\");\n        \n        // THEN: Context should be preserved in order\n        let contexts = error.contexts();\n        assert_eq!(contexts.len(), 2);\n        assert_eq!(contexts[0], \"Loading configuration\");\n        assert_eq!(contexts[1], \"Initializing application\");\n        \n        // Full error message should include context\n        let full_message = error.full_message();\n        assert!(full_message.contains(\"Initializing application\"));\n        assert!(full_message.contains(\"Loading configuration\"));\n        assert!(full_message.contains(\"File not found\"));\n    }\n\n    #[test]\n    fn test_error_recovery_suggestions() {\n        // GIVEN: Errors with recovery suggestions\n        \n        // WHEN: We create errors with recovery hints\n        let rate_limit_error = OpenCodeError::Provider(ProviderError::RateLimitExceeded)\n            .with_recovery(Recovery::Retry {\n                after: std::time::Duration::from_secs(60),\n                max_attempts: 3,\n            });\n        \n        let auth_error = OpenCodeError::Provider(ProviderError::AuthenticationError(\n            \"Invalid API key\".to_string()\n        ))\n            .with_recovery(Recovery::Manual(\n                \"Please check your API key in the configuration file\".to_string()\n            ));\n        \n        // THEN: Recovery suggestions should be accessible\n        match rate_limit_error.recovery() {\n            Some(Recovery::Retry { after, max_attempts }) =\u003e {\n                assert_eq!(after.as_secs(), 60);\n                assert_eq!(*max_attempts, 3);\n            }\n            _ =\u003e panic!(\"Expected Retry recovery\"),\n        }\n        \n        match auth_error.recovery() {\n            Some(Recovery::Manual(msg)) =\u003e {\n                assert!(msg.contains(\"API key\"));\n            }\n            _ =\u003e panic!(\"Expected Manual recovery\"),\n        }\n    }\n\n    #[test]\n    fn test_error_source_chain() {\n        // GIVEN: Nested errors with source chain\n        let io_error = io::Error::new(io::ErrorKind::PermissionDenied, \"Access denied\");\n        let config_error = ConfigError::Io(io_error);\n        let app_error = OpenCodeError::Configuration(config_error);\n        \n        // WHEN: We traverse the error source chain\n        let mut sources = vec![];\n        let mut current: Option\u003c\u0026dyn std::error::Error\u003e = Some(\u0026app_error);\n        \n        while let Some(err) = current {\n            sources.push(err.to_string());\n            current = err.source();\n        }\n        \n        // THEN: We should see the full error chain\n        assert_eq!(sources.len(), 3);\n        assert!(sources[0].contains(\"Configuration error\"));\n        assert!(sources[1].contains(\"IO error\"));\n        assert!(sources[2].contains(\"Access denied\"));\n    }\n\n    #[test]\n    fn test_error_categorization() {\n        // GIVEN: Various errors\n        let errors = vec![\n            OpenCodeError::Provider(ProviderError::RateLimitExceeded),\n            OpenCodeError::Network(NetworkError::ConnectionRefused),\n            OpenCodeError::Configuration(ConfigError::NotFound),\n            OpenCodeError::Internal(\"Unexpected state\".to_string()),\n        ];\n        \n        // WHEN: We categorize errors\n        for error in errors {\n            let category = error.category();\n            \n            // THEN: Each error should have appropriate category\n            match \u0026error {\n                OpenCodeError::Provider(ProviderError::RateLimitExceeded) =\u003e {\n                    assert_eq!(category, ErrorCategory::Transient);\n                }\n                OpenCodeError::Network(_) =\u003e {\n                    assert_eq!(category, ErrorCategory::Transient);\n                }\n                OpenCodeError::Configuration(_) =\u003e {\n                    assert_eq!(category, ErrorCategory::Configuration);\n                }\n                OpenCodeError::Internal(_) =\u003e {\n                    assert_eq!(category, ErrorCategory::Internal);\n                }\n                _ =\u003e {}\n            }\n        }\n    }\n\n    #[test]\n    fn test_error_retry_policy() {\n        // GIVEN: Errors with different retry policies\n        let transient_error = OpenCodeError::Network(NetworkError::Timeout {\n            operation: \"Request\".to_string(),\n            duration: std::time::Duration::from_secs(30),\n        });\n        \n        let permanent_error = OpenCodeError::Configuration(ConfigError::Validation(\n            \"Invalid setting\".to_string()\n        ));\n        \n        // WHEN: We check retry policies\n        let transient_policy = transient_error.retry_policy();\n        let permanent_policy = permanent_error.retry_policy();\n        \n        // THEN: Appropriate policies should be returned\n        match transient_policy {\n            RetryPolicy::Exponential { max_attempts, base_delay, .. } =\u003e {\n                assert_eq!(max_attempts, 3);\n                assert_eq!(base_delay.as_secs(), 1);\n            }\n            _ =\u003e panic!(\"Expected exponential retry for transient error\"),\n        }\n        \n        assert_eq!(permanent_policy, RetryPolicy::None);\n    }\n\n    #[test]\n    fn test_error_telemetry() {\n        // GIVEN: An error with telemetry data\n        let error = OpenCodeError::Provider(ProviderError::ApiError {\n            status: 500,\n            message: \"Internal server error\".to_string(),\n        })\n        .with_telemetry(ErrorTelemetry {\n            timestamp: std::time::SystemTime::now(),\n            request_id: Some(\"req-123\".to_string()),\n            user_id: Some(\"user-456\".to_string()),\n            additional_data: {\n                let mut map = std::collections::HashMap::new();\n                map.insert(\"provider\".to_string(), \"openai\".to_string());\n                map.insert(\"model\".to_string(), \"gpt-4\".to_string());\n                map\n            },\n        });\n        \n        // WHEN: We access telemetry data\n        let telemetry = error.telemetry().unwrap();\n        \n        // THEN: All telemetry fields should be accessible\n        assert_eq!(telemetry.request_id, Some(\"req-123\".to_string()));\n        assert_eq!(telemetry.user_id, Some(\"user-456\".to_string()));\n        assert_eq!(telemetry.additional_data.get(\"provider\"), Some(\u0026\"openai\".to_string()));\n        assert_eq!(telemetry.additional_data.get(\"model\"), Some(\u0026\"gpt-4\".to_string()));\n    }\n\n    #[test]\n    fn test_error_serialization() {\n        // GIVEN: An error that needs to be serialized\n        let error = OpenCodeError::Provider(ProviderError::ApiError {\n            status: 404,\n            message: \"Model not found\".to_string(),\n        })\n        .with_context(\"Calling OpenAI API\")\n        .with_recovery(Recovery::Fallback {\n            alternative: \"Use gpt-3.5-turbo instead\".to_string(),\n        });\n        \n        // WHEN: We serialize the error\n        let serialized = error.to_json();\n        \n        // THEN: JSON should contain all error information\n        let json: serde_json::Value = serde_json::from_str(\u0026serialized).unwrap();\n        assert_eq!(json[\"type\"], \"Provider\");\n        assert_eq!(json[\"message\"], \"Provider error: API error: Model not found (status: 404)\");\n        assert_eq!(json[\"contexts\"][0], \"Calling OpenAI API\");\n        assert_eq!(json[\"recovery\"][\"type\"], \"Fallback\");\n        assert_eq!(json[\"recovery\"][\"alternative\"], \"Use gpt-3.5-turbo instead\");\n    }\n\n    #[test]\n    fn test_error_aggregation() {\n        // GIVEN: Multiple errors that need to be aggregated\n        let errors = vec![\n            OpenCodeError::Provider(ProviderError::RateLimitExceeded),\n            OpenCodeError::Network(NetworkError::ConnectionRefused),\n            OpenCodeError::Provider(ProviderError::ApiError {\n                status: 500,\n                message: \"Server error\".to_string(),\n            }),\n        ];\n        \n        // WHEN: We aggregate errors\n        let aggregated = OpenCodeError::Multiple(errors);\n        \n        // THEN: All errors should be accessible\n        match \u0026aggregated {\n            OpenCodeError::Multiple(errs) =\u003e {\n                assert_eq!(errs.len(), 3);\n                // Check that we can iterate and handle each error\n                for (i, err) in errs.iter().enumerate() {\n                    match i {\n                        0 =\u003e assert!(matches!(err, OpenCodeError::Provider(ProviderError::RateLimitExceeded))),\n                        1 =\u003e assert!(matches!(err, OpenCodeError::Network(_))),\n                        2 =\u003e assert!(matches!(err, OpenCodeError::Provider(ProviderError::ApiError { .. }))),\n                        _ =\u003e panic!(\"Unexpected error count\"),\n                    }\n                }\n            }\n            _ =\u003e panic!(\"Expected Multiple error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_async_error_handling() {\n        // GIVEN: An async operation that might fail\n        async fn risky_operation() -\u003e Result\u003cString, OpenCodeError\u003e {\n            // Simulate async work\n            tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;\n            \n            Err(OpenCodeError::Network(NetworkError::Timeout {\n                operation: \"Async operation\".to_string(),\n                duration: std::time::Duration::from_secs(10),\n            }))\n        }\n        \n        // WHEN: We handle the error with async context\n        let result = risky_operation()\n            .await\n            .map_err(|e| e.with_context(\"Performing background task\"));\n        \n        // THEN: Error context should be preserved\n        assert!(result.is_err());\n        let error = result.unwrap_err();\n        assert!(error.contexts().contains(\u0026\"Performing background task\".to_string()));\n    }\n\n    #[test]\n    fn test_error_conversion() {\n        // GIVEN: Errors from external libraries\n        let io_error = io::Error::new(io::ErrorKind::NotFound, \"File not found\");\n        let parse_error = \"invalid digit found in string\".parse::\u003ci32\u003e().unwrap_err();\n        \n        // WHEN: We convert them to our error type\n        let our_io_error: OpenCodeError = io_error.into();\n        let our_parse_error: OpenCodeError = parse_error.into();\n        \n        // THEN: They should be properly wrapped\n        assert!(matches!(our_io_error, OpenCodeError::Io(_)));\n        assert!(matches!(our_parse_error, OpenCodeError::Parse(_)));\n    }\n}\n\n// Type definitions that will be moved to the actual implementation\nuse serde::{Serialize, Deserialize};\nuse std::collections::HashMap;\nuse thiserror::Error;\n\n#[derive(Debug, Error)]\npub enum OpenCodeError {\n    #[error(\"Provider error: {0}\")]\n    Provider(#[from] ProviderError),\n    \n    #[error(\"Configuration error: {0}\")]\n    Configuration(#[from] ConfigError),\n    \n    #[error(\"Network error: {0}\")]\n    Network(#[from] NetworkError),\n    \n    #[error(\"IO error: {0}\")]\n    Io(#[from] io::Error),\n    \n    #[error(\"Parse error: {0}\")]\n    Parse(#[from] std::num::ParseIntError),\n    \n    #[error(\"Internal error: {0}\")]\n    Internal(String),\n    \n    #[error(\"Multiple errors occurred: {0:?}\")]\n    Multiple(Vec\u003cOpenCodeError\u003e),\n}\n\n#[derive(Debug, Error)]\npub enum NetworkError {\n    #[error(\"Connection refused\")]\n    ConnectionRefused,\n    \n    #[error(\"Operation '{operation}' timed out after {duration:?}\")]\n    Timeout {\n        operation: String,\n        duration: std::time::Duration,\n    },\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub enum ErrorCategory {\n    Transient,\n    Configuration,\n    Internal,\n    External,\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub enum RetryPolicy {\n    None,\n    Exponential {\n        max_attempts: u32,\n        base_delay: std::time::Duration,\n        max_delay: std::time::Duration,\n    },\n    Fixed {\n        attempts: u32,\n        delay: std::time::Duration,\n    },\n}\n\n#[derive(Debug, Clone)]\npub enum Recovery {\n    Retry {\n        after: std::time::Duration,\n        max_attempts: u32,\n    },\n    Fallback {\n        alternative: String,\n    },\n    Manual(String),\n}\n\n#[derive(Debug, Clone)]\npub struct ErrorTelemetry {\n    pub timestamp: std::time::SystemTime,\n    pub request_id: Option\u003cString\u003e,\n    pub user_id: Option\u003cString\u003e,\n    pub additional_data: HashMap\u003cString, String\u003e,\n}\n\n#[derive(Serialize, Deserialize)]\nstruct SerializedError {\n    #[serde(rename = \"type\")]\n    error_type: String,\n    message: String,\n    contexts: Vec\u003cString\u003e,\n    recovery: Option\u003cSerializedRecovery\u003e,\n}\n\n#[derive(Serialize, Deserialize)]\nstruct SerializedRecovery {\n    #[serde(rename = \"type\")]\n    recovery_type: String,\n    #[serde(flatten)]\n    data: serde_json::Value,\n}\n\n// Error enhancement implementation\nstruct ErrorEnhancement {\n    contexts: Vec\u003cString\u003e,\n    recovery: Option\u003cRecovery\u003e,\n    telemetry: Option\u003cErrorTelemetry\u003e,\n}\n\n// Extension trait for error enhancement\nimpl OpenCodeError {\n    pub fn with_context(self, context: impl Into\u003cString\u003e) -\u003e Self {\n        // Implementation would store context\n        self\n    }\n    \n    pub fn with_recovery(self, recovery: Recovery) -\u003e Self {\n        // Implementation would store recovery\n        self\n    }\n    \n    pub fn with_telemetry(self, telemetry: ErrorTelemetry) -\u003e Self {\n        // Implementation would store telemetry\n        self\n    }\n    \n    pub fn contexts(\u0026self) -\u003e Vec\u003cString\u003e {\n        // Implementation would return stored contexts\n        vec![]\n    }\n    \n    pub fn recovery(\u0026self) -\u003e Option\u003c\u0026Recovery\u003e {\n        // Implementation would return stored recovery\n        None\n    }\n    \n    pub fn telemetry(\u0026self) -\u003e Option\u003c\u0026ErrorTelemetry\u003e {\n        // Implementation would return stored telemetry\n        None\n    }\n    \n    pub fn full_message(\u0026self) -\u003e String {\n        // Implementation would build full error message with context\n        self.to_string()\n    }\n    \n    pub fn category(\u0026self) -\u003e ErrorCategory {\n        match self {\n            Self::Provider(ProviderError::RateLimitExceeded) =\u003e ErrorCategory::Transient,\n            Self::Network(_) =\u003e ErrorCategory::Transient,\n            Self::Configuration(_) =\u003e ErrorCategory::Configuration,\n            Self::Internal(_) =\u003e ErrorCategory::Internal,\n            _ =\u003e ErrorCategory::External,\n        }\n    }\n    \n    pub fn retry_policy(\u0026self) -\u003e RetryPolicy {\n        match self.category() {\n            ErrorCategory::Transient =\u003e RetryPolicy::Exponential {\n                max_attempts: 3,\n                base_delay: std::time::Duration::from_secs(1),\n                max_delay: std::time::Duration::from_secs(60),\n            },\n            _ =\u003e RetryPolicy::None,\n        }\n    }\n    \n    pub fn to_json(\u0026self) -\u003e String {\n        // Implementation would serialize to JSON\n        serde_json::to_string_pretty(\u0026SerializedError {\n            error_type: \"Provider\".to_string(),\n            message: self.to_string(),\n            contexts: self.contexts(),\n            recovery: None,\n        }).unwrap()\n    }\n}","traces":[{"line":416,"address":[],"length":0,"stats":{"Line":0}},{"line":418,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":2},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","opencode-core","src","provider","tests.rs"],"content":"//! Tests for the Provider abstraction trait\n//! \n//! This test suite defines the expected behavior for AI provider implementations\n//! following TDD principles.\n\nuse super::*;\nuse async_trait::async_trait;\nuse mockall::*;\n\n#[cfg(test)]\nmod provider_trait_tests {\n    use super::*;\n\n    #[test]\n    fn test_provider_trait_requirements() {\n        // Verify that Provider trait has all required methods\n        fn assert_provider_trait\u003cT: Provider\u003e() {}\n        \n        // This test ensures the trait has the right shape\n        // Compilation will fail if trait requirements change\n    }\n\n    #[tokio::test]\n    async fn test_provider_completion() {\n        // GIVEN: A mock provider implementation\n        let mut mock_provider = MockProvider::new();\n        \n        // WHEN: We set up expectations for a completion request\n        mock_provider\n            .expect_complete()\n            .with(mockall::predicate::function(|req: \u0026CompletionRequest| {\n                req.messages.len() == 1 \u0026\u0026 \n                req.messages[0].role == MessageRole::User \u0026\u0026\n                req.messages[0].content == \"Hello, AI!\"\n            }))\n            .times(1)\n            .returning(|_| {\n                Ok(CompletionResponse {\n                    id: \"test-123\".to_string(),\n                    model: \"test-model\".to_string(),\n                    choices: vec![\n                        Choice {\n                            message: Message {\n                                role: MessageRole::Assistant,\n                                content: \"Hello! How can I help you?\".to_string(),\n                            },\n                            finish_reason: FinishReason::Stop,\n                            index: 0,\n                        }\n                    ],\n                    usage: Usage {\n                        prompt_tokens: 10,\n                        completion_tokens: 8,\n                        total_tokens: 18,\n                    },\n                    created: 1234567890,\n                })\n            });\n\n        // THEN: The provider should return the expected response\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![\n                Message {\n                    role: MessageRole::User,\n                    content: \"Hello, AI!\".to_string(),\n                }\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = mock_provider.complete(request).await.unwrap();\n        assert_eq!(response.id, \"test-123\");\n        assert_eq!(response.choices.len(), 1);\n        assert_eq!(response.choices[0].message.content, \"Hello! How can I help you?\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_streaming() {\n        // GIVEN: A mock provider that supports streaming\n        let mut mock_provider = MockProvider::new();\n        \n        // WHEN: We request a streaming completion\n        mock_provider\n            .expect_complete_stream()\n            .times(1)\n            .returning(|_| {\n                let (tx, rx) = tokio::sync::mpsc::channel(10);\n                \n                tokio::spawn(async move {\n                    // Simulate streaming chunks\n                    for chunk in [\"Hello\", \" from\", \" streaming\", \" AI!\"] {\n                        let _ = tx.send(Ok(StreamChunk {\n                            id: \"stream-123\".to_string(),\n                            choices: vec![\n                                StreamChoice {\n                                    delta: Delta {\n                                        content: Some(chunk.to_string()),\n                                        role: None,\n                                    },\n                                    index: 0,\n                                    finish_reason: None,\n                                }\n                            ],\n                            created: 1234567890,\n                        })).await;\n                    }\n                });\n                \n                Ok(Box::pin(tokio_stream::wrappers::ReceiverStream::new(rx)))\n            });\n\n        // THEN: We should receive all streaming chunks\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: MessageRole::User,\n                content: \"Stream this!\".to_string(),\n            }],\n            temperature: None,\n            max_tokens: None,\n            stream: true,\n        };\n\n        let mut stream = mock_provider.complete_stream(request).await.unwrap();\n        let mut full_response = String::new();\n        \n        while let Some(chunk_result) = stream.next().await {\n            let chunk = chunk_result.unwrap();\n            if let Some(content) = \u0026chunk.choices[0].delta.content {\n                full_response.push_str(content);\n            }\n        }\n        \n        assert_eq!(full_response, \"Hello from streaming AI!\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_error_handling() {\n        // GIVEN: A mock provider that returns errors\n        let mut mock_provider = MockProvider::new();\n        \n        // WHEN: The provider encounters an API error\n        mock_provider\n            .expect_complete()\n            .times(1)\n            .returning(|_| {\n                Err(ProviderError::ApiError {\n                    status: 429,\n                    message: \"Rate limit exceeded\".to_string(),\n                })\n            });\n\n        // THEN: The error should be properly propagated\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: MessageRole::User,\n                content: \"Test\".to_string(),\n            }],\n            temperature: None,\n            max_tokens: None,\n            stream: false,\n        };\n\n        let result = mock_provider.complete(request).await;\n        assert!(result.is_err());\n        \n        match result.unwrap_err() {\n            ProviderError::ApiError { status, message } =\u003e {\n                assert_eq!(status, 429);\n                assert_eq!(message, \"Rate limit exceeded\");\n            }\n            _ =\u003e panic!(\"Expected ApiError\"),\n        }\n    }\n\n    #[test]\n    fn test_provider_capabilities() {\n        // GIVEN: Different provider implementations\n        let mut mock_provider = MockProvider::new();\n        \n        // WHEN: We query provider capabilities\n        mock_provider\n            .expect_capabilities()\n            .times(1)\n            .returning(|| {\n                ProviderCapabilities {\n                    supports_streaming: true,\n                    supports_function_calling: true,\n                    supports_vision: false,\n                    max_tokens: 4096,\n                    models: vec![\n                        ModelInfo {\n                            id: \"gpt-4\".to_string(),\n                            display_name: \"GPT-4\".to_string(),\n                            context_window: 8192,\n                            max_output_tokens: 4096,\n                        }\n                    ],\n                }\n            });\n\n        // THEN: Capabilities should be correctly reported\n        let caps = mock_provider.capabilities();\n        assert!(caps.supports_streaming);\n        assert!(caps.supports_function_calling);\n        assert!(!caps.supports_vision);\n        assert_eq!(caps.max_tokens, 4096);\n        assert_eq!(caps.models.len(), 1);\n    }\n\n    #[test]\n    fn test_provider_configuration() {\n        // Test that providers can be configured with different settings\n        // This will be implemented based on the configuration trait\n    }\n}\n\n// Mock implementations for testing\n#[cfg(test)]\nmockall::mock! {\n    Provider {}\n    \n    #[async_trait]\n    impl Provider for Provider {\n        async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse, ProviderError\u003e;\n        async fn complete_stream(\u0026self, request: CompletionRequest) -\u003e Result\u003cPin\u003cBox\u003cdyn Stream\u003cItem = Result\u003cStreamChunk, ProviderError\u003e\u003e + Send\u003e\u003e, ProviderError\u003e;\n        fn capabilities(\u0026self) -\u003e ProviderCapabilities;\n        fn name(\u0026self) -\u003e \u0026str;\n    }\n}\n\n// Type definitions that will be moved to the actual implementation\n#[derive(Debug, Clone, PartialEq)]\npub enum MessageRole {\n    System,\n    User,\n    Assistant,\n}\n\n#[derive(Debug, Clone)]\npub struct Message {\n    pub role: MessageRole,\n    pub content: String,\n}\n\n#[derive(Debug, Clone)]\npub struct CompletionRequest {\n    pub model: String,\n    pub messages: Vec\u003cMessage\u003e,\n    pub temperature: Option\u003cf32\u003e,\n    pub max_tokens: Option\u003cu32\u003e,\n    pub stream: bool,\n}\n\n#[derive(Debug, Clone)]\npub struct CompletionResponse {\n    pub id: String,\n    pub model: String,\n    pub choices: Vec\u003cChoice\u003e,\n    pub usage: Usage,\n    pub created: i64,\n}\n\n#[derive(Debug, Clone)]\npub struct Choice {\n    pub message: Message,\n    pub finish_reason: FinishReason,\n    pub index: u32,\n}\n\n#[derive(Debug, Clone)]\npub enum FinishReason {\n    Stop,\n    Length,\n    FunctionCall,\n}\n\n#[derive(Debug, Clone)]\npub struct Usage {\n    pub prompt_tokens: u32,\n    pub completion_tokens: u32,\n    pub total_tokens: u32,\n}\n\n#[derive(Debug, Clone)]\npub struct StreamChunk {\n    pub id: String,\n    pub choices: Vec\u003cStreamChoice\u003e,\n    pub created: i64,\n}\n\n#[derive(Debug, Clone)]\npub struct StreamChoice {\n    pub delta: Delta,\n    pub index: u32,\n    pub finish_reason: Option\u003cFinishReason\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct Delta {\n    pub content: Option\u003cString\u003e,\n    pub role: Option\u003cMessageRole\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct ProviderCapabilities {\n    pub supports_streaming: bool,\n    pub supports_function_calling: bool,\n    pub supports_vision: bool,\n    pub max_tokens: u32,\n    pub models: Vec\u003cModelInfo\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct ModelInfo {\n    pub id: String,\n    pub display_name: String,\n    pub context_window: u32,\n    pub max_output_tokens: u32,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum ProviderError {\n    #[error(\"API error: {message} (status: {status})\")]\n    ApiError { status: u16, message: String },\n    \n    #[error(\"Network error: {0}\")]\n    NetworkError(String),\n    \n    #[error(\"Invalid request: {0}\")]\n    InvalidRequest(String),\n    \n    #[error(\"Rate limit exceeded\")]\n    RateLimitExceeded,\n    \n    #[error(\"Authentication failed: {0}\")]\n    AuthenticationError(String),\n}\n\n#[async_trait]\npub trait Provider: Send + Sync {\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse, ProviderError\u003e;\n    async fn complete_stream(\u0026self, request: CompletionRequest) -\u003e Result\u003cPin\u003cBox\u003cdyn Stream\u003cItem = Result\u003cStreamChunk, ProviderError\u003e\u003e + Send\u003e\u003e, ProviderError\u003e;\n    fn capabilities(\u0026self) -\u003e ProviderCapabilities;\n    fn name(\u0026self) -\u003e \u0026str;\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","config","mod.rs"],"content":"use crate::error::{Error, Result};\nuse serde::{Deserialize, Serialize};\nuse std::env;\nuse std::fs;\nuse std::path::Path;\n\n#[cfg(test)]\nmod tests;\n\n/// OpenAI configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OpenAIConfig {\n    pub default_model: String,\n    pub api_base: String,\n    pub max_retries: u32,\n    pub timeout_seconds: u32,\n}\n\nimpl Default for OpenAIConfig {\n    fn default() -\u003e Self {\n        Self {\n            default_model: \"gpt-4\".to_string(),\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        }\n    }\n}\n\n/// Main configuration structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Config {\n    pub openai: OpenAIConfig,\n}\n\nimpl Default for Config {\n    fn default() -\u003e Self {\n        Self {\n            openai: OpenAIConfig::default(),\n        }\n    }\n}\n\nimpl Config {\n    /// Load configuration from file and environment variables\n    /// Environment variables take precedence over file values\n    pub fn load\u003cP: AsRef\u003cPath\u003e\u003e(config_path: Option\u003cP\u003e) -\u003e Result\u003cSelf\u003e {\n        let mut config = if let Some(path) = config_path {\n            Self::from_file(path)?\n        } else {\n            Self::default()\n        };\n\n        // Override with environment variables\n        let env_config = Self::from_env()?;\n        config.merge_env(env_config);\n\n        Ok(config)\n    }\n\n    /// Load configuration from a TOML file\n    pub fn from_file\u003cP: AsRef\u003cPath\u003e\u003e(path: P) -\u003e Result\u003cSelf\u003e {\n        let content = fs::read_to_string(path)?;\n        let config: Config = toml::from_str(\u0026content)?;\n        Ok(config)\n    }\n\n    /// Load configuration from environment variables\n    pub fn from_env() -\u003e Result\u003cSelf\u003e {\n        let mut config = Self::default();\n\n        // OpenAI configuration\n        if let Ok(model) = env::var(\"OPENAI_MODEL\") {\n            config.openai.default_model = model;\n        }\n\n        if let Ok(api_base) = env::var(\"OPENAI_API_BASE\") {\n            config.openai.api_base = api_base;\n        }\n\n        if let Ok(max_retries) = env::var(\"OPENAI_MAX_RETRIES\") {\n            config.openai.max_retries = max_retries\n                .parse()\n                .map_err(|e| Error::Config(format!(\"Invalid OPENAI_MAX_RETRIES: {}\", e)))?;\n        }\n\n        if let Ok(timeout) = env::var(\"OPENAI_TIMEOUT\") {\n            config.openai.timeout_seconds = timeout\n                .parse()\n                .map_err(|e| Error::Config(format!(\"Invalid OPENAI_TIMEOUT: {}\", e)))?;\n        }\n\n        Ok(config)\n    }\n\n    /// Merge environment configuration into this config\n    /// Environment values take precedence\n    fn merge_env(\u0026mut self, env_config: Config) {\n        // Only update values that were actually set in environment\n        if env::var(\"OPENAI_MODEL\").is_ok() {\n            self.openai.default_model = env_config.openai.default_model;\n        }\n        if env::var(\"OPENAI_API_BASE\").is_ok() {\n            self.openai.api_base = env_config.openai.api_base;\n        }\n        if env::var(\"OPENAI_MAX_RETRIES\").is_ok() {\n            self.openai.max_retries = env_config.openai.max_retries;\n        }\n        if env::var(\"OPENAI_TIMEOUT\").is_ok() {\n            self.openai.timeout_seconds = env_config.openai.timeout_seconds;\n        }\n    }\n\n    /// Save configuration to a TOML file\n    pub fn save\u003cP: AsRef\u003cPath\u003e\u003e(\u0026self, path: P) -\u003e Result\u003c()\u003e {\n        let content = toml::to_string_pretty(self)\n            .map_err(|e| Error::Config(format!(\"Failed to serialize config: {}\", e)))?;\n        fs::write(path, content)?;\n        Ok(())\n    }\n}","traces":[{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":16},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","config","tests.rs"],"content":"use super::*;\nuse std::env;\nuse tempfile::NamedTempFile;\nuse std::io::Write;\n\n#[test]\nfn test_config_defaults() {\n    let config = Config::default();\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 3);\n    assert_eq!(config.openai.timeout_seconds, 30);\n}\n\n#[test]\nfn test_openai_config_defaults() {\n    let config = OpenAIConfig::default();\n    assert_eq!(config.default_model, \"gpt-4\");\n    assert_eq!(config.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.max_retries, 3);\n    assert_eq!(config.timeout_seconds, 30);\n}\n\n#[test]\nfn test_config_from_toml() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-3.5-turbo\"\napi_base = \"https://custom.openai.com/v1\"\nmax_retries = 5\ntimeout_seconds = 60\n\"#;\n\n    let config: Config = toml::from_str(toml_content).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-3.5-turbo\");\n    assert_eq!(config.openai.api_base, \"https://custom.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 5);\n    assert_eq!(config.openai.timeout_seconds, 60);\n}\n\n#[test]\nfn test_config_from_file() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4-turbo\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 2\ntimeout_seconds = 45\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    let config = Config::from_file(temp_file.path()).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4-turbo\");\n    assert_eq!(config.openai.max_retries, 2);\n    assert_eq!(config.openai.timeout_seconds, 45);\n}\n\n#[test]\nfn test_config_from_file_not_found() {\n    let result = Config::from_file(\"non_existent_file.toml\");\n    assert!(result.is_err());\n    match result {\n        Err(Error::Io(_)) =\u003e {}\n        _ =\u003e panic!(\"Expected IO error\"),\n    }\n}\n\n#[test]\nfn test_config_from_env() {\n    // Set environment variables\n    env::set_var(\"OPENAI_MODEL\", \"gpt-4-vision\");\n    env::set_var(\"OPENAI_API_BASE\", \"https://custom-api.com/v1\");\n    env::set_var(\"OPENAI_MAX_RETRIES\", \"7\");\n    env::set_var(\"OPENAI_TIMEOUT\", \"90\");\n\n    let config = Config::from_env().unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4-vision\");\n    assert_eq!(config.openai.api_base, \"https://custom-api.com/v1\");\n    assert_eq!(config.openai.max_retries, 7);\n    assert_eq!(config.openai.timeout_seconds, 90);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n}\n\n#[test]\nfn test_config_from_env_partial() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Only set some environment variables\n    env::set_var(\"OPENAI_MODEL\", \"gpt-3.5-turbo-16k\");\n\n    let config = Config::from_env().unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-3.5-turbo-16k\");\n    // Should use defaults for other values\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    assert_eq!(config.openai.max_retries, 3);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n}\n\n#[test]\nfn test_config_load_priority() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Test that environment variables override file values\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 3\ntimeout_seconds = 30\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    // Set environment variable\n    env::set_var(\"OPENAI_MODEL\", \"gpt-4-turbo\");\n\n    let config = Config::load(Some(temp_file.path())).unwrap();\n    // Environment variable should override file value\n    assert_eq!(config.openai.default_model, \"gpt-4-turbo\");\n    // File value should be used for non-overridden values\n    assert_eq!(config.openai.max_retries, 3);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MODEL\");\n}\n\n#[test]\nfn test_config_load_file_only() {\n    let toml_content = r#\"\n[openai]\ndefault_model = \"gpt-4\"\napi_base = \"https://api.openai.com/v1\"\nmax_retries = 4\ntimeout_seconds = 25\n\"#;\n\n    let mut temp_file = NamedTempFile::new().unwrap();\n    write!(temp_file, \"{}\", toml_content).unwrap();\n\n    let config = Config::load(Some(temp_file.path())).unwrap();\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.max_retries, 4);\n    assert_eq!(config.openai.timeout_seconds, 25);\n}\n\n#[test]\nfn test_config_load_no_file() {\n    // Clean up any existing env vars first\n    env::remove_var(\"OPENAI_MODEL\");\n    env::remove_var(\"OPENAI_API_BASE\");\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n    env::remove_var(\"OPENAI_TIMEOUT\");\n    \n    // Load with no file specified - should use defaults + env\n    env::set_var(\"OPENAI_MAX_RETRIES\", \"10\");\n\n    let config = Config::load::\u003c\u0026str\u003e(None).unwrap();\n    // Should use default for most values\n    assert_eq!(config.openai.default_model, \"gpt-4\");\n    assert_eq!(config.openai.api_base, \"https://api.openai.com/v1\");\n    // But use env var where set\n    assert_eq!(config.openai.max_retries, 10);\n\n    // Clean up\n    env::remove_var(\"OPENAI_MAX_RETRIES\");\n}\n\n#[test]\nfn test_invalid_toml() {\n    let invalid_toml = r#\"\n[openai\ndefault_model = \"gpt-4\"\n\"#;\n\n    let result: std::result::Result\u003cConfig, toml::de::Error\u003e = toml::from_str(invalid_toml);\n    assert!(result.is_err());\n}\n\n#[test]\nfn test_config_serialization() {\n    let config = Config {\n        openai: OpenAIConfig {\n            default_model: \"gpt-4\".to_string(),\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        },\n    };\n\n    let toml_str = toml::to_string(\u0026config).unwrap();\n    assert!(toml_str.contains(\"default_model = \\\"gpt-4\\\"\"));\n    assert!(toml_str.contains(\"max_retries = 3\"));\n\n    // Round trip test\n    let parsed: Config = toml::from_str(\u0026toml_str).unwrap();\n    assert_eq!(parsed.openai.default_model, config.openai.default_model);\n    assert_eq!(parsed.openai.max_retries, config.openai.max_retries);\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","error.rs"],"content":"use std::fmt;\n\n/// Custom error type for the application\n#[derive(Debug)]\npub enum Error {\n    /// Configuration errors\n    Config(String),\n    /// Provider errors (API calls, network, etc.)\n    Provider(String),\n    /// Service container errors\n    Service(String),\n    /// IO errors\n    Io(std::io::Error),\n    /// Other errors\n    Other(String),\n}\n\nimpl fmt::Display for Error {\n    fn fmt(\u0026self, f: \u0026mut fmt::Formatter\u003c'_\u003e) -\u003e fmt::Result {\n        match self {\n            Error::Config(msg) =\u003e write!(f, \"Configuration error: {}\", msg),\n            Error::Provider(msg) =\u003e write!(f, \"Provider error: {}\", msg),\n            Error::Service(msg) =\u003e write!(f, \"Service error: {}\", msg),\n            Error::Io(err) =\u003e write!(f, \"IO error: {}\", err),\n            Error::Other(msg) =\u003e write!(f, \"Error: {}\", msg),\n        }\n    }\n}\n\nimpl std::error::Error for Error {\n    fn source(\u0026self) -\u003e Option\u003c\u0026(dyn std::error::Error + 'static)\u003e {\n        match self {\n            Error::Io(err) =\u003e Some(err),\n            _ =\u003e None,\n        }\n    }\n}\n\nimpl From\u003cstd::io::Error\u003e for Error {\n    fn from(err: std::io::Error) -\u003e Self {\n        Error::Io(err)\n    }\n}\n\nimpl From\u003ctoml::de::Error\u003e for Error {\n    fn from(err: toml::de::Error) -\u003e Self {\n        Error::Config(format!(\"TOML parsing error: {}\", err))\n    }\n}\n\nimpl From\u003cstd::env::VarError\u003e for Error {\n    fn from(err: std::env::VarError) -\u003e Self {\n        Error::Config(format!(\"Environment variable error: {}\", err))\n    }\n}\n\n/// Result type alias\npub type Result\u003cT\u003e = std::result::Result\u003cT, Error\u003e;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::error::Error as StdError;\n\n    #[test]\n    fn test_error_display() {\n        let err = Error::Config(\"Invalid API key\".to_string());\n        assert_eq!(err.to_string(), \"Configuration error: Invalid API key\");\n\n        let err = Error::Provider(\"API rate limit exceeded\".to_string());\n        assert_eq!(err.to_string(), \"Provider error: API rate limit exceeded\");\n\n        let err = Error::Service(\"Service not found\".to_string());\n        assert_eq!(err.to_string(), \"Service error: Service not found\");\n\n        let err = Error::Other(\"Unknown error\".to_string());\n        assert_eq!(err.to_string(), \"Error: Unknown error\");\n    }\n\n    #[test]\n    fn test_error_from_io() {\n        let io_err = std::io::Error::new(std::io::ErrorKind::NotFound, \"File not found\");\n        let err: Error = io_err.into();\n        assert!(matches!(err, Error::Io(_)));\n    }\n\n    #[test]\n    fn test_error_source() {\n        let io_err = std::io::Error::new(std::io::ErrorKind::PermissionDenied, \"Access denied\");\n        let err = Error::Io(io_err);\n        assert!(StdError::source(\u0026err).is_some());\n\n        let err = Error::Config(\"Bad config\".to_string());\n        assert!(StdError::source(\u0026err).is_none());\n    }\n\n    #[test]\n    fn test_error_from_env_var() {\n        let env_err = std::env::VarError::NotPresent;\n        let err: Error = env_err.into();\n        match err {\n            Error::Config(msg) =\u003e assert!(msg.contains(\"Environment variable error\")),\n            _ =\u003e panic!(\"Expected Config error\"),\n        }\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","lib.rs"],"content":"pub mod config;\npub mod error;\npub mod provider;\npub mod service;\n\nuse config::Config;\nuse error::Result;\nuse provider::{CompletionRequest, Message};\nuse service::ServiceContainer;\nuse std::sync::OnceLock;\n\nstatic SERVICE_CONTAINER: OnceLock\u003cServiceContainer\u003e = OnceLock::new();\n\n/// Initialize the global service container\npub fn init(config: Config) -\u003e Result\u003c()\u003e {\n    let container = ServiceContainer::new(config)?;\n    SERVICE_CONTAINER\n        .set(container)\n        .map_err(|_| error::Error::Service(\"Service container already initialized\".into()))?;\n    Ok(())\n}\n\n/// Get the global service container\npub fn get_service_container() -\u003e Result\u003c\u0026'static ServiceContainer\u003e {\n    SERVICE_CONTAINER\n        .get()\n        .ok_or_else(|| error::Error::Service(\"Service container not initialized\".into()))\n}\n\n/// Backward compatible ask function\npub async fn ask(prompt: \u0026str) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: container.config().openai.default_model.clone(),\n        messages: vec![Message {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }],\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n/// Ask with a specific model\npub async fn ask_with_model(prompt: \u0026str, model: \u0026str) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: model.to_string(),\n        messages: vec![Message {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }],\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n/// Ask with messages (conversation context)\npub async fn ask_with_messages(messages: Vec\u003cMessage\u003e) -\u003e Result\u003cString\u003e {\n    let container = get_service_container()?;\n    let provider = container.get_default_provider()?;\n\n    let request = CompletionRequest {\n        model: container.config().openai.default_model.clone(),\n        messages,\n        temperature: Some(0.7),\n        max_tokens: Some(1000),\n        stream: false,\n    };\n\n    let response = provider.complete(request).await?;\n    Ok(response.content)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::provider::tests::MockProvider;\n    use std::sync::Arc;\n\n    fn setup_test_container() -\u003e ServiceContainer {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n        \n        let mock_provider = Arc::new(MockProvider {\n            response: \"Test response from global\".to_string(),\n            should_fail: false,\n        });\n        \n        container.register_provider(\"mock\", mock_provider);\n        container\n    }\n\n    #[test]\n    fn test_init_and_get_container() {\n        // Reset for test\n        let config = Config::default();\n        \n        // This might fail if already initialized, but that's okay for tests\n        let _ = init(config);\n        \n        // Should be able to get the container\n        let result = get_service_container();\n        // In a real test environment, this might be initialized already\n        // so we just check it doesn't panic\n        assert!(result.is_ok() || result.is_err());\n    }\n\n    #[tokio::test]\n    async fn test_ask_backward_compatibility() {\n        // For this test, we'll test the ask function logic without global state\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_model_logic() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Test with specific model\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[tokio::test]\n    async fn test_ask_with_messages_logic() {\n        let container = setup_test_container();\n        let provider = container.get_provider(\"mock\").unwrap();\n\n        let messages = vec![\n            Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            },\n            Message {\n                role: \"assistant\".to_string(),\n                content: \"Hi there!\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"How are you?\".to_string(),\n            },\n        ];\n\n        let request = CompletionRequest {\n            model: container.config().openai.default_model.clone(),\n            messages,\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Test response from global\");\n    }\n\n    #[test]\n    fn test_service_not_initialized() {\n        // Clear any existing container (this is a limitation of using OnceLock in tests)\n        // In practice, we'd use a different pattern for testability\n        \n        // This test verifies the error when service is not initialized\n        // The actual behavior depends on whether init() was called previously\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","provider","mod.rs"],"content":"use crate::error::{Error, Result};\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse serde::{Deserialize, Serialize};\n\n#[cfg(test)]\npub mod tests;\n\n/// Message in a conversation\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Message {\n    pub role: String,\n    pub content: String,\n}\n\n/// Request for LLM completion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompletionRequest {\n    pub model: String,\n    pub messages: Vec\u003cMessage\u003e,\n    pub temperature: Option\u003cf32\u003e,\n    pub max_tokens: Option\u003cu32\u003e,\n    pub stream: bool,\n}\n\n/// Response from LLM completion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompletionResponse {\n    pub content: String,\n    pub model: String,\n    pub usage: Usage,\n}\n\n/// Token usage information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Usage {\n    pub prompt_tokens: u32,\n    pub completion_tokens: u32,\n    pub total_tokens: u32,\n}\n\n/// Streaming chunk from LLM\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct StreamChunk {\n    pub delta: String,\n    pub finish_reason: Option\u003cString\u003e,\n}\n\n/// Trait for LLM providers\n#[async_trait]\npub trait LLMProvider: Send + Sync {\n    /// Get the name of the provider\n    fn name(\u0026self) -\u003e \u0026str;\n\n    /// Complete a request and return the full response\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e;\n\n    /// Stream a response\n    async fn stream(\n        \u0026self,\n        request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e;\n}\n\npub mod openai;\n\npub use openai::OpenAIProvider;","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","provider","openai.rs"],"content":"use super::*;\nuse crate::config::OpenAIConfig;\nuse async_openai::{\n    types::{\n        ChatCompletionRequestMessage, ChatCompletionRequestSystemMessageArgs,\n        ChatCompletionRequestUserMessageArgs, ChatCompletionRequestAssistantMessageArgs,\n        CreateChatCompletionRequestArgs, CreateChatCompletionStreamResponse,\n    },\n    Client,\n};\nuse futures::StreamExt;\n\n/// OpenAI provider implementation\npub struct OpenAIProvider {\n    client: Client\u003casync_openai::config::OpenAIConfig\u003e,\n    config: OpenAIConfig,\n}\n\nimpl OpenAIProvider {\n    /// Create a new OpenAI provider\n    pub fn new(api_key: String, config: OpenAIConfig) -\u003e Self {\n        let openai_config = async_openai::config::OpenAIConfig::new()\n            .with_api_key(api_key)\n            .with_api_base(config.api_base.clone());\n\n        Self {\n            client: Client::with_config(openai_config),\n            config,\n        }\n    }\n\n    fn convert_messages(\u0026self, messages: Vec\u003cMessage\u003e) -\u003e Vec\u003cChatCompletionRequestMessage\u003e {\n        messages\n            .into_iter()\n            .map(|msg| match msg.role.as_str() {\n                \"system\" =\u003e ChatCompletionRequestSystemMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n                \"assistant\" =\u003e ChatCompletionRequestAssistantMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n                _ =\u003e ChatCompletionRequestUserMessageArgs::default()\n                    .content(msg.content)\n                    .build()\n                    .unwrap()\n                    .into(),\n            })\n            .collect()\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for OpenAIProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"openai\"\n    }\n\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e {\n        let mut builder = CreateChatCompletionRequestArgs::default();\n        builder\n            .model(\u0026request.model)\n            .messages(self.convert_messages(request.messages));\n\n        if let Some(temp) = request.temperature {\n            builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = request.max_tokens {\n            builder.max_tokens(max_tokens as u16);\n        }\n\n        let openai_request = builder\n            .build()\n            .map_err(|e| Error::Provider(format!(\"Failed to build request: {}\", e)))?;\n\n        let response = self\n            .client\n            .chat()\n            .create(openai_request)\n            .await\n            .map_err(|e| Error::Provider(format!(\"OpenAI API error: {}\", e)))?;\n\n        let content = response\n            .choices\n            .first()\n            .and_then(|c| c.message.content.as_ref())\n            .ok_or_else(|| Error::Provider(\"No content in response\".into()))?\n            .clone();\n\n        Ok(CompletionResponse {\n            content,\n            model: response.model,\n            usage: Usage {\n                prompt_tokens: response.usage.as_ref().map(|u| u.prompt_tokens).unwrap_or(0) as u32,\n                completion_tokens: response\n                    .usage\n                    .as_ref()\n                    .map(|u| u.completion_tokens)\n                    .unwrap_or(0) as u32,\n                total_tokens: response.usage.as_ref().map(|u| u.total_tokens).unwrap_or(0) as u32,\n            },\n        })\n    }\n\n    async fn stream(\n        \u0026self,\n        request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e {\n        let mut builder = CreateChatCompletionRequestArgs::default();\n        builder\n            .model(\u0026request.model)\n            .messages(self.convert_messages(request.messages))\n            .stream(true);\n\n        if let Some(temp) = request.temperature {\n            builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = request.max_tokens {\n            builder.max_tokens(max_tokens as u16);\n        }\n\n        let openai_request = builder\n            .build()\n            .map_err(|e| Error::Provider(format!(\"Failed to build request: {}\", e)))?;\n\n        let stream = self\n            .client\n            .chat()\n            .create_stream(openai_request)\n            .await\n            .map_err(|e| Error::Provider(format!(\"OpenAI API error: {}\", e)))?;\n\n        let mapped_stream = stream.map(|result| match result {\n            Ok(response) =\u003e {\n                let chunk = extract_chunk(response);\n                Ok(chunk)\n            }\n            Err(e) =\u003e Err(Error::Provider(format!(\"Stream error: {}\", e))),\n        });\n\n        Ok(Box::pin(mapped_stream))\n    }\n}\n\nfn extract_chunk(response: CreateChatCompletionStreamResponse) -\u003e StreamChunk {\n    let delta = response\n        .choices\n        .first()\n        .and_then(|c| c.delta.content.as_ref())\n        .map(|s| s.clone())\n        .unwrap_or_default();\n\n    let finish_reason = response\n        .choices\n        .first()\n        .and_then(|c| c.finish_reason.as_ref())\n        .map(|r| format!(\"{:?}\", r));\n\n    StreamChunk {\n        delta,\n        finish_reason,\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_openai_provider_creation() {\n        let config = OpenAIConfig {\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            default_model: \"gpt-4\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n\n        let provider = OpenAIProvider::new(\"test-key\".to_string(), config.clone());\n        assert_eq!(provider.name(), \"openai\");\n        assert_eq!(provider.config.default_model, \"gpt-4\");\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let config = OpenAIConfig {\n            api_base: \"https://api.openai.com/v1\".to_string(),\n            default_model: \"gpt-4\".to_string(),\n            max_retries: 3,\n            timeout_seconds: 30,\n        };\n\n        let provider = OpenAIProvider::new(\"test-key\".to_string(), config);\n\n        let messages = vec![\n            Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            },\n            Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            },\n            Message {\n                role: \"assistant\".to_string(),\n                content: \"Hi there!\".to_string(),\n            },\n        ];\n\n        let converted = provider.convert_messages(messages);\n        assert_eq!(converted.len(), 3);\n    }\n\n    #[test]\n    fn test_extract_chunk() {\n        // This would require mocking CreateChatCompletionStreamResponse\n        // which is complex due to the async-openai types\n        // For now, we'll focus on the integration tests\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","provider","tests.rs"],"content":"use super::*;\nuse async_trait::async_trait;\nuse tokio_stream::StreamExt;\n\n#[derive(Debug, Clone)]\npub struct MockProvider {\n    pub response: String,\n    pub should_fail: bool,\n}\n\n#[async_trait]\nimpl LLMProvider for MockProvider {\n    fn name(\u0026self) -\u003e \u0026str {\n        \"mock\"\n    }\n\n    async fn complete(\u0026self, request: CompletionRequest) -\u003e Result\u003cCompletionResponse\u003e {\n        if self.should_fail {\n            return Err(Error::Provider(\"Mock provider error\".into()));\n        }\n\n        Ok(CompletionResponse {\n            content: self.response.clone(),\n            model: request.model,\n            usage: Usage {\n                prompt_tokens: 10,\n                completion_tokens: 20,\n                total_tokens: 30,\n            },\n        })\n    }\n\n    async fn stream(\n        \u0026self,\n        _request: CompletionRequest,\n    ) -\u003e Result\u003cBoxStream\u003c'static, Result\u003cStreamChunk\u003e\u003e\u003e {\n        if self.should_fail {\n            return Err(Error::Provider(\"Mock provider error\".into()));\n        }\n\n        let chunks = vec![\n            StreamChunk {\n                delta: self.response.clone(),\n                finish_reason: None,\n            },\n            StreamChunk {\n                delta: String::new(),\n                finish_reason: Some(\"stop\".to_string()),\n            },\n        ];\n\n        Ok(Box::pin(tokio_stream::iter(chunks.into_iter().map(Ok))))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_mock_provider_complete() {\n        let provider = MockProvider {\n            response: \"Test response\".to_string(),\n            should_fail: false,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = provider.complete(request.clone()).await.unwrap();\n        assert_eq!(response.content, \"Test response\");\n        assert_eq!(response.model, \"gpt-4\");\n        assert_eq!(response.usage.total_tokens, 30);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_complete_error() {\n        let provider = MockProvider {\n            response: String::new(),\n            should_fail: true,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            temperature: None,\n            max_tokens: None,\n            stream: false,\n        };\n\n        let result = provider.complete(request).await;\n        assert!(result.is_err());\n        match result {\n            Err(Error::Provider(msg)) =\u003e assert_eq!(msg, \"Mock provider error\"),\n            _ =\u003e panic!(\"Expected Provider error\"),\n        }\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_stream() {\n        let provider = MockProvider {\n            response: \"Streaming response\".to_string(),\n            should_fail: false,\n        };\n\n        let request = CompletionRequest {\n            model: \"gpt-4\".to_string(),\n            messages: vec![Message {\n                role: \"system\".to_string(),\n                content: \"You are a helpful assistant\".to_string(),\n            }],\n            temperature: Some(0.5),\n            max_tokens: Some(200),\n            stream: true,\n        };\n\n        let mut stream = provider.stream(request).await.unwrap();\n        \n        let mut chunks = Vec::new();\n        while let Some(chunk) = stream.next().await {\n            chunks.push(chunk.unwrap());\n        }\n\n        assert_eq!(chunks.len(), 2);\n        assert_eq!(chunks[0].delta, \"Streaming response\");\n        assert_eq!(chunks[0].finish_reason, None);\n        assert_eq!(chunks[1].delta, \"\");\n        assert_eq!(chunks[1].finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[tokio::test]\n    async fn test_provider_trait_methods() {\n        let provider = MockProvider {\n            response: \"Test\".to_string(),\n            should_fail: false,\n        };\n\n        assert_eq!(provider.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_message_construction() {\n        let msg = Message {\n            role: \"assistant\".to_string(),\n            content: \"I can help with that\".to_string(),\n        };\n\n        assert_eq!(msg.role, \"assistant\");\n        assert_eq!(msg.content, \"I can help with that\");\n    }\n\n    #[test]\n    fn test_completion_request_builder() {\n        let request = CompletionRequest {\n            model: \"gpt-3.5-turbo\".to_string(),\n            messages: vec![\n                Message {\n                    role: \"system\".to_string(),\n                    content: \"You are a coding assistant\".to_string(),\n                },\n                Message {\n                    role: \"user\".to_string(),\n                    content: \"Write a hello world program\".to_string(),\n                },\n            ],\n            temperature: Some(0.8),\n            max_tokens: Some(1000),\n            stream: true,\n        };\n\n        assert_eq!(request.model, \"gpt-3.5-turbo\");\n        assert_eq!(request.messages.len(), 2);\n        assert_eq!(request.temperature, Some(0.8));\n        assert_eq!(request.max_tokens, Some(1000));\n        assert!(request.stream);\n    }\n\n    #[test]\n    fn test_usage_calculation() {\n        let usage = Usage {\n            prompt_tokens: 50,\n            completion_tokens: 100,\n            total_tokens: 150,\n        };\n\n        assert_eq!(usage.prompt_tokens, 50);\n        assert_eq!(usage.completion_tokens, 100);\n        assert_eq!(usage.total_tokens, 150);\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","crates","core","src","service.rs"],"content":"use crate::config::Config;\nuse crate::error::{Error, Result};\nuse crate::provider::{LLMProvider, OpenAIProvider};\nuse std::collections::HashMap;\nuse std::sync::Arc;\n\n/// Service container for dependency injection\npub struct ServiceContainer {\n    providers: HashMap\u003cString, Arc\u003cdyn LLMProvider\u003e\u003e,\n    config: Config,\n}\n\nimpl ServiceContainer {\n    /// Create a new service container\n    pub fn new(config: Config) -\u003e Result\u003cSelf\u003e {\n        let mut container = Self {\n            providers: HashMap::new(),\n            config,\n        };\n\n        // Register default providers\n        container.register_default_providers()?;\n\n        Ok(container)\n    }\n\n    /// Register default providers based on configuration\n    fn register_default_providers(\u0026mut self) -\u003e Result\u003c()\u003e {\n        // Register OpenAI provider if API key is available\n        if let Ok(api_key) = std::env::var(\"OPENAI_API_KEY\") {\n            let provider = OpenAIProvider::new(api_key, self.config.openai.clone());\n            self.register_provider(\"openai\", Arc::new(provider));\n        }\n\n        Ok(())\n    }\n\n    /// Register a provider with the container\n    pub fn register_provider(\u0026mut self, name: \u0026str, provider: Arc\u003cdyn LLMProvider\u003e) {\n        self.providers.insert(name.to_string(), provider);\n    }\n\n    /// Get a provider by name\n    pub fn get_provider(\u0026self, name: \u0026str) -\u003e Result\u003cArc\u003cdyn LLMProvider\u003e\u003e {\n        self.providers\n            .get(name)\n            .cloned()\n            .ok_or_else(|| Error::Service(format!(\"Provider '{}' not found\", name)))\n    }\n\n    /// Get the default provider (first available)\n    pub fn get_default_provider(\u0026self) -\u003e Result\u003cArc\u003cdyn LLMProvider\u003e\u003e {\n        // Try OpenAI first as the default\n        if let Ok(provider) = self.get_provider(\"openai\") {\n            return Ok(provider);\n        }\n\n        // If no specific provider, return the first available\n        self.providers\n            .values()\n            .next()\n            .cloned()\n            .ok_or_else(|| Error::Service(\"No providers available\".into()))\n    }\n\n    /// List all registered provider names\n    pub fn list_providers(\u0026self) -\u003e Vec\u003cString\u003e {\n        self.providers.keys().cloned().collect()\n    }\n\n    /// Get the configuration\n    pub fn config(\u0026self) -\u003e \u0026Config {\n        \u0026self.config\n    }\n\n    /// Update the configuration and re-register providers\n    pub fn update_config(\u0026mut self, config: Config) -\u003e Result\u003c()\u003e {\n        self.config = config;\n        self.providers.clear();\n        self.register_default_providers()?;\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::provider::tests::MockProvider;\n\n    #[test]\n    fn test_service_container_creation() {\n        let config = Config::default();\n        let container = ServiceContainer::new(config).unwrap();\n        \n        // Should create without error\n        assert!(container.providers.is_empty() || !container.providers.is_empty());\n    }\n\n    #[test]\n    fn test_register_and_get_provider() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Test response\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"mock\", mock_provider.clone());\n\n        let retrieved = container.get_provider(\"mock\").unwrap();\n        assert_eq!(retrieved.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_get_provider_not_found() {\n        let config = Config::default();\n        let container = ServiceContainer::new(config).unwrap();\n\n        let result = container.get_provider(\"nonexistent\");\n        assert!(result.is_err());\n        match result {\n            Err(Error::Service(msg)) =\u003e assert!(msg.contains(\"not found\")),\n            _ =\u003e panic!(\"Expected Service error\"),\n        }\n    }\n\n    #[test]\n    fn test_list_providers() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Clear any existing providers first\n        container.providers.clear();\n\n        let mock1 = Arc::new(MockProvider {\n            response: \"Test1\".to_string(),\n            should_fail: false,\n        });\n        let mock2 = Arc::new(MockProvider {\n            response: \"Test2\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"mock1\", mock1);\n        container.register_provider(\"mock2\", mock2);\n\n        let providers = container.list_providers();\n        assert_eq!(providers.len(), 2);\n        assert!(providers.contains(\u0026\"mock1\".to_string()));\n        assert!(providers.contains(\u0026\"mock2\".to_string()));\n    }\n\n    #[test]\n    fn test_get_default_provider() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        // Clear any existing providers first\n        container.providers.clear();\n\n        // With no providers registered, should fail\n        let result = container.get_default_provider();\n        assert!(result.is_err());\n\n        // Register a provider\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Default\".to_string(),\n            should_fail: false,\n        });\n        container.register_provider(\"default\", mock_provider);\n\n        let default = container.get_default_provider().unwrap();\n        assert_eq!(default.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_config_access() {\n        let config = Config::default();\n        let original_model = config.openai.default_model.clone();\n        \n        let container = ServiceContainer::new(config).unwrap();\n        assert_eq!(container.config().openai.default_model, original_model);\n    }\n\n    #[test]\n    fn test_update_config() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mut new_config = Config::default();\n        new_config.openai.default_model = \"gpt-3.5-turbo\".to_string();\n\n        container.update_config(new_config).unwrap();\n        assert_eq!(container.config().openai.default_model, \"gpt-3.5-turbo\");\n    }\n\n    #[tokio::test]\n    async fn test_provider_functionality() {\n        let config = Config::default();\n        let mut container = ServiceContainer::new(config).unwrap();\n\n        let mock_provider = Arc::new(MockProvider {\n            response: \"Hello from service container\".to_string(),\n            should_fail: false,\n        });\n\n        container.register_provider(\"test\", mock_provider);\n\n        let provider = container.get_provider(\"test\").unwrap();\n        \n        let request = crate::provider::CompletionRequest {\n            model: \"test-model\".to_string(),\n            messages: vec![crate::provider::Message {\n                role: \"user\".to_string(),\n                content: \"Test message\".to_string(),\n            }],\n            temperature: Some(0.7),\n            max_tokens: Some(100),\n            stream: false,\n        };\n\n        let response = provider.complete(request).await.unwrap();\n        assert_eq!(response.content, \"Hello from service container\");\n    }\n}","traces":[],"covered":0,"coverable":0},{"path":["/","Users","cortex","CascadeProjects","opencode-rs","slice-1.5","target","debug","build","mime_guess-9cec288f4dc4b10d","out","mime_types_generated.rs"],"content":"","traces":[],"covered":0,"coverable":0}]};
    </script>
    <script crossorigin>/** @license React v16.13.1
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
'use strict';(function(d,r){"object"===typeof exports&&"undefined"!==typeof module?r(exports):"function"===typeof define&&define.amd?define(["exports"],r):(d=d||self,r(d.React={}))})(this,function(d){function r(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function w(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function da(){}function L(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function ea(a,b,c){var g,e={},fa=null,d=null;if(null!=b)for(g in void 0!==b.ref&&(d=b.ref),void 0!==b.key&&(fa=""+b.key),b)ha.call(b,g)&&!ia.hasOwnProperty(g)&&(e[g]=b[g]);var h=arguments.length-2;if(1===h)e.children=c;else if(1<h){for(var k=Array(h),f=0;f<h;f++)k[f]=arguments[f+2];e.children=k}if(a&&a.defaultProps)for(g in h=a.defaultProps,
h)void 0===e[g]&&(e[g]=h[g]);return{$$typeof:x,type:a,key:fa,ref:d,props:e,_owner:M.current}}function va(a,b){return{$$typeof:x,type:a.type,key:b,ref:a.ref,props:a.props,_owner:a._owner}}function N(a){return"object"===typeof a&&null!==a&&a.$$typeof===x}function wa(a){var b={"=":"=0",":":"=2"};return"$"+(""+a).replace(/[=:]/g,function(a){return b[a]})}function ja(a,b,c,g){if(C.length){var e=C.pop();e.result=a;e.keyPrefix=b;e.func=c;e.context=g;e.count=0;return e}return{result:a,keyPrefix:b,func:c,
context:g,count:0}}function ka(a){a.result=null;a.keyPrefix=null;a.func=null;a.context=null;a.count=0;10>C.length&&C.push(a)}function O(a,b,c,g){var e=typeof a;if("undefined"===e||"boolean"===e)a=null;var d=!1;if(null===a)d=!0;else switch(e){case "string":case "number":d=!0;break;case "object":switch(a.$$typeof){case x:case xa:d=!0}}if(d)return c(g,a,""===b?"."+P(a,0):b),1;d=0;b=""===b?".":b+":";if(Array.isArray(a))for(var f=0;f<a.length;f++){e=a[f];var h=b+P(e,f);d+=O(e,h,c,g)}else if(null===a||
"object"!==typeof a?h=null:(h=la&&a[la]||a["@@iterator"],h="function"===typeof h?h:null),"function"===typeof h)for(a=h.call(a),f=0;!(e=a.next()).done;)e=e.value,h=b+P(e,f++),d+=O(e,h,c,g);else if("object"===e)throw c=""+a,Error(r(31,"[object Object]"===c?"object with keys {"+Object.keys(a).join(", ")+"}":c,""));return d}function Q(a,b,c){return null==a?0:O(a,"",b,c)}function P(a,b){return"object"===typeof a&&null!==a&&null!=a.key?wa(a.key):b.toString(36)}function ya(a,b,c){a.func.call(a.context,b,
a.count++)}function za(a,b,c){var g=a.result,e=a.keyPrefix;a=a.func.call(a.context,b,a.count++);Array.isArray(a)?R(a,g,c,function(a){return a}):null!=a&&(N(a)&&(a=va(a,e+(!a.key||b&&b.key===a.key?"":(""+a.key).replace(ma,"$&/")+"/")+c)),g.push(a))}function R(a,b,c,g,e){var d="";null!=c&&(d=(""+c).replace(ma,"$&/")+"/");b=ja(b,d,g,e);Q(a,za,b);ka(b)}function t(){var a=na.current;if(null===a)throw Error(r(321));return a}function S(a,b){var c=a.length;a.push(b);a:for(;;){var g=c-1>>>1,e=a[g];if(void 0!==
e&&0<D(e,b))a[g]=b,a[c]=e,c=g;else break a}}function n(a){a=a[0];return void 0===a?null:a}function E(a){var b=a[0];if(void 0!==b){var c=a.pop();if(c!==b){a[0]=c;a:for(var g=0,e=a.length;g<e;){var d=2*(g+1)-1,f=a[d],h=d+1,k=a[h];if(void 0!==f&&0>D(f,c))void 0!==k&&0>D(k,f)?(a[g]=k,a[h]=c,g=h):(a[g]=f,a[d]=c,g=d);else if(void 0!==k&&0>D(k,c))a[g]=k,a[h]=c,g=h;else break a}}return b}return null}function D(a,b){var c=a.sortIndex-b.sortIndex;return 0!==c?c:a.id-b.id}function F(a){for(var b=n(u);null!==
b;){if(null===b.callback)E(u);else if(b.startTime<=a)E(u),b.sortIndex=b.expirationTime,S(p,b);else break;b=n(u)}}function T(a){y=!1;F(a);if(!v)if(null!==n(p))v=!0,z(U);else{var b=n(u);null!==b&&G(T,b.startTime-a)}}function U(a,b){v=!1;y&&(y=!1,V());H=!0;var c=m;try{F(b);for(l=n(p);null!==l&&(!(l.expirationTime>b)||a&&!W());){var g=l.callback;if(null!==g){l.callback=null;m=l.priorityLevel;var e=g(l.expirationTime<=b);b=q();"function"===typeof e?l.callback=e:l===n(p)&&E(p);F(b)}else E(p);l=n(p)}if(null!==
l)var d=!0;else{var f=n(u);null!==f&&G(T,f.startTime-b);d=!1}return d}finally{l=null,m=c,H=!1}}function oa(a){switch(a){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1E4;default:return 5E3}}var f="function"===typeof Symbol&&Symbol.for,x=f?Symbol.for("react.element"):60103,xa=f?Symbol.for("react.portal"):60106,Aa=f?Symbol.for("react.fragment"):60107,Ba=f?Symbol.for("react.strict_mode"):60108,Ca=f?Symbol.for("react.profiler"):60114,Da=f?Symbol.for("react.provider"):60109,
Ea=f?Symbol.for("react.context"):60110,Fa=f?Symbol.for("react.forward_ref"):60112,Ga=f?Symbol.for("react.suspense"):60113,Ha=f?Symbol.for("react.memo"):60115,Ia=f?Symbol.for("react.lazy"):60116,la="function"===typeof Symbol&&Symbol.iterator,pa=Object.getOwnPropertySymbols,Ja=Object.prototype.hasOwnProperty,Ka=Object.prototype.propertyIsEnumerable,I=function(){try{if(!Object.assign)return!1;var a=new String("abc");a[5]="de";if("5"===Object.getOwnPropertyNames(a)[0])return!1;var b={};for(a=0;10>a;a++)b["_"+
String.fromCharCode(a)]=a;if("0123456789"!==Object.getOwnPropertyNames(b).map(function(a){return b[a]}).join(""))return!1;var c={};"abcdefghijklmnopqrst".split("").forEach(function(a){c[a]=a});return"abcdefghijklmnopqrst"!==Object.keys(Object.assign({},c)).join("")?!1:!0}catch(g){return!1}}()?Object.assign:function(a,b){if(null===a||void 0===a)throw new TypeError("Object.assign cannot be called with null or undefined");var c=Object(a);for(var g,e=1;e<arguments.length;e++){var d=Object(arguments[e]);
for(var f in d)Ja.call(d,f)&&(c[f]=d[f]);if(pa){g=pa(d);for(var h=0;h<g.length;h++)Ka.call(d,g[h])&&(c[g[h]]=d[g[h]])}}return c},ca={isMounted:function(a){return!1},enqueueForceUpdate:function(a,b,c){},enqueueReplaceState:function(a,b,c,d){},enqueueSetState:function(a,b,c,d){}},ba={};w.prototype.isReactComponent={};w.prototype.setState=function(a,b){if("object"!==typeof a&&"function"!==typeof a&&null!=a)throw Error(r(85));this.updater.enqueueSetState(this,a,b,"setState")};w.prototype.forceUpdate=
function(a){this.updater.enqueueForceUpdate(this,a,"forceUpdate")};da.prototype=w.prototype;f=L.prototype=new da;f.constructor=L;I(f,w.prototype);f.isPureReactComponent=!0;var M={current:null},ha=Object.prototype.hasOwnProperty,ia={key:!0,ref:!0,__self:!0,__source:!0},ma=/\/+/g,C=[],na={current:null},X;if("undefined"===typeof window||"function"!==typeof MessageChannel){var A=null,qa=null,ra=function(){if(null!==A)try{var a=q();A(!0,a);A=null}catch(b){throw setTimeout(ra,0),b;}},La=Date.now();var q=
function(){return Date.now()-La};var z=function(a){null!==A?setTimeout(z,0,a):(A=a,setTimeout(ra,0))};var G=function(a,b){qa=setTimeout(a,b)};var V=function(){clearTimeout(qa)};var W=function(){return!1};f=X=function(){}}else{var Y=window.performance,sa=window.Date,Ma=window.setTimeout,Na=window.clearTimeout;"undefined"!==typeof console&&(f=window.cancelAnimationFrame,"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),
"function"!==typeof f&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"));if("object"===typeof Y&&"function"===typeof Y.now)q=function(){return Y.now()};else{var Oa=sa.now();q=function(){return sa.now()-Oa}}var J=!1,K=null,Z=-1,ta=5,ua=0;W=function(){return q()>=ua};f=function(){};X=function(a){0>a||125<a?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):
ta=0<a?Math.floor(1E3/a):5};var B=new MessageChannel,aa=B.port2;B.port1.onmessage=function(){if(null!==K){var a=q();ua=a+ta;try{K(!0,a)?aa.postMessage(null):(J=!1,K=null)}catch(b){throw aa.postMessage(null),b;}}else J=!1};z=function(a){K=a;J||(J=!0,aa.postMessage(null))};G=function(a,b){Z=Ma(function(){a(q())},b)};V=function(){Na(Z);Z=-1}}var p=[],u=[],Pa=1,l=null,m=3,H=!1,v=!1,y=!1,Qa=0;B={ReactCurrentDispatcher:na,ReactCurrentOwner:M,IsSomeRendererActing:{current:!1},assign:I};I(B,{Scheduler:{__proto__:null,
unstable_ImmediatePriority:1,unstable_UserBlockingPriority:2,unstable_NormalPriority:3,unstable_IdlePriority:5,unstable_LowPriority:4,unstable_runWithPriority:function(a,b){switch(a){case 1:case 2:case 3:case 4:case 5:break;default:a=3}var c=m;m=a;try{return b()}finally{m=c}},unstable_next:function(a){switch(m){case 1:case 2:case 3:var b=3;break;default:b=m}var c=m;m=b;try{return a()}finally{m=c}},unstable_scheduleCallback:function(a,b,c){var d=q();if("object"===typeof c&&null!==c){var e=c.delay;
e="number"===typeof e&&0<e?d+e:d;c="number"===typeof c.timeout?c.timeout:oa(a)}else c=oa(a),e=d;c=e+c;a={id:Pa++,callback:b,priorityLevel:a,startTime:e,expirationTime:c,sortIndex:-1};e>d?(a.sortIndex=e,S(u,a),null===n(p)&&a===n(u)&&(y?V():y=!0,G(T,e-d))):(a.sortIndex=c,S(p,a),v||H||(v=!0,z(U)));return a},unstable_cancelCallback:function(a){a.callback=null},unstable_wrapCallback:function(a){var b=m;return function(){var c=m;m=b;try{return a.apply(this,arguments)}finally{m=c}}},unstable_getCurrentPriorityLevel:function(){return m},
unstable_shouldYield:function(){var a=q();F(a);var b=n(p);return b!==l&&null!==l&&null!==b&&null!==b.callback&&b.startTime<=a&&b.expirationTime<l.expirationTime||W()},unstable_requestPaint:f,unstable_continueExecution:function(){v||H||(v=!0,z(U))},unstable_pauseExecution:function(){},unstable_getFirstCallbackNode:function(){return n(p)},get unstable_now(){return q},get unstable_forceFrameRate(){return X},unstable_Profiling:null},SchedulerTracing:{__proto__:null,__interactionsRef:null,__subscriberRef:null,
unstable_clear:function(a){return a()},unstable_getCurrent:function(){return null},unstable_getThreadID:function(){return++Qa},unstable_trace:function(a,b,c){return c()},unstable_wrap:function(a){return a},unstable_subscribe:function(a){},unstable_unsubscribe:function(a){}}});d.Children={map:function(a,b,c){if(null==a)return a;var d=[];R(a,d,null,b,c);return d},forEach:function(a,b,c){if(null==a)return a;b=ja(null,null,b,c);Q(a,ya,b);ka(b)},count:function(a){return Q(a,function(){return null},null)},
toArray:function(a){var b=[];R(a,b,null,function(a){return a});return b},only:function(a){if(!N(a))throw Error(r(143));return a}};d.Component=w;d.Fragment=Aa;d.Profiler=Ca;d.PureComponent=L;d.StrictMode=Ba;d.Suspense=Ga;d.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=B;d.cloneElement=function(a,b,c){if(null===a||void 0===a)throw Error(r(267,a));var d=I({},a.props),e=a.key,f=a.ref,m=a._owner;if(null!=b){void 0!==b.ref&&(f=b.ref,m=M.current);void 0!==b.key&&(e=""+b.key);if(a.type&&a.type.defaultProps)var h=
a.type.defaultProps;for(k in b)ha.call(b,k)&&!ia.hasOwnProperty(k)&&(d[k]=void 0===b[k]&&void 0!==h?h[k]:b[k])}var k=arguments.length-2;if(1===k)d.children=c;else if(1<k){h=Array(k);for(var l=0;l<k;l++)h[l]=arguments[l+2];d.children=h}return{$$typeof:x,type:a.type,key:e,ref:f,props:d,_owner:m}};d.createContext=function(a,b){void 0===b&&(b=null);a={$$typeof:Ea,_calculateChangedBits:b,_currentValue:a,_currentValue2:a,_threadCount:0,Provider:null,Consumer:null};a.Provider={$$typeof:Da,_context:a};return a.Consumer=
a};d.createElement=ea;d.createFactory=function(a){var b=ea.bind(null,a);b.type=a;return b};d.createRef=function(){return{current:null}};d.forwardRef=function(a){return{$$typeof:Fa,render:a}};d.isValidElement=N;d.lazy=function(a){return{$$typeof:Ia,_ctor:a,_status:-1,_result:null}};d.memo=function(a,b){return{$$typeof:Ha,type:a,compare:void 0===b?null:b}};d.useCallback=function(a,b){return t().useCallback(a,b)};d.useContext=function(a,b){return t().useContext(a,b)};d.useDebugValue=function(a,b){};
d.useEffect=function(a,b){return t().useEffect(a,b)};d.useImperativeHandle=function(a,b,c){return t().useImperativeHandle(a,b,c)};d.useLayoutEffect=function(a,b){return t().useLayoutEffect(a,b)};d.useMemo=function(a,b){return t().useMemo(a,b)};d.useReducer=function(a,b,c){return t().useReducer(a,b,c)};d.useRef=function(a){return t().useRef(a)};d.useState=function(a){return t().useState(a)};d.version="16.13.1"});
</script>
    <script crossorigin>/** @license React v16.13.1
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
/*
 Modernizr 3.0.0pre (Custom Build) | MIT
*/
'use strict';(function(I,ea){"object"===typeof exports&&"undefined"!==typeof module?ea(exports,require("react")):"function"===typeof define&&define.amd?define(["exports","react"],ea):(I=I||self,ea(I.ReactDOM={},I.React))})(this,function(I,ea){function k(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function ji(a,b,c,d,e,f,g,h,m){yb=!1;gc=null;ki.apply(li,arguments)}function mi(a,b,c,d,e,f,g,h,m){ji.apply(this,arguments);if(yb){if(yb){var n=gc;yb=!1;gc=null}else throw Error(k(198));hc||(hc=!0,pd=n)}}function lf(a,b,c){var d=a.type||"unknown-event";a.currentTarget=mf(c);mi(d,b,void 0,a);a.currentTarget=null}function nf(){if(ic)for(var a in cb){var b=cb[a],c=ic.indexOf(a);if(!(-1<c))throw Error(k(96,a));if(!jc[c]){if(!b.extractEvents)throw Error(k(97,a));jc[c]=b;c=b.eventTypes;for(var d in c){var e=
void 0;var f=c[d],g=b,h=d;if(qd.hasOwnProperty(h))throw Error(k(99,h));qd[h]=f;var m=f.phasedRegistrationNames;if(m){for(e in m)m.hasOwnProperty(e)&&of(m[e],g,h);e=!0}else f.registrationName?(of(f.registrationName,g,h),e=!0):e=!1;if(!e)throw Error(k(98,d,a));}}}}function of(a,b,c){if(db[a])throw Error(k(100,a));db[a]=b;rd[a]=b.eventTypes[c].dependencies}function pf(a){var b=!1,c;for(c in a)if(a.hasOwnProperty(c)){var d=a[c];if(!cb.hasOwnProperty(c)||cb[c]!==d){if(cb[c])throw Error(k(102,c));cb[c]=
d;b=!0}}b&&nf()}function qf(a){if(a=rf(a)){if("function"!==typeof sd)throw Error(k(280));var b=a.stateNode;b&&(b=td(b),sd(a.stateNode,a.type,b))}}function sf(a){eb?fb?fb.push(a):fb=[a]:eb=a}function tf(){if(eb){var a=eb,b=fb;fb=eb=null;qf(a);if(b)for(a=0;a<b.length;a++)qf(b[a])}}function ud(){if(null!==eb||null!==fb)vd(),tf()}function uf(a,b,c){if(wd)return a(b,c);wd=!0;try{return vf(a,b,c)}finally{wd=!1,ud()}}function ni(a){if(wf.call(xf,a))return!0;if(wf.call(yf,a))return!1;if(oi.test(a))return xf[a]=
!0;yf[a]=!0;return!1}function pi(a,b,c,d){if(null!==c&&0===c.type)return!1;switch(typeof b){case "function":case "symbol":return!0;case "boolean":if(d)return!1;if(null!==c)return!c.acceptsBooleans;a=a.toLowerCase().slice(0,5);return"data-"!==a&&"aria-"!==a;default:return!1}}function qi(a,b,c,d){if(null===b||"undefined"===typeof b||pi(a,b,c,d))return!0;if(d)return!1;if(null!==c)switch(c.type){case 3:return!b;case 4:return!1===b;case 5:return isNaN(b);case 6:return isNaN(b)||1>b}return!1}function L(a,
b,c,d,e,f){this.acceptsBooleans=2===b||3===b||4===b;this.attributeName=d;this.attributeNamespace=e;this.mustUseProperty=c;this.propertyName=a;this.type=b;this.sanitizeURL=f}function xd(a,b,c,d){var e=E.hasOwnProperty(b)?E[b]:null;var f=null!==e?0===e.type:d?!1:!(2<b.length)||"o"!==b[0]&&"O"!==b[0]||"n"!==b[1]&&"N"!==b[1]?!1:!0;f||(qi(b,c,e,d)&&(c=null),d||null===e?ni(b)&&(null===c?a.removeAttribute(b):a.setAttribute(b,""+c)):e.mustUseProperty?a[e.propertyName]=null===c?3===e.type?!1:"":c:(b=e.attributeName,
d=e.attributeNamespace,null===c?a.removeAttribute(b):(e=e.type,c=3===e||4===e&&!0===c?"":""+c,d?a.setAttributeNS(d,b,c):a.setAttribute(b,c))))}function zb(a){if(null===a||"object"!==typeof a)return null;a=zf&&a[zf]||a["@@iterator"];return"function"===typeof a?a:null}function ri(a){if(-1===a._status){a._status=0;var b=a._ctor;b=b();a._result=b;b.then(function(b){0===a._status&&(b=b.default,a._status=1,a._result=b)},function(b){0===a._status&&(a._status=2,a._result=b)})}}function na(a){if(null==a)return null;
if("function"===typeof a)return a.displayName||a.name||null;if("string"===typeof a)return a;switch(a){case Ma:return"Fragment";case gb:return"Portal";case kc:return"Profiler";case Af:return"StrictMode";case lc:return"Suspense";case yd:return"SuspenseList"}if("object"===typeof a)switch(a.$$typeof){case Bf:return"Context.Consumer";case Cf:return"Context.Provider";case zd:var b=a.render;b=b.displayName||b.name||"";return a.displayName||(""!==b?"ForwardRef("+b+")":"ForwardRef");case Ad:return na(a.type);
case Df:return na(a.render);case Ef:if(a=1===a._status?a._result:null)return na(a)}return null}function Bd(a){var b="";do{a:switch(a.tag){case 3:case 4:case 6:case 7:case 10:case 9:var c="";break a;default:var d=a._debugOwner,e=a._debugSource,f=na(a.type);c=null;d&&(c=na(d.type));d=f;f="";e?f=" (at "+e.fileName.replace(si,"")+":"+e.lineNumber+")":c&&(f=" (created by "+c+")");c="\n    in "+(d||"Unknown")+f}b+=c;a=a.return}while(a);return b}function va(a){switch(typeof a){case "boolean":case "number":case "object":case "string":case "undefined":return a;
default:return""}}function Ff(a){var b=a.type;return(a=a.nodeName)&&"input"===a.toLowerCase()&&("checkbox"===b||"radio"===b)}function ti(a){var b=Ff(a)?"checked":"value",c=Object.getOwnPropertyDescriptor(a.constructor.prototype,b),d=""+a[b];if(!a.hasOwnProperty(b)&&"undefined"!==typeof c&&"function"===typeof c.get&&"function"===typeof c.set){var e=c.get,f=c.set;Object.defineProperty(a,b,{configurable:!0,get:function(){return e.call(this)},set:function(a){d=""+a;f.call(this,a)}});Object.defineProperty(a,
b,{enumerable:c.enumerable});return{getValue:function(){return d},setValue:function(a){d=""+a},stopTracking:function(){a._valueTracker=null;delete a[b]}}}}function mc(a){a._valueTracker||(a._valueTracker=ti(a))}function Gf(a){if(!a)return!1;var b=a._valueTracker;if(!b)return!0;var c=b.getValue();var d="";a&&(d=Ff(a)?a.checked?"true":"false":a.value);a=d;return a!==c?(b.setValue(a),!0):!1}function Cd(a,b){var c=b.checked;return M({},b,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=
c?c:a._wrapperState.initialChecked})}function Hf(a,b){var c=null==b.defaultValue?"":b.defaultValue,d=null!=b.checked?b.checked:b.defaultChecked;c=va(null!=b.value?b.value:c);a._wrapperState={initialChecked:d,initialValue:c,controlled:"checkbox"===b.type||"radio"===b.type?null!=b.checked:null!=b.value}}function If(a,b){b=b.checked;null!=b&&xd(a,"checked",b,!1)}function Dd(a,b){If(a,b);var c=va(b.value),d=b.type;if(null!=c)if("number"===d){if(0===c&&""===a.value||a.value!=c)a.value=""+c}else a.value!==
""+c&&(a.value=""+c);else if("submit"===d||"reset"===d){a.removeAttribute("value");return}b.hasOwnProperty("value")?Ed(a,b.type,c):b.hasOwnProperty("defaultValue")&&Ed(a,b.type,va(b.defaultValue));null==b.checked&&null!=b.defaultChecked&&(a.defaultChecked=!!b.defaultChecked)}function Jf(a,b,c){if(b.hasOwnProperty("value")||b.hasOwnProperty("defaultValue")){var d=b.type;if(!("submit"!==d&&"reset"!==d||void 0!==b.value&&null!==b.value))return;b=""+a._wrapperState.initialValue;c||b===a.value||(a.value=
b);a.defaultValue=b}c=a.name;""!==c&&(a.name="");a.defaultChecked=!!a._wrapperState.initialChecked;""!==c&&(a.name=c)}function Ed(a,b,c){if("number"!==b||a.ownerDocument.activeElement!==a)null==c?a.defaultValue=""+a._wrapperState.initialValue:a.defaultValue!==""+c&&(a.defaultValue=""+c)}function ui(a){var b="";ea.Children.forEach(a,function(a){null!=a&&(b+=a)});return b}function Fd(a,b){a=M({children:void 0},b);if(b=ui(b.children))a.children=b;return a}function hb(a,b,c,d){a=a.options;if(b){b={};
for(var e=0;e<c.length;e++)b["$"+c[e]]=!0;for(c=0;c<a.length;c++)e=b.hasOwnProperty("$"+a[c].value),a[c].selected!==e&&(a[c].selected=e),e&&d&&(a[c].defaultSelected=!0)}else{c=""+va(c);b=null;for(e=0;e<a.length;e++){if(a[e].value===c){a[e].selected=!0;d&&(a[e].defaultSelected=!0);return}null!==b||a[e].disabled||(b=a[e])}null!==b&&(b.selected=!0)}}function Gd(a,b){if(null!=b.dangerouslySetInnerHTML)throw Error(k(91));return M({},b,{value:void 0,defaultValue:void 0,children:""+a._wrapperState.initialValue})}
function Kf(a,b){var c=b.value;if(null==c){c=b.children;b=b.defaultValue;if(null!=c){if(null!=b)throw Error(k(92));if(Array.isArray(c)){if(!(1>=c.length))throw Error(k(93));c=c[0]}b=c}null==b&&(b="");c=b}a._wrapperState={initialValue:va(c)}}function Lf(a,b){var c=va(b.value),d=va(b.defaultValue);null!=c&&(c=""+c,c!==a.value&&(a.value=c),null==b.defaultValue&&a.defaultValue!==c&&(a.defaultValue=c));null!=d&&(a.defaultValue=""+d)}function Mf(a,b){b=a.textContent;b===a._wrapperState.initialValue&&""!==
b&&null!==b&&(a.value=b)}function Nf(a){switch(a){case "svg":return"http://www.w3.org/2000/svg";case "math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function Hd(a,b){return null==a||"http://www.w3.org/1999/xhtml"===a?Nf(b):"http://www.w3.org/2000/svg"===a&&"foreignObject"===b?"http://www.w3.org/1999/xhtml":a}function nc(a,b){var c={};c[a.toLowerCase()]=b.toLowerCase();c["Webkit"+a]="webkit"+b;c["Moz"+a]="moz"+b;return c}function oc(a){if(Id[a])return Id[a];
if(!ib[a])return a;var b=ib[a],c;for(c in b)if(b.hasOwnProperty(c)&&c in Of)return Id[a]=b[c];return a}function Jd(a){var b=Pf.get(a);void 0===b&&(b=new Map,Pf.set(a,b));return b}function Na(a){var b=a,c=a;if(a.alternate)for(;b.return;)b=b.return;else{a=b;do b=a,0!==(b.effectTag&1026)&&(c=b.return),a=b.return;while(a)}return 3===b.tag?c:null}function Qf(a){if(13===a.tag){var b=a.memoizedState;null===b&&(a=a.alternate,null!==a&&(b=a.memoizedState));if(null!==b)return b.dehydrated}return null}function Rf(a){if(Na(a)!==
a)throw Error(k(188));}function vi(a){var b=a.alternate;if(!b){b=Na(a);if(null===b)throw Error(k(188));return b!==a?null:a}for(var c=a,d=b;;){var e=c.return;if(null===e)break;var f=e.alternate;if(null===f){d=e.return;if(null!==d){c=d;continue}break}if(e.child===f.child){for(f=e.child;f;){if(f===c)return Rf(e),a;if(f===d)return Rf(e),b;f=f.sibling}throw Error(k(188));}if(c.return!==d.return)c=e,d=f;else{for(var g=!1,h=e.child;h;){if(h===c){g=!0;c=e;d=f;break}if(h===d){g=!0;d=e;c=f;break}h=h.sibling}if(!g){for(h=
f.child;h;){if(h===c){g=!0;c=f;d=e;break}if(h===d){g=!0;d=f;c=e;break}h=h.sibling}if(!g)throw Error(k(189));}}if(c.alternate!==d)throw Error(k(190));}if(3!==c.tag)throw Error(k(188));return c.stateNode.current===c?a:b}function Sf(a){a=vi(a);if(!a)return null;for(var b=a;;){if(5===b.tag||6===b.tag)return b;if(b.child)b.child.return=b,b=b.child;else{if(b===a)break;for(;!b.sibling;){if(!b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}}return null}function jb(a,b){if(null==
b)throw Error(k(30));if(null==a)return b;if(Array.isArray(a)){if(Array.isArray(b))return a.push.apply(a,b),a;a.push(b);return a}return Array.isArray(b)?[a].concat(b):[a,b]}function Kd(a,b,c){Array.isArray(a)?a.forEach(b,c):a&&b.call(c,a)}function pc(a){null!==a&&(Ab=jb(Ab,a));a=Ab;Ab=null;if(a){Kd(a,wi);if(Ab)throw Error(k(95));if(hc)throw a=pd,hc=!1,pd=null,a;}}function Ld(a){a=a.target||a.srcElement||window;a.correspondingUseElement&&(a=a.correspondingUseElement);return 3===a.nodeType?a.parentNode:
a}function Tf(a){if(!wa)return!1;a="on"+a;var b=a in document;b||(b=document.createElement("div"),b.setAttribute(a,"return;"),b="function"===typeof b[a]);return b}function Uf(a){a.topLevelType=null;a.nativeEvent=null;a.targetInst=null;a.ancestors.length=0;10>qc.length&&qc.push(a)}function Vf(a,b,c,d){if(qc.length){var e=qc.pop();e.topLevelType=a;e.eventSystemFlags=d;e.nativeEvent=b;e.targetInst=c;return e}return{topLevelType:a,eventSystemFlags:d,nativeEvent:b,targetInst:c,ancestors:[]}}function Wf(a){var b=
a.targetInst,c=b;do{if(!c){a.ancestors.push(c);break}var d=c;if(3===d.tag)d=d.stateNode.containerInfo;else{for(;d.return;)d=d.return;d=3!==d.tag?null:d.stateNode.containerInfo}if(!d)break;b=c.tag;5!==b&&6!==b||a.ancestors.push(c);c=Bb(d)}while(c);for(c=0;c<a.ancestors.length;c++){b=a.ancestors[c];var e=Ld(a.nativeEvent);d=a.topLevelType;var f=a.nativeEvent,g=a.eventSystemFlags;0===c&&(g|=64);for(var h=null,m=0;m<jc.length;m++){var n=jc[m];n&&(n=n.extractEvents(d,b,f,e,g))&&(h=jb(h,n))}pc(h)}}function Md(a,
b,c){if(!c.has(a)){switch(a){case "scroll":Cb(b,"scroll",!0);break;case "focus":case "blur":Cb(b,"focus",!0);Cb(b,"blur",!0);c.set("blur",null);c.set("focus",null);break;case "cancel":case "close":Tf(a)&&Cb(b,a,!0);break;case "invalid":case "submit":case "reset":break;default:-1===Db.indexOf(a)&&w(a,b)}c.set(a,null)}}function xi(a,b){var c=Jd(b);Nd.forEach(function(a){Md(a,b,c)});yi.forEach(function(a){Md(a,b,c)})}function Od(a,b,c,d,e){return{blockedOn:a,topLevelType:b,eventSystemFlags:c|32,nativeEvent:e,
container:d}}function Xf(a,b){switch(a){case "focus":case "blur":xa=null;break;case "dragenter":case "dragleave":ya=null;break;case "mouseover":case "mouseout":za=null;break;case "pointerover":case "pointerout":Eb.delete(b.pointerId);break;case "gotpointercapture":case "lostpointercapture":Fb.delete(b.pointerId)}}function Gb(a,b,c,d,e,f){if(null===a||a.nativeEvent!==f)return a=Od(b,c,d,e,f),null!==b&&(b=Hb(b),null!==b&&Yf(b)),a;a.eventSystemFlags|=d;return a}function zi(a,b,c,d,e){switch(b){case "focus":return xa=
Gb(xa,a,b,c,d,e),!0;case "dragenter":return ya=Gb(ya,a,b,c,d,e),!0;case "mouseover":return za=Gb(za,a,b,c,d,e),!0;case "pointerover":var f=e.pointerId;Eb.set(f,Gb(Eb.get(f)||null,a,b,c,d,e));return!0;case "gotpointercapture":return f=e.pointerId,Fb.set(f,Gb(Fb.get(f)||null,a,b,c,d,e)),!0}return!1}function Ai(a){var b=Bb(a.target);if(null!==b){var c=Na(b);if(null!==c)if(b=c.tag,13===b){if(b=Qf(c),null!==b){a.blockedOn=b;Pd(a.priority,function(){Bi(c)});return}}else if(3===b&&c.stateNode.hydrate){a.blockedOn=
3===c.tag?c.stateNode.containerInfo:null;return}}a.blockedOn=null}function rc(a){if(null!==a.blockedOn)return!1;var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);if(null!==b){var c=Hb(b);null!==c&&Yf(c);a.blockedOn=b;return!1}return!0}function Zf(a,b,c){rc(a)&&c.delete(b)}function Ci(){for(Rd=!1;0<fa.length;){var a=fa[0];if(null!==a.blockedOn){a=Hb(a.blockedOn);null!==a&&Di(a);break}var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);null!==b?a.blockedOn=b:fa.shift()}null!==
xa&&rc(xa)&&(xa=null);null!==ya&&rc(ya)&&(ya=null);null!==za&&rc(za)&&(za=null);Eb.forEach(Zf);Fb.forEach(Zf)}function Ib(a,b){a.blockedOn===b&&(a.blockedOn=null,Rd||(Rd=!0,$f(ag,Ci)))}function bg(a){if(0<fa.length){Ib(fa[0],a);for(var b=1;b<fa.length;b++){var c=fa[b];c.blockedOn===a&&(c.blockedOn=null)}}null!==xa&&Ib(xa,a);null!==ya&&Ib(ya,a);null!==za&&Ib(za,a);b=function(b){return Ib(b,a)};Eb.forEach(b);Fb.forEach(b);for(b=0;b<Jb.length;b++)c=Jb[b],c.blockedOn===a&&(c.blockedOn=null);for(;0<Jb.length&&
(b=Jb[0],null===b.blockedOn);)Ai(b),null===b.blockedOn&&Jb.shift()}function Sd(a,b){for(var c=0;c<a.length;c+=2){var d=a[c],e=a[c+1],f="on"+(e[0].toUpperCase()+e.slice(1));f={phasedRegistrationNames:{bubbled:f,captured:f+"Capture"},dependencies:[d],eventPriority:b};Td.set(d,b);cg.set(d,f);dg[e]=f}}function w(a,b){Cb(b,a,!1)}function Cb(a,b,c){var d=Td.get(b);switch(void 0===d?2:d){case 0:d=Ei.bind(null,b,1,a);break;case 1:d=Fi.bind(null,b,1,a);break;default:d=sc.bind(null,b,1,a)}c?a.addEventListener(b,
d,!0):a.addEventListener(b,d,!1)}function Ei(a,b,c,d){Oa||vd();var e=sc,f=Oa;Oa=!0;try{eg(e,a,b,c,d)}finally{(Oa=f)||ud()}}function Fi(a,b,c,d){Gi(Hi,sc.bind(null,a,b,c,d))}function sc(a,b,c,d){if(tc)if(0<fa.length&&-1<Nd.indexOf(a))a=Od(null,a,b,c,d),fa.push(a);else{var e=Qd(a,b,c,d);if(null===e)Xf(a,d);else if(-1<Nd.indexOf(a))a=Od(e,a,b,c,d),fa.push(a);else if(!zi(e,a,b,c,d)){Xf(a,d);a=Vf(a,d,null,b);try{uf(Wf,a)}finally{Uf(a)}}}}function Qd(a,b,c,d){c=Ld(d);c=Bb(c);if(null!==c){var e=Na(c);if(null===
e)c=null;else{var f=e.tag;if(13===f){c=Qf(e);if(null!==c)return c;c=null}else if(3===f){if(e.stateNode.hydrate)return 3===e.tag?e.stateNode.containerInfo:null;c=null}else e!==c&&(c=null)}}a=Vf(a,d,c,b);try{uf(Wf,a)}finally{Uf(a)}return null}function fg(a,b,c){return null==b||"boolean"===typeof b||""===b?"":c||"number"!==typeof b||0===b||Kb.hasOwnProperty(a)&&Kb[a]?(""+b).trim():b+"px"}function gg(a,b){a=a.style;for(var c in b)if(b.hasOwnProperty(c)){var d=0===c.indexOf("--"),e=fg(c,b[c],d);"float"===
c&&(c="cssFloat");d?a.setProperty(c,e):a[c]=e}}function Ud(a,b){if(b){if(Ii[a]&&(null!=b.children||null!=b.dangerouslySetInnerHTML))throw Error(k(137,a,""));if(null!=b.dangerouslySetInnerHTML){if(null!=b.children)throw Error(k(60));if(!("object"===typeof b.dangerouslySetInnerHTML&&"__html"in b.dangerouslySetInnerHTML))throw Error(k(61));}if(null!=b.style&&"object"!==typeof b.style)throw Error(k(62,""));}}function Vd(a,b){if(-1===a.indexOf("-"))return"string"===typeof b.is;switch(a){case "annotation-xml":case "color-profile":case "font-face":case "font-face-src":case "font-face-uri":case "font-face-format":case "font-face-name":case "missing-glyph":return!1;
default:return!0}}function oa(a,b){a=9===a.nodeType||11===a.nodeType?a:a.ownerDocument;var c=Jd(a);b=rd[b];for(var d=0;d<b.length;d++)Md(b[d],a,c)}function uc(){}function Wd(a){a=a||("undefined"!==typeof document?document:void 0);if("undefined"===typeof a)return null;try{return a.activeElement||a.body}catch(b){return a.body}}function hg(a){for(;a&&a.firstChild;)a=a.firstChild;return a}function ig(a,b){var c=hg(a);a=0;for(var d;c;){if(3===c.nodeType){d=a+c.textContent.length;if(a<=b&&d>=b)return{node:c,
offset:b-a};a=d}a:{for(;c;){if(c.nextSibling){c=c.nextSibling;break a}c=c.parentNode}c=void 0}c=hg(c)}}function jg(a,b){return a&&b?a===b?!0:a&&3===a.nodeType?!1:b&&3===b.nodeType?jg(a,b.parentNode):"contains"in a?a.contains(b):a.compareDocumentPosition?!!(a.compareDocumentPosition(b)&16):!1:!1}function kg(){for(var a=window,b=Wd();b instanceof a.HTMLIFrameElement;){try{var c="string"===typeof b.contentWindow.location.href}catch(d){c=!1}if(c)a=b.contentWindow;else break;b=Wd(a.document)}return b}
function Xd(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return b&&("input"===b&&("text"===a.type||"search"===a.type||"tel"===a.type||"url"===a.type||"password"===a.type)||"textarea"===b||"true"===a.contentEditable)}function lg(a,b){switch(a){case "button":case "input":case "select":case "textarea":return!!b.autoFocus}return!1}function Yd(a,b){return"textarea"===a||"option"===a||"noscript"===a||"string"===typeof b.children||"number"===typeof b.children||"object"===typeof b.dangerouslySetInnerHTML&&
null!==b.dangerouslySetInnerHTML&&null!=b.dangerouslySetInnerHTML.__html}function kb(a){for(;null!=a;a=a.nextSibling){var b=a.nodeType;if(1===b||3===b)break}return a}function mg(a){a=a.previousSibling;for(var b=0;a;){if(8===a.nodeType){var c=a.data;if(c===ng||c===Zd||c===$d){if(0===b)return a;b--}else c===og&&b++}a=a.previousSibling}return null}function Bb(a){var b=a[Aa];if(b)return b;for(var c=a.parentNode;c;){if(b=c[Lb]||c[Aa]){c=b.alternate;if(null!==b.child||null!==c&&null!==c.child)for(a=mg(a);null!==
a;){if(c=a[Aa])return c;a=mg(a)}return b}a=c;c=a.parentNode}return null}function Hb(a){a=a[Aa]||a[Lb];return!a||5!==a.tag&&6!==a.tag&&13!==a.tag&&3!==a.tag?null:a}function Pa(a){if(5===a.tag||6===a.tag)return a.stateNode;throw Error(k(33));}function ae(a){return a[vc]||null}function pa(a){do a=a.return;while(a&&5!==a.tag);return a?a:null}function pg(a,b){var c=a.stateNode;if(!c)return null;var d=td(c);if(!d)return null;c=d[b];a:switch(b){case "onClick":case "onClickCapture":case "onDoubleClick":case "onDoubleClickCapture":case "onMouseDown":case "onMouseDownCapture":case "onMouseMove":case "onMouseMoveCapture":case "onMouseUp":case "onMouseUpCapture":case "onMouseEnter":(d=
!d.disabled)||(a=a.type,d=!("button"===a||"input"===a||"select"===a||"textarea"===a));a=!d;break a;default:a=!1}if(a)return null;if(c&&"function"!==typeof c)throw Error(k(231,b,typeof c));return c}function qg(a,b,c){if(b=pg(a,c.dispatchConfig.phasedRegistrationNames[b]))c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a)}function Ji(a){if(a&&a.dispatchConfig.phasedRegistrationNames){for(var b=a._targetInst,c=[];b;)c.push(b),b=pa(b);for(b=c.length;0<b--;)qg(c[b],
"captured",a);for(b=0;b<c.length;b++)qg(c[b],"bubbled",a)}}function be(a,b,c){a&&c&&c.dispatchConfig.registrationName&&(b=pg(a,c.dispatchConfig.registrationName))&&(c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a))}function Ki(a){a&&a.dispatchConfig.registrationName&&be(a._targetInst,null,a)}function lb(a){Kd(a,Ji)}function rg(){if(wc)return wc;var a,b=ce,c=b.length,d,e="value"in Ba?Ba.value:Ba.textContent,f=e.length;for(a=0;a<c&&b[a]===e[a];a++);var g=
c-a;for(d=1;d<=g&&b[c-d]===e[f-d];d++);return wc=e.slice(a,1<d?1-d:void 0)}function xc(){return!0}function yc(){return!1}function R(a,b,c,d){this.dispatchConfig=a;this._targetInst=b;this.nativeEvent=c;a=this.constructor.Interface;for(var e in a)a.hasOwnProperty(e)&&((b=a[e])?this[e]=b(c):"target"===e?this.target=d:this[e]=c[e]);this.isDefaultPrevented=(null!=c.defaultPrevented?c.defaultPrevented:!1===c.returnValue)?xc:yc;this.isPropagationStopped=yc;return this}function Li(a,b,c,d){if(this.eventPool.length){var e=
this.eventPool.pop();this.call(e,a,b,c,d);return e}return new this(a,b,c,d)}function Mi(a){if(!(a instanceof this))throw Error(k(279));a.destructor();10>this.eventPool.length&&this.eventPool.push(a)}function sg(a){a.eventPool=[];a.getPooled=Li;a.release=Mi}function tg(a,b){switch(a){case "keyup":return-1!==Ni.indexOf(b.keyCode);case "keydown":return 229!==b.keyCode;case "keypress":case "mousedown":case "blur":return!0;default:return!1}}function ug(a){a=a.detail;return"object"===typeof a&&"data"in
a?a.data:null}function Oi(a,b){switch(a){case "compositionend":return ug(b);case "keypress":if(32!==b.which)return null;vg=!0;return wg;case "textInput":return a=b.data,a===wg&&vg?null:a;default:return null}}function Pi(a,b){if(mb)return"compositionend"===a||!de&&tg(a,b)?(a=rg(),wc=ce=Ba=null,mb=!1,a):null;switch(a){case "paste":return null;case "keypress":if(!(b.ctrlKey||b.altKey||b.metaKey)||b.ctrlKey&&b.altKey){if(b.char&&1<b.char.length)return b.char;if(b.which)return String.fromCharCode(b.which)}return null;
case "compositionend":return xg&&"ko"!==b.locale?null:b.data;default:return null}}function yg(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return"input"===b?!!Qi[a.type]:"textarea"===b?!0:!1}function zg(a,b,c){a=R.getPooled(Ag.change,a,b,c);a.type="change";sf(c);lb(a);return a}function Ri(a){pc(a)}function zc(a){var b=Pa(a);if(Gf(b))return a}function Si(a,b){if("change"===a)return b}function Bg(){Mb&&(Mb.detachEvent("onpropertychange",Cg),Nb=Mb=null)}function Cg(a){if("value"===a.propertyName&&
zc(Nb))if(a=zg(Nb,a,Ld(a)),Oa)pc(a);else{Oa=!0;try{ee(Ri,a)}finally{Oa=!1,ud()}}}function Ti(a,b,c){"focus"===a?(Bg(),Mb=b,Nb=c,Mb.attachEvent("onpropertychange",Cg)):"blur"===a&&Bg()}function Ui(a,b){if("selectionchange"===a||"keyup"===a||"keydown"===a)return zc(Nb)}function Vi(a,b){if("click"===a)return zc(b)}function Wi(a,b){if("input"===a||"change"===a)return zc(b)}function Xi(a){var b=this.nativeEvent;return b.getModifierState?b.getModifierState(a):(a=Yi[a])?!!b[a]:!1}function fe(a){return Xi}
function Zi(a,b){return a===b&&(0!==a||1/a===1/b)||a!==a&&b!==b}function Ob(a,b){if(Qa(a,b))return!0;if("object"!==typeof a||null===a||"object"!==typeof b||null===b)return!1;var c=Object.keys(a),d=Object.keys(b);if(c.length!==d.length)return!1;for(d=0;d<c.length;d++)if(!$i.call(b,c[d])||!Qa(a[c[d]],b[c[d]]))return!1;return!0}function Dg(a,b){var c=b.window===b?b.document:9===b.nodeType?b:b.ownerDocument;if(ge||null==nb||nb!==Wd(c))return null;c=nb;"selectionStart"in c&&Xd(c)?c={start:c.selectionStart,
end:c.selectionEnd}:(c=(c.ownerDocument&&c.ownerDocument.defaultView||window).getSelection(),c={anchorNode:c.anchorNode,anchorOffset:c.anchorOffset,focusNode:c.focusNode,focusOffset:c.focusOffset});return Pb&&Ob(Pb,c)?null:(Pb=c,a=R.getPooled(Eg.select,he,a,b),a.type="select",a.target=nb,lb(a),a)}function Ac(a){var b=a.keyCode;"charCode"in a?(a=a.charCode,0===a&&13===b&&(a=13)):a=b;10===a&&(a=13);return 32<=a||13===a?a:0}function q(a,b){0>ob||(a.current=ie[ob],ie[ob]=null,ob--)}function y(a,b,c){ob++;
ie[ob]=a.current;a.current=b}function pb(a,b){var c=a.type.contextTypes;if(!c)return Ca;var d=a.stateNode;if(d&&d.__reactInternalMemoizedUnmaskedChildContext===b)return d.__reactInternalMemoizedMaskedChildContext;var e={},f;for(f in c)e[f]=b[f];d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=b,a.__reactInternalMemoizedMaskedChildContext=e);return e}function N(a){a=a.childContextTypes;return null!==a&&void 0!==a}function Fg(a,b,c){if(B.current!==Ca)throw Error(k(168));y(B,b);y(G,c)}
function Gg(a,b,c){var d=a.stateNode;a=b.childContextTypes;if("function"!==typeof d.getChildContext)return c;d=d.getChildContext();for(var e in d)if(!(e in a))throw Error(k(108,na(b)||"Unknown",e));return M({},c,{},d)}function Bc(a){a=(a=a.stateNode)&&a.__reactInternalMemoizedMergedChildContext||Ca;Ra=B.current;y(B,a);y(G,G.current);return!0}function Hg(a,b,c){var d=a.stateNode;if(!d)throw Error(k(169));c?(a=Gg(a,b,Ra),d.__reactInternalMemoizedMergedChildContext=a,q(G),q(B),y(B,a)):q(G);y(G,c)}function Cc(){switch(aj()){case Dc:return 99;
case Ig:return 98;case Jg:return 97;case Kg:return 96;case Lg:return 95;default:throw Error(k(332));}}function Mg(a){switch(a){case 99:return Dc;case 98:return Ig;case 97:return Jg;case 96:return Kg;case 95:return Lg;default:throw Error(k(332));}}function Da(a,b){a=Mg(a);return bj(a,b)}function Ng(a,b,c){a=Mg(a);return je(a,b,c)}function Og(a){null===qa?(qa=[a],Ec=je(Dc,Pg)):qa.push(a);return Qg}function ha(){if(null!==Ec){var a=Ec;Ec=null;Rg(a)}Pg()}function Pg(){if(!ke&&null!==qa){ke=!0;var a=0;
try{var b=qa;Da(99,function(){for(;a<b.length;a++){var c=b[a];do c=c(!0);while(null!==c)}});qa=null}catch(c){throw null!==qa&&(qa=qa.slice(a+1)),je(Dc,ha),c;}finally{ke=!1}}}function Fc(a,b,c){c/=10;return 1073741821-(((1073741821-a+b/10)/c|0)+1)*c}function aa(a,b){if(a&&a.defaultProps){b=M({},b);a=a.defaultProps;for(var c in a)void 0===b[c]&&(b[c]=a[c])}return b}function le(){Gc=qb=Hc=null}function me(a){var b=Ic.current;q(Ic);a.type._context._currentValue=b}function Sg(a,b){for(;null!==a;){var c=
a.alternate;if(a.childExpirationTime<b)a.childExpirationTime=b,null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);else if(null!==c&&c.childExpirationTime<b)c.childExpirationTime=b;else break;a=a.return}}function rb(a,b){Hc=a;Gc=qb=null;a=a.dependencies;null!==a&&null!==a.firstContext&&(a.expirationTime>=b&&(ia=!0),a.firstContext=null)}function W(a,b){if(Gc!==a&&!1!==b&&0!==b){if("number"!==typeof b||1073741823===b)Gc=a,b=1073741823;b={context:a,observedBits:b,next:null};if(null===qb){if(null===
Hc)throw Error(k(308));qb=b;Hc.dependencies={expirationTime:0,firstContext:b,responders:null}}else qb=qb.next=b}return a._currentValue}function ne(a){a.updateQueue={baseState:a.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function oe(a,b){a=a.updateQueue;b.updateQueue===a&&(b.updateQueue={baseState:a.baseState,baseQueue:a.baseQueue,shared:a.shared,effects:a.effects})}function Ea(a,b){a={expirationTime:a,suspenseConfig:b,tag:Tg,payload:null,callback:null,next:null};return a.next=
a}function Fa(a,b){a=a.updateQueue;if(null!==a){a=a.shared;var c=a.pending;null===c?b.next=b:(b.next=c.next,c.next=b);a.pending=b}}function Ug(a,b){var c=a.alternate;null!==c&&oe(c,a);a=a.updateQueue;c=a.baseQueue;null===c?(a.baseQueue=b.next=b,b.next=b):(b.next=c.next,c.next=b)}function Qb(a,b,c,d){var e=a.updateQueue;Ga=!1;var f=e.baseQueue,g=e.shared.pending;if(null!==g){if(null!==f){var h=f.next;f.next=g.next;g.next=h}f=g;e.shared.pending=null;h=a.alternate;null!==h&&(h=h.updateQueue,null!==h&&
(h.baseQueue=g))}if(null!==f){h=f.next;var m=e.baseState,n=0,k=null,ba=null,l=null;if(null!==h){var p=h;do{g=p.expirationTime;if(g<d){var t={expirationTime:p.expirationTime,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null};null===l?(ba=l=t,k=m):l=l.next=t;g>n&&(n=g)}else{null!==l&&(l=l.next={expirationTime:1073741823,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null});Vg(g,p.suspenseConfig);a:{var q=a,r=p;g=b;t=c;switch(r.tag){case 1:q=
r.payload;if("function"===typeof q){m=q.call(t,m,g);break a}m=q;break a;case 3:q.effectTag=q.effectTag&-4097|64;case Tg:q=r.payload;g="function"===typeof q?q.call(t,m,g):q;if(null===g||void 0===g)break a;m=M({},m,g);break a;case Jc:Ga=!0}}null!==p.callback&&(a.effectTag|=32,g=e.effects,null===g?e.effects=[p]:g.push(p))}p=p.next;if(null===p||p===h)if(g=e.shared.pending,null===g)break;else p=f.next=g.next,g.next=h,e.baseQueue=f=g,e.shared.pending=null}while(1)}null===l?k=m:l.next=ba;e.baseState=k;e.baseQueue=
l;Kc(n);a.expirationTime=n;a.memoizedState=m}}function Wg(a,b,c){a=b.effects;b.effects=null;if(null!==a)for(b=0;b<a.length;b++){var d=a[b],e=d.callback;if(null!==e){d.callback=null;d=e;e=c;if("function"!==typeof d)throw Error(k(191,d));d.call(e)}}}function Lc(a,b,c,d){b=a.memoizedState;c=c(d,b);c=null===c||void 0===c?b:M({},b,c);a.memoizedState=c;0===a.expirationTime&&(a.updateQueue.baseState=c)}function Xg(a,b,c,d,e,f,g){a=a.stateNode;return"function"===typeof a.shouldComponentUpdate?a.shouldComponentUpdate(d,
f,g):b.prototype&&b.prototype.isPureReactComponent?!Ob(c,d)||!Ob(e,f):!0}function Yg(a,b,c){var d=!1,e=Ca;var f=b.contextType;"object"===typeof f&&null!==f?f=W(f):(e=N(b)?Ra:B.current,d=b.contextTypes,f=(d=null!==d&&void 0!==d)?pb(a,e):Ca);b=new b(c,f);a.memoizedState=null!==b.state&&void 0!==b.state?b.state:null;b.updater=Mc;a.stateNode=b;b._reactInternalFiber=a;d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=e,a.__reactInternalMemoizedMaskedChildContext=f);return b}function Zg(a,
b,c,d){a=b.state;"function"===typeof b.componentWillReceiveProps&&b.componentWillReceiveProps(c,d);"function"===typeof b.UNSAFE_componentWillReceiveProps&&b.UNSAFE_componentWillReceiveProps(c,d);b.state!==a&&Mc.enqueueReplaceState(b,b.state,null)}function pe(a,b,c,d){var e=a.stateNode;e.props=c;e.state=a.memoizedState;e.refs=$g;ne(a);var f=b.contextType;"object"===typeof f&&null!==f?e.context=W(f):(f=N(b)?Ra:B.current,e.context=pb(a,f));Qb(a,c,e,d);e.state=a.memoizedState;f=b.getDerivedStateFromProps;
"function"===typeof f&&(Lc(a,b,f,c),e.state=a.memoizedState);"function"===typeof b.getDerivedStateFromProps||"function"===typeof e.getSnapshotBeforeUpdate||"function"!==typeof e.UNSAFE_componentWillMount&&"function"!==typeof e.componentWillMount||(b=e.state,"function"===typeof e.componentWillMount&&e.componentWillMount(),"function"===typeof e.UNSAFE_componentWillMount&&e.UNSAFE_componentWillMount(),b!==e.state&&Mc.enqueueReplaceState(e,e.state,null),Qb(a,c,e,d),e.state=a.memoizedState);"function"===
typeof e.componentDidMount&&(a.effectTag|=4)}function Rb(a,b,c){a=c.ref;if(null!==a&&"function"!==typeof a&&"object"!==typeof a){if(c._owner){c=c._owner;if(c){if(1!==c.tag)throw Error(k(309));var d=c.stateNode}if(!d)throw Error(k(147,a));var e=""+a;if(null!==b&&null!==b.ref&&"function"===typeof b.ref&&b.ref._stringRef===e)return b.ref;b=function(a){var b=d.refs;b===$g&&(b=d.refs={});null===a?delete b[e]:b[e]=a};b._stringRef=e;return b}if("string"!==typeof a)throw Error(k(284));if(!c._owner)throw Error(k(290,
a));}return a}function Nc(a,b){if("textarea"!==a.type)throw Error(k(31,"[object Object]"===Object.prototype.toString.call(b)?"object with keys {"+Object.keys(b).join(", ")+"}":b,""));}function ah(a){function b(b,c){if(a){var d=b.lastEffect;null!==d?(d.nextEffect=c,b.lastEffect=c):b.firstEffect=b.lastEffect=c;c.nextEffect=null;c.effectTag=8}}function c(c,d){if(!a)return null;for(;null!==d;)b(c,d),d=d.sibling;return null}function d(a,b){for(a=new Map;null!==b;)null!==b.key?a.set(b.key,b):a.set(b.index,
b),b=b.sibling;return a}function e(a,b){a=Sa(a,b);a.index=0;a.sibling=null;return a}function f(b,c,d){b.index=d;if(!a)return c;d=b.alternate;if(null!==d)return d=d.index,d<c?(b.effectTag=2,c):d;b.effectTag=2;return c}function g(b){a&&null===b.alternate&&(b.effectTag=2);return b}function h(a,b,c,d){if(null===b||6!==b.tag)return b=qe(c,a.mode,d),b.return=a,b;b=e(b,c);b.return=a;return b}function m(a,b,c,d){if(null!==b&&b.elementType===c.type)return d=e(b,c.props),d.ref=Rb(a,b,c),d.return=a,d;d=Oc(c.type,
c.key,c.props,null,a.mode,d);d.ref=Rb(a,b,c);d.return=a;return d}function n(a,b,c,d){if(null===b||4!==b.tag||b.stateNode.containerInfo!==c.containerInfo||b.stateNode.implementation!==c.implementation)return b=re(c,a.mode,d),b.return=a,b;b=e(b,c.children||[]);b.return=a;return b}function l(a,b,c,d,f){if(null===b||7!==b.tag)return b=Ha(c,a.mode,d,f),b.return=a,b;b=e(b,c);b.return=a;return b}function ba(a,b,c){if("string"===typeof b||"number"===typeof b)return b=qe(""+b,a.mode,c),b.return=a,b;if("object"===
typeof b&&null!==b){switch(b.$$typeof){case Pc:return c=Oc(b.type,b.key,b.props,null,a.mode,c),c.ref=Rb(a,null,b),c.return=a,c;case gb:return b=re(b,a.mode,c),b.return=a,b}if(Qc(b)||zb(b))return b=Ha(b,a.mode,c,null),b.return=a,b;Nc(a,b)}return null}function p(a,b,c,d){var e=null!==b?b.key:null;if("string"===typeof c||"number"===typeof c)return null!==e?null:h(a,b,""+c,d);if("object"===typeof c&&null!==c){switch(c.$$typeof){case Pc:return c.key===e?c.type===Ma?l(a,b,c.props.children,d,e):m(a,b,c,
d):null;case gb:return c.key===e?n(a,b,c,d):null}if(Qc(c)||zb(c))return null!==e?null:l(a,b,c,d,null);Nc(a,c)}return null}function t(a,b,c,d,e){if("string"===typeof d||"number"===typeof d)return a=a.get(c)||null,h(b,a,""+d,e);if("object"===typeof d&&null!==d){switch(d.$$typeof){case Pc:return a=a.get(null===d.key?c:d.key)||null,d.type===Ma?l(b,a,d.props.children,e,d.key):m(b,a,d,e);case gb:return a=a.get(null===d.key?c:d.key)||null,n(b,a,d,e)}if(Qc(d)||zb(d))return a=a.get(c)||null,l(b,a,d,e,null);
Nc(b,d)}return null}function q(e,g,h,m){for(var n=null,k=null,l=g,r=g=0,C=null;null!==l&&r<h.length;r++){l.index>r?(C=l,l=null):C=l.sibling;var O=p(e,l,h[r],m);if(null===O){null===l&&(l=C);break}a&&l&&null===O.alternate&&b(e,l);g=f(O,g,r);null===k?n=O:k.sibling=O;k=O;l=C}if(r===h.length)return c(e,l),n;if(null===l){for(;r<h.length;r++)l=ba(e,h[r],m),null!==l&&(g=f(l,g,r),null===k?n=l:k.sibling=l,k=l);return n}for(l=d(e,l);r<h.length;r++)C=t(l,e,r,h[r],m),null!==C&&(a&&null!==C.alternate&&l.delete(null===
C.key?r:C.key),g=f(C,g,r),null===k?n=C:k.sibling=C,k=C);a&&l.forEach(function(a){return b(e,a)});return n}function w(e,g,h,n){var m=zb(h);if("function"!==typeof m)throw Error(k(150));h=m.call(h);if(null==h)throw Error(k(151));for(var l=m=null,r=g,C=g=0,O=null,v=h.next();null!==r&&!v.done;C++,v=h.next()){r.index>C?(O=r,r=null):O=r.sibling;var q=p(e,r,v.value,n);if(null===q){null===r&&(r=O);break}a&&r&&null===q.alternate&&b(e,r);g=f(q,g,C);null===l?m=q:l.sibling=q;l=q;r=O}if(v.done)return c(e,r),m;
if(null===r){for(;!v.done;C++,v=h.next())v=ba(e,v.value,n),null!==v&&(g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);return m}for(r=d(e,r);!v.done;C++,v=h.next())v=t(r,e,C,v.value,n),null!==v&&(a&&null!==v.alternate&&r.delete(null===v.key?C:v.key),g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);a&&r.forEach(function(a){return b(e,a)});return m}return function(a,d,f,h){var m="object"===typeof f&&null!==f&&f.type===Ma&&null===f.key;m&&(f=f.props.children);var n="object"===typeof f&&null!==f;if(n)switch(f.$$typeof){case Pc:a:{n=
f.key;for(m=d;null!==m;){if(m.key===n){switch(m.tag){case 7:if(f.type===Ma){c(a,m.sibling);d=e(m,f.props.children);d.return=a;a=d;break a}break;default:if(m.elementType===f.type){c(a,m.sibling);d=e(m,f.props);d.ref=Rb(a,m,f);d.return=a;a=d;break a}}c(a,m);break}else b(a,m);m=m.sibling}f.type===Ma?(d=Ha(f.props.children,a.mode,h,f.key),d.return=a,a=d):(h=Oc(f.type,f.key,f.props,null,a.mode,h),h.ref=Rb(a,d,f),h.return=a,a=h)}return g(a);case gb:a:{for(m=f.key;null!==d;){if(d.key===m)if(4===d.tag&&d.stateNode.containerInfo===
f.containerInfo&&d.stateNode.implementation===f.implementation){c(a,d.sibling);d=e(d,f.children||[]);d.return=a;a=d;break a}else{c(a,d);break}else b(a,d);d=d.sibling}d=re(f,a.mode,h);d.return=a;a=d}return g(a)}if("string"===typeof f||"number"===typeof f)return f=""+f,null!==d&&6===d.tag?(c(a,d.sibling),d=e(d,f),d.return=a,a=d):(c(a,d),d=qe(f,a.mode,h),d.return=a,a=d),g(a);if(Qc(f))return q(a,d,f,h);if(zb(f))return w(a,d,f,h);n&&Nc(a,f);if("undefined"===typeof f&&!m)switch(a.tag){case 1:case 0:throw a=
a.type,Error(k(152,a.displayName||a.name||"Component"));}return c(a,d)}}function Ta(a){if(a===Sb)throw Error(k(174));return a}function se(a,b){y(Tb,b);y(Ub,a);y(ja,Sb);a=b.nodeType;switch(a){case 9:case 11:b=(b=b.documentElement)?b.namespaceURI:Hd(null,"");break;default:a=8===a?b.parentNode:b,b=a.namespaceURI||null,a=a.tagName,b=Hd(b,a)}q(ja);y(ja,b)}function tb(a){q(ja);q(Ub);q(Tb)}function bh(a){Ta(Tb.current);var b=Ta(ja.current);var c=Hd(b,a.type);b!==c&&(y(Ub,a),y(ja,c))}function te(a){Ub.current===
a&&(q(ja),q(Ub))}function Rc(a){for(var b=a;null!==b;){if(13===b.tag){var c=b.memoizedState;if(null!==c&&(c=c.dehydrated,null===c||c.data===$d||c.data===Zd))return b}else if(19===b.tag&&void 0!==b.memoizedProps.revealOrder){if(0!==(b.effectTag&64))return b}else if(null!==b.child){b.child.return=b;b=b.child;continue}if(b===a)break;for(;null===b.sibling;){if(null===b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}return null}function ue(a,b){return{responder:a,props:b}}
function S(){throw Error(k(321));}function ve(a,b){if(null===b)return!1;for(var c=0;c<b.length&&c<a.length;c++)if(!Qa(a[c],b[c]))return!1;return!0}function we(a,b,c,d,e,f){Ia=f;z=b;b.memoizedState=null;b.updateQueue=null;b.expirationTime=0;Sc.current=null===a||null===a.memoizedState?dj:ej;a=c(d,e);if(b.expirationTime===Ia){f=0;do{b.expirationTime=0;if(!(25>f))throw Error(k(301));f+=1;J=K=null;b.updateQueue=null;Sc.current=fj;a=c(d,e)}while(b.expirationTime===Ia)}Sc.current=Tc;b=null!==K&&null!==K.next;
Ia=0;J=K=z=null;Uc=!1;if(b)throw Error(k(300));return a}function ub(){var a={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};null===J?z.memoizedState=J=a:J=J.next=a;return J}function vb(){if(null===K){var a=z.alternate;a=null!==a?a.memoizedState:null}else a=K.next;var b=null===J?z.memoizedState:J.next;if(null!==b)J=b,K=a;else{if(null===a)throw Error(k(310));K=a;a={memoizedState:K.memoizedState,baseState:K.baseState,baseQueue:K.baseQueue,queue:K.queue,next:null};null===J?z.memoizedState=
J=a:J=J.next=a}return J}function Ua(a,b){return"function"===typeof b?b(a):b}function Vc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=K,e=d.baseQueue,f=c.pending;if(null!==f){if(null!==e){var g=e.next;e.next=f.next;f.next=g}d.baseQueue=e=f;c.pending=null}if(null!==e){e=e.next;d=d.baseState;var h=g=f=null,m=e;do{var n=m.expirationTime;if(n<Ia){var l={expirationTime:m.expirationTime,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,
next:null};null===h?(g=h=l,f=d):h=h.next=l;n>z.expirationTime&&(z.expirationTime=n,Kc(n))}else null!==h&&(h=h.next={expirationTime:1073741823,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,next:null}),Vg(n,m.suspenseConfig),d=m.eagerReducer===a?m.eagerState:a(d,m.action);m=m.next}while(null!==m&&m!==e);null===h?f=d:h.next=g;Qa(d,b.memoizedState)||(ia=!0);b.memoizedState=d;b.baseState=f;b.baseQueue=h;c.lastRenderedState=d}return[b.memoizedState,
c.dispatch]}function Wc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=c.dispatch,e=c.pending,f=b.memoizedState;if(null!==e){c.pending=null;var g=e=e.next;do f=a(f,g.action),g=g.next;while(g!==e);Qa(f,b.memoizedState)||(ia=!0);b.memoizedState=f;null===b.baseQueue&&(b.baseState=f);c.lastRenderedState=f}return[f,d]}function xe(a){var b=ub();"function"===typeof a&&(a=a());b.memoizedState=b.baseState=a;a=b.queue={pending:null,dispatch:null,lastRenderedReducer:Ua,
lastRenderedState:a};a=a.dispatch=ch.bind(null,z,a);return[b.memoizedState,a]}function ye(a,b,c,d){a={tag:a,create:b,destroy:c,deps:d,next:null};b=z.updateQueue;null===b?(b={lastEffect:null},z.updateQueue=b,b.lastEffect=a.next=a):(c=b.lastEffect,null===c?b.lastEffect=a.next=a:(d=c.next,c.next=a,a.next=d,b.lastEffect=a));return a}function dh(a){return vb().memoizedState}function ze(a,b,c,d){var e=ub();z.effectTag|=a;e.memoizedState=ye(1|b,c,void 0,void 0===d?null:d)}function Ae(a,b,c,d){var e=vb();
d=void 0===d?null:d;var f=void 0;if(null!==K){var g=K.memoizedState;f=g.destroy;if(null!==d&&ve(d,g.deps)){ye(b,c,f,d);return}}z.effectTag|=a;e.memoizedState=ye(1|b,c,f,d)}function eh(a,b){return ze(516,4,a,b)}function Xc(a,b){return Ae(516,4,a,b)}function fh(a,b){return Ae(4,2,a,b)}function gh(a,b){if("function"===typeof b)return a=a(),b(a),function(){b(null)};if(null!==b&&void 0!==b)return a=a(),b.current=a,function(){b.current=null}}function hh(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;
return Ae(4,2,gh.bind(null,b,a),c)}function Be(a,b){}function ih(a,b){ub().memoizedState=[a,void 0===b?null:b];return a}function Yc(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];c.memoizedState=[a,b];return a}function jh(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];a=a();c.memoizedState=[a,b];return a}function Ce(a,b,c){var d=Cc();Da(98>d?98:d,function(){a(!0)});Da(97<d?97:d,function(){var d=
X.suspense;X.suspense=void 0===b?null:b;try{a(!1),c()}finally{X.suspense=d}})}function ch(a,b,c){var d=ka(),e=Vb.suspense;d=Va(d,a,e);e={expirationTime:d,suspenseConfig:e,action:c,eagerReducer:null,eagerState:null,next:null};var f=b.pending;null===f?e.next=e:(e.next=f.next,f.next=e);b.pending=e;f=a.alternate;if(a===z||null!==f&&f===z)Uc=!0,e.expirationTime=Ia,z.expirationTime=Ia;else{if(0===a.expirationTime&&(null===f||0===f.expirationTime)&&(f=b.lastRenderedReducer,null!==f))try{var g=b.lastRenderedState,
h=f(g,c);e.eagerReducer=f;e.eagerState=h;if(Qa(h,g))return}catch(m){}finally{}Ja(a,d)}}function kh(a,b){var c=la(5,null,null,0);c.elementType="DELETED";c.type="DELETED";c.stateNode=b;c.return=a;c.effectTag=8;null!==a.lastEffect?(a.lastEffect.nextEffect=c,a.lastEffect=c):a.firstEffect=a.lastEffect=c}function lh(a,b){switch(a.tag){case 5:var c=a.type;b=1!==b.nodeType||c.toLowerCase()!==b.nodeName.toLowerCase()?null:b;return null!==b?(a.stateNode=b,!0):!1;case 6:return b=""===a.pendingProps||3!==b.nodeType?
null:b,null!==b?(a.stateNode=b,!0):!1;case 13:return!1;default:return!1}}function De(a){if(Wa){var b=Ka;if(b){var c=b;if(!lh(a,b)){b=kb(c.nextSibling);if(!b||!lh(a,b)){a.effectTag=a.effectTag&-1025|2;Wa=!1;ra=a;return}kh(ra,c)}ra=a;Ka=kb(b.firstChild)}else a.effectTag=a.effectTag&-1025|2,Wa=!1,ra=a}}function mh(a){for(a=a.return;null!==a&&5!==a.tag&&3!==a.tag&&13!==a.tag;)a=a.return;ra=a}function Zc(a){if(a!==ra)return!1;if(!Wa)return mh(a),Wa=!0,!1;var b=a.type;if(5!==a.tag||"head"!==b&&"body"!==
b&&!Yd(b,a.memoizedProps))for(b=Ka;b;)kh(a,b),b=kb(b.nextSibling);mh(a);if(13===a.tag){a=a.memoizedState;a=null!==a?a.dehydrated:null;if(!a)throw Error(k(317));a:{a=a.nextSibling;for(b=0;a;){if(8===a.nodeType){var c=a.data;if(c===og){if(0===b){Ka=kb(a.nextSibling);break a}b--}else c!==ng&&c!==Zd&&c!==$d||b++}a=a.nextSibling}Ka=null}}else Ka=ra?kb(a.stateNode.nextSibling):null;return!0}function Ee(){Ka=ra=null;Wa=!1}function T(a,b,c,d){b.child=null===a?Fe(b,null,c,d):wb(b,a.child,c,d)}function nh(a,
b,c,d,e){c=c.render;var f=b.ref;rb(b,e);d=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,d,e);return b.child}function oh(a,b,c,d,e,f){if(null===a){var g=c.type;if("function"===typeof g&&!Ge(g)&&void 0===g.defaultProps&&null===c.compare&&void 0===c.defaultProps)return b.tag=15,b.type=g,ph(a,b,g,d,e,f);a=Oc(c.type,null,d,null,b.mode,f);a.ref=b.ref;a.return=b;return b.child=a}g=a.child;if(e<
f&&(e=g.memoizedProps,c=c.compare,c=null!==c?c:Ob,c(e,d)&&a.ref===b.ref))return sa(a,b,f);b.effectTag|=1;a=Sa(g,d);a.ref=b.ref;a.return=b;return b.child=a}function ph(a,b,c,d,e,f){return null!==a&&Ob(a.memoizedProps,d)&&a.ref===b.ref&&(ia=!1,e<f)?(b.expirationTime=a.expirationTime,sa(a,b,f)):He(a,b,c,d,f)}function qh(a,b){var c=b.ref;if(null===a&&null!==c||null!==a&&a.ref!==c)b.effectTag|=128}function He(a,b,c,d,e){var f=N(c)?Ra:B.current;f=pb(b,f);rb(b,e);c=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=
a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,c,e);return b.child}function rh(a,b,c,d,e){if(N(c)){var f=!0;Bc(b)}else f=!1;rb(b,e);if(null===b.stateNode)null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),Yg(b,c,d),pe(b,c,d,e),d=!0;else if(null===a){var g=b.stateNode,h=b.memoizedProps;g.props=h;var m=g.context,n=c.contextType;"object"===typeof n&&null!==n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n));var l=c.getDerivedStateFromProps,k="function"===
typeof l||"function"===typeof g.getSnapshotBeforeUpdate;k||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n);Ga=!1;var p=b.memoizedState;g.state=p;Qb(b,d,g,e);m=b.memoizedState;h!==d||p!==m||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),m=b.memoizedState),(h=Ga||Xg(b,c,h,d,p,m,n))?(k||"function"!==typeof g.UNSAFE_componentWillMount&&"function"!==typeof g.componentWillMount||("function"===typeof g.componentWillMount&&
g.componentWillMount(),"function"===typeof g.UNSAFE_componentWillMount&&g.UNSAFE_componentWillMount()),"function"===typeof g.componentDidMount&&(b.effectTag|=4)):("function"===typeof g.componentDidMount&&(b.effectTag|=4),b.memoizedProps=d,b.memoizedState=m),g.props=d,g.state=m,g.context=n,d=h):("function"===typeof g.componentDidMount&&(b.effectTag|=4),d=!1)}else g=b.stateNode,oe(a,b),h=b.memoizedProps,g.props=b.type===b.elementType?h:aa(b.type,h),m=g.context,n=c.contextType,"object"===typeof n&&null!==
n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n)),l=c.getDerivedStateFromProps,(k="function"===typeof l||"function"===typeof g.getSnapshotBeforeUpdate)||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n),Ga=!1,m=b.memoizedState,g.state=m,Qb(b,d,g,e),p=b.memoizedState,h!==d||m!==p||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),p=b.memoizedState),(l=Ga||Xg(b,c,h,d,m,p,n))?(k||"function"!==typeof g.UNSAFE_componentWillUpdate&&
"function"!==typeof g.componentWillUpdate||("function"===typeof g.componentWillUpdate&&g.componentWillUpdate(d,p,n),"function"===typeof g.UNSAFE_componentWillUpdate&&g.UNSAFE_componentWillUpdate(d,p,n)),"function"===typeof g.componentDidUpdate&&(b.effectTag|=4),"function"===typeof g.getSnapshotBeforeUpdate&&(b.effectTag|=256)):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===
a.memoizedState||(b.effectTag|=256),b.memoizedProps=d,b.memoizedState=p),g.props=d,g.state=p,g.context=n,d=l):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=256),d=!1);return Ie(a,b,c,d,f,e)}function Ie(a,b,c,d,e,f){qh(a,b);var g=0!==(b.effectTag&64);if(!d&&!g)return e&&Hg(b,c,!1),sa(a,b,f);d=b.stateNode;gj.current=b;var h=g&&"function"!==typeof c.getDerivedStateFromError?
null:d.render();b.effectTag|=1;null!==a&&g?(b.child=wb(b,a.child,null,f),b.child=wb(b,null,h,f)):T(a,b,h,f);b.memoizedState=d.state;e&&Hg(b,c,!0);return b.child}function sh(a){var b=a.stateNode;b.pendingContext?Fg(a,b.pendingContext,b.pendingContext!==b.context):b.context&&Fg(a,b.context,!1);se(a,b.containerInfo)}function th(a,b,c){var d=b.mode,e=b.pendingProps,f=D.current,g=!1,h;(h=0!==(b.effectTag&64))||(h=0!==(f&2)&&(null===a||null!==a.memoizedState));h?(g=!0,b.effectTag&=-65):null!==a&&null===
a.memoizedState||void 0===e.fallback||!0===e.unstable_avoidThisFallback||(f|=1);y(D,f&1);if(null===a){void 0!==e.fallback&&De(b);if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;b.memoizedState=Je;b.child=e;return c}d=e.children;b.memoizedState=null;return b.child=Fe(b,null,d,c)}if(null!==a.memoizedState){a=a.child;d=a.sibling;if(g){e=e.fallback;
c=Sa(a,a.pendingProps);c.return=b;if(0===(b.mode&2)&&(g=null!==b.memoizedState?b.child.child:b.child,g!==a.child))for(c.child=g;null!==g;)g.return=c,g=g.sibling;d=Sa(d,e);d.return=b;c.sibling=d;c.childExpirationTime=0;b.memoizedState=Je;b.child=c;return d}c=wb(b,a.child,e.children,c);b.memoizedState=null;return b.child=c}a=a.child;if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;e.child=a;null!==a&&(a.return=e);if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==
a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;c.effectTag|=2;e.childExpirationTime=0;b.memoizedState=Je;b.child=e;return c}b.memoizedState=null;return b.child=wb(b,a,e.children,c)}function uh(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);Sg(a.return,b)}function Ke(a,b,c,d,e,f){var g=a.memoizedState;null===g?a.memoizedState={isBackwards:b,rendering:null,renderingStartTime:0,last:d,tail:c,tailExpiration:0,tailMode:e,
lastEffect:f}:(g.isBackwards=b,g.rendering=null,g.renderingStartTime=0,g.last=d,g.tail=c,g.tailExpiration=0,g.tailMode=e,g.lastEffect=f)}function vh(a,b,c){var d=b.pendingProps,e=d.revealOrder,f=d.tail;T(a,b,d.children,c);d=D.current;if(0!==(d&2))d=d&1|2,b.effectTag|=64;else{if(null!==a&&0!==(a.effectTag&64))a:for(a=b.child;null!==a;){if(13===a.tag)null!==a.memoizedState&&uh(a,c);else if(19===a.tag)uh(a,c);else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===b)break a;for(;null===a.sibling;){if(null===
a.return||a.return===b)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}d&=1}y(D,d);if(0===(b.mode&2))b.memoizedState=null;else switch(e){case "forwards":c=b.child;for(e=null;null!==c;)a=c.alternate,null!==a&&null===Rc(a)&&(e=c),c=c.sibling;c=e;null===c?(e=b.child,b.child=null):(e=c.sibling,c.sibling=null);Ke(b,!1,e,c,f,b.lastEffect);break;case "backwards":c=null;e=b.child;for(b.child=null;null!==e;){a=e.alternate;if(null!==a&&null===Rc(a)){b.child=e;break}a=e.sibling;e.sibling=c;c=e;e=a}Ke(b,
!0,c,null,f,b.lastEffect);break;case "together":Ke(b,!1,null,null,void 0,b.lastEffect);break;default:b.memoizedState=null}return b.child}function sa(a,b,c){null!==a&&(b.dependencies=a.dependencies);var d=b.expirationTime;0!==d&&Kc(d);if(b.childExpirationTime<c)return null;if(null!==a&&b.child!==a.child)throw Error(k(153));if(null!==b.child){a=b.child;c=Sa(a,a.pendingProps);b.child=c;for(c.return=b;null!==a.sibling;)a=a.sibling,c=c.sibling=Sa(a,a.pendingProps),c.return=b;c.sibling=null}return b.child}
function $c(a,b){switch(a.tailMode){case "hidden":b=a.tail;for(var c=null;null!==b;)null!==b.alternate&&(c=b),b=b.sibling;null===c?a.tail=null:c.sibling=null;break;case "collapsed":c=a.tail;for(var d=null;null!==c;)null!==c.alternate&&(d=c),c=c.sibling;null===d?b||null===a.tail?a.tail=null:a.tail.sibling=null:d.sibling=null}}function hj(a,b,c){var d=b.pendingProps;switch(b.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return N(b.type)&&(q(G),q(B)),
null;case 3:return tb(),q(G),q(B),c=b.stateNode,c.pendingContext&&(c.context=c.pendingContext,c.pendingContext=null),null!==a&&null!==a.child||!Zc(b)||(b.effectTag|=4),wh(b),null;case 5:te(b);c=Ta(Tb.current);var e=b.type;if(null!==a&&null!=b.stateNode)ij(a,b,e,d,c),a.ref!==b.ref&&(b.effectTag|=128);else{if(!d){if(null===b.stateNode)throw Error(k(166));return null}a=Ta(ja.current);if(Zc(b)){d=b.stateNode;e=b.type;var f=b.memoizedProps;d[Aa]=b;d[vc]=f;switch(e){case "iframe":case "object":case "embed":w("load",
d);break;case "video":case "audio":for(a=0;a<Db.length;a++)w(Db[a],d);break;case "source":w("error",d);break;case "img":case "image":case "link":w("error",d);w("load",d);break;case "form":w("reset",d);w("submit",d);break;case "details":w("toggle",d);break;case "input":Hf(d,f);w("invalid",d);oa(c,"onChange");break;case "select":d._wrapperState={wasMultiple:!!f.multiple};w("invalid",d);oa(c,"onChange");break;case "textarea":Kf(d,f),w("invalid",d),oa(c,"onChange")}Ud(e,f);a=null;for(var g in f)if(f.hasOwnProperty(g)){var h=
f[g];"children"===g?"string"===typeof h?d.textContent!==h&&(a=["children",h]):"number"===typeof h&&d.textContent!==""+h&&(a=["children",""+h]):db.hasOwnProperty(g)&&null!=h&&oa(c,g)}switch(e){case "input":mc(d);Jf(d,f,!0);break;case "textarea":mc(d);Mf(d);break;case "select":case "option":break;default:"function"===typeof f.onClick&&(d.onclick=uc)}c=a;b.updateQueue=c;null!==c&&(b.effectTag|=4)}else{g=9===c.nodeType?c:c.ownerDocument;"http://www.w3.org/1999/xhtml"===a&&(a=Nf(e));"http://www.w3.org/1999/xhtml"===
a?"script"===e?(a=g.createElement("div"),a.innerHTML="<script>\x3c/script>",a=a.removeChild(a.firstChild)):"string"===typeof d.is?a=g.createElement(e,{is:d.is}):(a=g.createElement(e),"select"===e&&(g=a,d.multiple?g.multiple=!0:d.size&&(g.size=d.size))):a=g.createElementNS(a,e);a[Aa]=b;a[vc]=d;jj(a,b,!1,!1);b.stateNode=a;g=Vd(e,d);switch(e){case "iframe":case "object":case "embed":w("load",a);h=d;break;case "video":case "audio":for(h=0;h<Db.length;h++)w(Db[h],a);h=d;break;case "source":w("error",a);
h=d;break;case "img":case "image":case "link":w("error",a);w("load",a);h=d;break;case "form":w("reset",a);w("submit",a);h=d;break;case "details":w("toggle",a);h=d;break;case "input":Hf(a,d);h=Cd(a,d);w("invalid",a);oa(c,"onChange");break;case "option":h=Fd(a,d);break;case "select":a._wrapperState={wasMultiple:!!d.multiple};h=M({},d,{value:void 0});w("invalid",a);oa(c,"onChange");break;case "textarea":Kf(a,d);h=Gd(a,d);w("invalid",a);oa(c,"onChange");break;default:h=d}Ud(e,h);var m=h;for(f in m)if(m.hasOwnProperty(f)){var n=
m[f];"style"===f?gg(a,n):"dangerouslySetInnerHTML"===f?(n=n?n.__html:void 0,null!=n&&xh(a,n)):"children"===f?"string"===typeof n?("textarea"!==e||""!==n)&&Wb(a,n):"number"===typeof n&&Wb(a,""+n):"suppressContentEditableWarning"!==f&&"suppressHydrationWarning"!==f&&"autoFocus"!==f&&(db.hasOwnProperty(f)?null!=n&&oa(c,f):null!=n&&xd(a,f,n,g))}switch(e){case "input":mc(a);Jf(a,d,!1);break;case "textarea":mc(a);Mf(a);break;case "option":null!=d.value&&a.setAttribute("value",""+va(d.value));break;case "select":a.multiple=
!!d.multiple;c=d.value;null!=c?hb(a,!!d.multiple,c,!1):null!=d.defaultValue&&hb(a,!!d.multiple,d.defaultValue,!0);break;default:"function"===typeof h.onClick&&(a.onclick=uc)}lg(e,d)&&(b.effectTag|=4)}null!==b.ref&&(b.effectTag|=128)}return null;case 6:if(a&&null!=b.stateNode)kj(a,b,a.memoizedProps,d);else{if("string"!==typeof d&&null===b.stateNode)throw Error(k(166));c=Ta(Tb.current);Ta(ja.current);Zc(b)?(c=b.stateNode,d=b.memoizedProps,c[Aa]=b,c.nodeValue!==d&&(b.effectTag|=4)):(c=(9===c.nodeType?
c:c.ownerDocument).createTextNode(d),c[Aa]=b,b.stateNode=c)}return null;case 13:q(D);d=b.memoizedState;if(0!==(b.effectTag&64))return b.expirationTime=c,b;c=null!==d;d=!1;null===a?void 0!==b.memoizedProps.fallback&&Zc(b):(e=a.memoizedState,d=null!==e,c||null===e||(e=a.child.sibling,null!==e&&(f=b.firstEffect,null!==f?(b.firstEffect=e,e.nextEffect=f):(b.firstEffect=b.lastEffect=e,e.nextEffect=null),e.effectTag=8)));if(c&&!d&&0!==(b.mode&2))if(null===a&&!0!==b.memoizedProps.unstable_avoidThisFallback||
0!==(D.current&1))F===Xa&&(F=ad);else{if(F===Xa||F===ad)F=bd;0!==Xb&&null!==U&&(Ya(U,P),yh(U,Xb))}if(c||d)b.effectTag|=4;return null;case 4:return tb(),wh(b),null;case 10:return me(b),null;case 17:return N(b.type)&&(q(G),q(B)),null;case 19:q(D);d=b.memoizedState;if(null===d)return null;e=0!==(b.effectTag&64);f=d.rendering;if(null===f)if(e)$c(d,!1);else{if(F!==Xa||null!==a&&0!==(a.effectTag&64))for(f=b.child;null!==f;){a=Rc(f);if(null!==a){b.effectTag|=64;$c(d,!1);e=a.updateQueue;null!==e&&(b.updateQueue=
e,b.effectTag|=4);null===d.lastEffect&&(b.firstEffect=null);b.lastEffect=d.lastEffect;for(d=b.child;null!==d;)e=d,f=c,e.effectTag&=2,e.nextEffect=null,e.firstEffect=null,e.lastEffect=null,a=e.alternate,null===a?(e.childExpirationTime=0,e.expirationTime=f,e.child=null,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null):(e.childExpirationTime=a.childExpirationTime,e.expirationTime=a.expirationTime,e.child=a.child,e.memoizedProps=a.memoizedProps,e.memoizedState=a.memoizedState,
e.updateQueue=a.updateQueue,f=a.dependencies,e.dependencies=null===f?null:{expirationTime:f.expirationTime,firstContext:f.firstContext,responders:f.responders}),d=d.sibling;y(D,D.current&1|2);return b.child}f=f.sibling}}else{if(!e)if(a=Rc(f),null!==a){if(b.effectTag|=64,e=!0,c=a.updateQueue,null!==c&&(b.updateQueue=c,b.effectTag|=4),$c(d,!0),null===d.tail&&"hidden"===d.tailMode&&!f.alternate)return b=b.lastEffect=d.lastEffect,null!==b&&(b.nextEffect=null),null}else 2*Y()-d.renderingStartTime>d.tailExpiration&&
1<c&&(b.effectTag|=64,e=!0,$c(d,!1),b.expirationTime=b.childExpirationTime=c-1);d.isBackwards?(f.sibling=b.child,b.child=f):(c=d.last,null!==c?c.sibling=f:b.child=f,d.last=f)}return null!==d.tail?(0===d.tailExpiration&&(d.tailExpiration=Y()+500),c=d.tail,d.rendering=c,d.tail=c.sibling,d.lastEffect=b.lastEffect,d.renderingStartTime=Y(),c.sibling=null,b=D.current,y(D,e?b&1|2:b&1),c):null}throw Error(k(156,b.tag));}function lj(a,b){switch(a.tag){case 1:return N(a.type)&&(q(G),q(B)),b=a.effectTag,b&4096?
(a.effectTag=b&-4097|64,a):null;case 3:tb();q(G);q(B);b=a.effectTag;if(0!==(b&64))throw Error(k(285));a.effectTag=b&-4097|64;return a;case 5:return te(a),null;case 13:return q(D),b=a.effectTag,b&4096?(a.effectTag=b&-4097|64,a):null;case 19:return q(D),null;case 4:return tb(),null;case 10:return me(a),null;default:return null}}function Le(a,b){return{value:a,source:b,stack:Bd(b)}}function Me(a,b){var c=b.source,d=b.stack;null===d&&null!==c&&(d=Bd(c));null!==c&&na(c.type);b=b.value;null!==a&&1===a.tag&&
na(a.type);try{console.error(b)}catch(e){setTimeout(function(){throw e;})}}function mj(a,b){try{b.props=a.memoizedProps,b.state=a.memoizedState,b.componentWillUnmount()}catch(c){Za(a,c)}}function zh(a){var b=a.ref;if(null!==b)if("function"===typeof b)try{b(null)}catch(c){Za(a,c)}else b.current=null}function nj(a,b){switch(b.tag){case 0:case 11:case 15:case 22:return;case 1:if(b.effectTag&256&&null!==a){var c=a.memoizedProps,d=a.memoizedState;a=b.stateNode;b=a.getSnapshotBeforeUpdate(b.elementType===
b.type?c:aa(b.type,c),d);a.__reactInternalSnapshotBeforeUpdate=b}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(k(163));}function Ah(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.destroy;c.destroy=void 0;void 0!==d&&d()}c=c.next}while(c!==b)}}function Bh(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.create;c.destroy=d()}c=c.next}while(c!==b)}}function oj(a,b,c,d){switch(c.tag){case 0:case 11:case 15:case 22:Bh(3,
c);return;case 1:a=c.stateNode;c.effectTag&4&&(null===b?a.componentDidMount():(d=c.elementType===c.type?b.memoizedProps:aa(c.type,b.memoizedProps),a.componentDidUpdate(d,b.memoizedState,a.__reactInternalSnapshotBeforeUpdate)));b=c.updateQueue;null!==b&&Wg(c,b,a);return;case 3:b=c.updateQueue;if(null!==b){a=null;if(null!==c.child)switch(c.child.tag){case 5:a=c.child.stateNode;break;case 1:a=c.child.stateNode}Wg(c,b,a)}return;case 5:a=c.stateNode;null===b&&c.effectTag&4&&lg(c.type,c.memoizedProps)&&
a.focus();return;case 6:return;case 4:return;case 12:return;case 13:null===c.memoizedState&&(c=c.alternate,null!==c&&(c=c.memoizedState,null!==c&&(c=c.dehydrated,null!==c&&bg(c))));return;case 19:case 17:case 20:case 21:return}throw Error(k(163));}function Ch(a,b,c){"function"===typeof Ne&&Ne(b);switch(b.tag){case 0:case 11:case 14:case 15:case 22:a=b.updateQueue;if(null!==a&&(a=a.lastEffect,null!==a)){var d=a.next;Da(97<c?97:c,function(){var a=d;do{var c=a.destroy;if(void 0!==c){var g=b;try{c()}catch(h){Za(g,
h)}}a=a.next}while(a!==d)})}break;case 1:zh(b);c=b.stateNode;"function"===typeof c.componentWillUnmount&&mj(b,c);break;case 5:zh(b);break;case 4:Dh(a,b,c)}}function Eh(a){var b=a.alternate;a.return=null;a.child=null;a.memoizedState=null;a.updateQueue=null;a.dependencies=null;a.alternate=null;a.firstEffect=null;a.lastEffect=null;a.pendingProps=null;a.memoizedProps=null;a.stateNode=null;null!==b&&Eh(b)}function Fh(a){return 5===a.tag||3===a.tag||4===a.tag}function Gh(a){a:{for(var b=a.return;null!==
b;){if(Fh(b)){var c=b;break a}b=b.return}throw Error(k(160));}b=c.stateNode;switch(c.tag){case 5:var d=!1;break;case 3:b=b.containerInfo;d=!0;break;case 4:b=b.containerInfo;d=!0;break;default:throw Error(k(161));}c.effectTag&16&&(Wb(b,""),c.effectTag&=-17);a:b:for(c=a;;){for(;null===c.sibling;){if(null===c.return||Fh(c.return)){c=null;break a}c=c.return}c.sibling.return=c.return;for(c=c.sibling;5!==c.tag&&6!==c.tag&&18!==c.tag;){if(c.effectTag&2)continue b;if(null===c.child||4===c.tag)continue b;
else c.child.return=c,c=c.child}if(!(c.effectTag&2)){c=c.stateNode;break a}}d?Oe(a,c,b):Pe(a,c,b)}function Oe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?8===c.nodeType?c.parentNode.insertBefore(a,b):c.insertBefore(a,b):(8===c.nodeType?(b=c.parentNode,b.insertBefore(a,c)):(b=c,b.appendChild(a)),c=c._reactRootContainer,null!==c&&void 0!==c||null!==b.onclick||(b.onclick=uc));else if(4!==d&&(a=a.child,null!==a))for(Oe(a,b,c),a=a.sibling;null!==a;)Oe(a,b,c),a=a.sibling}
function Pe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?c.insertBefore(a,b):c.appendChild(a);else if(4!==d&&(a=a.child,null!==a))for(Pe(a,b,c),a=a.sibling;null!==a;)Pe(a,b,c),a=a.sibling}function Dh(a,b,c){for(var d=b,e=!1,f,g;;){if(!e){e=d.return;a:for(;;){if(null===e)throw Error(k(160));f=e.stateNode;switch(e.tag){case 5:g=!1;break a;case 3:f=f.containerInfo;g=!0;break a;case 4:f=f.containerInfo;g=!0;break a}e=e.return}e=!0}if(5===d.tag||6===d.tag){a:for(var h=
a,m=d,n=c,l=m;;)if(Ch(h,l,n),null!==l.child&&4!==l.tag)l.child.return=l,l=l.child;else{if(l===m)break a;for(;null===l.sibling;){if(null===l.return||l.return===m)break a;l=l.return}l.sibling.return=l.return;l=l.sibling}g?(h=f,m=d.stateNode,8===h.nodeType?h.parentNode.removeChild(m):h.removeChild(m)):f.removeChild(d.stateNode)}else if(4===d.tag){if(null!==d.child){f=d.stateNode.containerInfo;g=!0;d.child.return=d;d=d.child;continue}}else if(Ch(a,d,c),null!==d.child){d.child.return=d;d=d.child;continue}if(d===
b)break;for(;null===d.sibling;){if(null===d.return||d.return===b)return;d=d.return;4===d.tag&&(e=!1)}d.sibling.return=d.return;d=d.sibling}}function Qe(a,b){switch(b.tag){case 0:case 11:case 14:case 15:case 22:Ah(3,b);return;case 1:return;case 5:var c=b.stateNode;if(null!=c){var d=b.memoizedProps,e=null!==a?a.memoizedProps:d;a=b.type;var f=b.updateQueue;b.updateQueue=null;if(null!==f){c[vc]=d;"input"===a&&"radio"===d.type&&null!=d.name&&If(c,d);Vd(a,e);b=Vd(a,d);for(e=0;e<f.length;e+=2){var g=f[e],
h=f[e+1];"style"===g?gg(c,h):"dangerouslySetInnerHTML"===g?xh(c,h):"children"===g?Wb(c,h):xd(c,g,h,b)}switch(a){case "input":Dd(c,d);break;case "textarea":Lf(c,d);break;case "select":b=c._wrapperState.wasMultiple,c._wrapperState.wasMultiple=!!d.multiple,a=d.value,null!=a?hb(c,!!d.multiple,a,!1):b!==!!d.multiple&&(null!=d.defaultValue?hb(c,!!d.multiple,d.defaultValue,!0):hb(c,!!d.multiple,d.multiple?[]:"",!1))}}}return;case 6:if(null===b.stateNode)throw Error(k(162));b.stateNode.nodeValue=b.memoizedProps;
return;case 3:b=b.stateNode;b.hydrate&&(b.hydrate=!1,bg(b.containerInfo));return;case 12:return;case 13:c=b;null===b.memoizedState?d=!1:(d=!0,c=b.child,Re=Y());if(null!==c)a:for(a=c;;){if(5===a.tag)f=a.stateNode,d?(f=f.style,"function"===typeof f.setProperty?f.setProperty("display","none","important"):f.display="none"):(f=a.stateNode,e=a.memoizedProps.style,e=void 0!==e&&null!==e&&e.hasOwnProperty("display")?e.display:null,f.style.display=fg("display",e));else if(6===a.tag)a.stateNode.nodeValue=d?
"":a.memoizedProps;else if(13===a.tag&&null!==a.memoizedState&&null===a.memoizedState.dehydrated){f=a.child.sibling;f.return=a;a=f;continue}else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===c)break;for(;null===a.sibling;){if(null===a.return||a.return===c)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}Hh(b);return;case 19:Hh(b);return;case 17:return}throw Error(k(163));}function Hh(a){var b=a.updateQueue;if(null!==b){a.updateQueue=null;var c=a.stateNode;null===c&&(c=a.stateNode=
new pj);b.forEach(function(b){var d=qj.bind(null,a,b);c.has(b)||(c.add(b),b.then(d,d))})}}function Ih(a,b,c){c=Ea(c,null);c.tag=3;c.payload={element:null};var d=b.value;c.callback=function(){cd||(cd=!0,Se=d);Me(a,b)};return c}function Jh(a,b,c){c=Ea(c,null);c.tag=3;var d=a.type.getDerivedStateFromError;if("function"===typeof d){var e=b.value;c.payload=function(){Me(a,b);return d(e)}}var f=a.stateNode;null!==f&&"function"===typeof f.componentDidCatch&&(c.callback=function(){"function"!==typeof d&&
(null===La?La=new Set([this]):La.add(this),Me(a,b));var c=b.stack;this.componentDidCatch(b.value,{componentStack:null!==c?c:""})});return c}function ka(){return(p&(ca|ma))!==H?1073741821-(Y()/10|0):0!==dd?dd:dd=1073741821-(Y()/10|0)}function Va(a,b,c){b=b.mode;if(0===(b&2))return 1073741823;var d=Cc();if(0===(b&4))return 99===d?1073741823:1073741822;if((p&ca)!==H)return P;if(null!==c)a=Fc(a,c.timeoutMs|0||5E3,250);else switch(d){case 99:a=1073741823;break;case 98:a=Fc(a,150,100);break;case 97:case 96:a=
Fc(a,5E3,250);break;case 95:a=2;break;default:throw Error(k(326));}null!==U&&a===P&&--a;return a}function ed(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);var d=a.return,e=null;if(null===d&&3===a.tag)e=a.stateNode;else for(;null!==d;){c=d.alternate;d.childExpirationTime<b&&(d.childExpirationTime=b);null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);if(null===d.return&&3===d.tag){e=d.stateNode;break}d=d.return}null!==e&&
(U===e&&(Kc(b),F===bd&&Ya(e,P)),yh(e,b));return e}function fd(a){var b=a.lastExpiredTime;if(0!==b)return b;b=a.firstPendingTime;if(!Kh(a,b))return b;var c=a.lastPingedTime;a=a.nextKnownPendingLevel;a=c>a?c:a;return 2>=a&&b!==a?0:a}function V(a){if(0!==a.lastExpiredTime)a.callbackExpirationTime=1073741823,a.callbackPriority=99,a.callbackNode=Og(Te.bind(null,a));else{var b=fd(a),c=a.callbackNode;if(0===b)null!==c&&(a.callbackNode=null,a.callbackExpirationTime=0,a.callbackPriority=90);else{var d=ka();
1073741823===b?d=99:1===b||2===b?d=95:(d=10*(1073741821-b)-10*(1073741821-d),d=0>=d?99:250>=d?98:5250>=d?97:95);if(null!==c){var e=a.callbackPriority;if(a.callbackExpirationTime===b&&e>=d)return;c!==Qg&&Rg(c)}a.callbackExpirationTime=b;a.callbackPriority=d;b=1073741823===b?Og(Te.bind(null,a)):Ng(d,Lh.bind(null,a),{timeout:10*(1073741821-b)-Y()});a.callbackNode=b}}}function Lh(a,b){dd=0;if(b)return b=ka(),Ue(a,b),V(a),null;var c=fd(a);if(0!==c){b=a.callbackNode;if((p&(ca|ma))!==H)throw Error(k(327));
xb();a===U&&c===P||$a(a,c);if(null!==t){var d=p;p|=ca;var e=Mh();do try{rj();break}catch(h){Nh(a,h)}while(1);le();p=d;gd.current=e;if(F===hd)throw b=id,$a(a,c),Ya(a,c),V(a),b;if(null===t)switch(e=a.finishedWork=a.current.alternate,a.finishedExpirationTime=c,d=F,U=null,d){case Xa:case hd:throw Error(k(345));case Oh:Ue(a,2<c?2:c);break;case ad:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(1073741823===ta&&(e=Re+Ph-Y(),10<e)){if(jd){var f=a.lastPingedTime;if(0===f||f>=c){a.lastPingedTime=
c;$a(a,c);break}}f=fd(a);if(0!==f&&f!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}a.timeoutHandle=We(ab.bind(null,a),e);break}ab(a);break;case bd:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(jd&&(e=a.lastPingedTime,0===e||e>=c)){a.lastPingedTime=c;$a(a,c);break}e=fd(a);if(0!==e&&e!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}1073741823!==Yb?d=10*(1073741821-Yb)-Y():1073741823===ta?d=0:(d=10*(1073741821-ta)-5E3,e=Y(),c=10*(1073741821-c)-e,d=e-d,0>d&&(d=0),d=
(120>d?120:480>d?480:1080>d?1080:1920>d?1920:3E3>d?3E3:4320>d?4320:1960*sj(d/1960))-d,c<d&&(d=c));if(10<d){a.timeoutHandle=We(ab.bind(null,a),d);break}ab(a);break;case Xe:if(1073741823!==ta&&null!==kd){f=ta;var g=kd;d=g.busyMinDurationMs|0;0>=d?d=0:(e=g.busyDelayMs|0,f=Y()-(10*(1073741821-f)-(g.timeoutMs|0||5E3)),d=f<=e?0:e+d-f);if(10<d){Ya(a,c);a.timeoutHandle=We(ab.bind(null,a),d);break}}ab(a);break;default:throw Error(k(329));}V(a);if(a.callbackNode===b)return Lh.bind(null,a)}}return null}function Te(a){var b=
a.lastExpiredTime;b=0!==b?b:1073741823;if((p&(ca|ma))!==H)throw Error(k(327));xb();a===U&&b===P||$a(a,b);if(null!==t){var c=p;p|=ca;var d=Mh();do try{tj();break}catch(e){Nh(a,e)}while(1);le();p=c;gd.current=d;if(F===hd)throw c=id,$a(a,b),Ya(a,b),V(a),c;if(null!==t)throw Error(k(261));a.finishedWork=a.current.alternate;a.finishedExpirationTime=b;U=null;ab(a);V(a)}return null}function uj(){if(null!==bb){var a=bb;bb=null;a.forEach(function(a,c){Ue(c,a);V(c)});ha()}}function Qh(a,b){var c=p;p|=1;try{return a(b)}finally{p=
c,p===H&&ha()}}function Rh(a,b){var c=p;p&=-2;p|=Ye;try{return a(b)}finally{p=c,p===H&&ha()}}function $a(a,b){a.finishedWork=null;a.finishedExpirationTime=0;var c=a.timeoutHandle;-1!==c&&(a.timeoutHandle=-1,vj(c));if(null!==t)for(c=t.return;null!==c;){var d=c;switch(d.tag){case 1:d=d.type.childContextTypes;null!==d&&void 0!==d&&(q(G),q(B));break;case 3:tb();q(G);q(B);break;case 5:te(d);break;case 4:tb();break;case 13:q(D);break;case 19:q(D);break;case 10:me(d)}c=c.return}U=a;t=Sa(a.current,null);
P=b;F=Xa;id=null;Yb=ta=1073741823;kd=null;Xb=0;jd=!1}function Nh(a,b){do{try{le();Sc.current=Tc;if(Uc)for(var c=z.memoizedState;null!==c;){var d=c.queue;null!==d&&(d.pending=null);c=c.next}Ia=0;J=K=z=null;Uc=!1;if(null===t||null===t.return)return F=hd,id=b,t=null;a:{var e=a,f=t.return,g=t,h=b;b=P;g.effectTag|=2048;g.firstEffect=g.lastEffect=null;if(null!==h&&"object"===typeof h&&"function"===typeof h.then){var m=h;if(0===(g.mode&2)){var n=g.alternate;n?(g.updateQueue=n.updateQueue,g.memoizedState=
n.memoizedState,g.expirationTime=n.expirationTime):(g.updateQueue=null,g.memoizedState=null)}var l=0!==(D.current&1),k=f;do{var p;if(p=13===k.tag){var q=k.memoizedState;if(null!==q)p=null!==q.dehydrated?!0:!1;else{var w=k.memoizedProps;p=void 0===w.fallback?!1:!0!==w.unstable_avoidThisFallback?!0:l?!1:!0}}if(p){var y=k.updateQueue;if(null===y){var r=new Set;r.add(m);k.updateQueue=r}else y.add(m);if(0===(k.mode&2)){k.effectTag|=64;g.effectTag&=-2981;if(1===g.tag)if(null===g.alternate)g.tag=17;else{var O=
Ea(1073741823,null);O.tag=Jc;Fa(g,O)}g.expirationTime=1073741823;break a}h=void 0;g=b;var v=e.pingCache;null===v?(v=e.pingCache=new wj,h=new Set,v.set(m,h)):(h=v.get(m),void 0===h&&(h=new Set,v.set(m,h)));if(!h.has(g)){h.add(g);var x=xj.bind(null,e,m,g);m.then(x,x)}k.effectTag|=4096;k.expirationTime=b;break a}k=k.return}while(null!==k);h=Error((na(g.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+
Bd(g))}F!==Xe&&(F=Oh);h=Le(h,g);k=f;do{switch(k.tag){case 3:m=h;k.effectTag|=4096;k.expirationTime=b;var A=Ih(k,m,b);Ug(k,A);break a;case 1:m=h;var u=k.type,B=k.stateNode;if(0===(k.effectTag&64)&&("function"===typeof u.getDerivedStateFromError||null!==B&&"function"===typeof B.componentDidCatch&&(null===La||!La.has(B)))){k.effectTag|=4096;k.expirationTime=b;var H=Jh(k,m,b);Ug(k,H);break a}}k=k.return}while(null!==k)}t=Sh(t)}catch(cj){b=cj;continue}break}while(1)}function Mh(a){a=gd.current;gd.current=
Tc;return null===a?Tc:a}function Vg(a,b){a<ta&&2<a&&(ta=a);null!==b&&a<Yb&&2<a&&(Yb=a,kd=b)}function Kc(a){a>Xb&&(Xb=a)}function tj(){for(;null!==t;)t=Th(t)}function rj(){for(;null!==t&&!yj();)t=Th(t)}function Th(a){var b=zj(a.alternate,a,P);a.memoizedProps=a.pendingProps;null===b&&(b=Sh(a));Uh.current=null;return b}function Sh(a){t=a;do{var b=t.alternate;a=t.return;if(0===(t.effectTag&2048)){b=hj(b,t,P);if(1===P||1!==t.childExpirationTime){for(var c=0,d=t.child;null!==d;){var e=d.expirationTime,
f=d.childExpirationTime;e>c&&(c=e);f>c&&(c=f);d=d.sibling}t.childExpirationTime=c}if(null!==b)return b;null!==a&&0===(a.effectTag&2048)&&(null===a.firstEffect&&(a.firstEffect=t.firstEffect),null!==t.lastEffect&&(null!==a.lastEffect&&(a.lastEffect.nextEffect=t.firstEffect),a.lastEffect=t.lastEffect),1<t.effectTag&&(null!==a.lastEffect?a.lastEffect.nextEffect=t:a.firstEffect=t,a.lastEffect=t))}else{b=lj(t);if(null!==b)return b.effectTag&=2047,b;null!==a&&(a.firstEffect=a.lastEffect=null,a.effectTag|=
2048)}b=t.sibling;if(null!==b)return b;t=a}while(null!==t);F===Xa&&(F=Xe);return null}function Ve(a){var b=a.expirationTime;a=a.childExpirationTime;return b>a?b:a}function ab(a){var b=Cc();Da(99,Aj.bind(null,a,b));return null}function Aj(a,b){do xb();while(null!==Zb);if((p&(ca|ma))!==H)throw Error(k(327));var c=a.finishedWork,d=a.finishedExpirationTime;if(null===c)return null;a.finishedWork=null;a.finishedExpirationTime=0;if(c===a.current)throw Error(k(177));a.callbackNode=null;a.callbackExpirationTime=
0;a.callbackPriority=90;a.nextKnownPendingLevel=0;var e=Ve(c);a.firstPendingTime=e;d<=a.lastSuspendedTime?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:d<=a.firstSuspendedTime&&(a.firstSuspendedTime=d-1);d<=a.lastPingedTime&&(a.lastPingedTime=0);d<=a.lastExpiredTime&&(a.lastExpiredTime=0);a===U&&(t=U=null,P=0);1<c.effectTag?null!==c.lastEffect?(c.lastEffect.nextEffect=c,e=c.firstEffect):e=c:e=c.firstEffect;if(null!==e){var f=p;p|=ma;Uh.current=null;Ze=tc;var g=kg();if(Xd(g)){if("selectionStart"in
g)var h={start:g.selectionStart,end:g.selectionEnd};else a:{h=(h=g.ownerDocument)&&h.defaultView||window;var m=h.getSelection&&h.getSelection();if(m&&0!==m.rangeCount){h=m.anchorNode;var n=m.anchorOffset,q=m.focusNode;m=m.focusOffset;try{h.nodeType,q.nodeType}catch(sb){h=null;break a}var ba=0,w=-1,y=-1,B=0,D=0,r=g,z=null;b:for(;;){for(var v;;){r!==h||0!==n&&3!==r.nodeType||(w=ba+n);r!==q||0!==m&&3!==r.nodeType||(y=ba+m);3===r.nodeType&&(ba+=r.nodeValue.length);if(null===(v=r.firstChild))break;z=r;
r=v}for(;;){if(r===g)break b;z===h&&++B===n&&(w=ba);z===q&&++D===m&&(y=ba);if(null!==(v=r.nextSibling))break;r=z;z=r.parentNode}r=v}h=-1===w||-1===y?null:{start:w,end:y}}else h=null}h=h||{start:0,end:0}}else h=null;$e={activeElementDetached:null,focusedElem:g,selectionRange:h};tc=!1;l=e;do try{Bj()}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=e;do try{for(g=a,h=b;null!==l;){var x=l.effectTag;x&16&&Wb(l.stateNode,"");if(x&128){var A=l.alternate;if(null!==A){var u=
A.ref;null!==u&&("function"===typeof u?u(null):u.current=null)}}switch(x&1038){case 2:Gh(l);l.effectTag&=-3;break;case 6:Gh(l);l.effectTag&=-3;Qe(l.alternate,l);break;case 1024:l.effectTag&=-1025;break;case 1028:l.effectTag&=-1025;Qe(l.alternate,l);break;case 4:Qe(l.alternate,l);break;case 8:n=l,Dh(g,n,h),Eh(n)}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);u=$e;A=kg();x=u.focusedElem;h=u.selectionRange;if(A!==x&&x&&x.ownerDocument&&jg(x.ownerDocument.documentElement,
x)){null!==h&&Xd(x)&&(A=h.start,u=h.end,void 0===u&&(u=A),"selectionStart"in x?(x.selectionStart=A,x.selectionEnd=Math.min(u,x.value.length)):(u=(A=x.ownerDocument||document)&&A.defaultView||window,u.getSelection&&(u=u.getSelection(),n=x.textContent.length,g=Math.min(h.start,n),h=void 0===h.end?g:Math.min(h.end,n),!u.extend&&g>h&&(n=h,h=g,g=n),n=ig(x,g),q=ig(x,h),n&&q&&(1!==u.rangeCount||u.anchorNode!==n.node||u.anchorOffset!==n.offset||u.focusNode!==q.node||u.focusOffset!==q.offset)&&(A=A.createRange(),
A.setStart(n.node,n.offset),u.removeAllRanges(),g>h?(u.addRange(A),u.extend(q.node,q.offset)):(A.setEnd(q.node,q.offset),u.addRange(A))))));A=[];for(u=x;u=u.parentNode;)1===u.nodeType&&A.push({element:u,left:u.scrollLeft,top:u.scrollTop});"function"===typeof x.focus&&x.focus();for(x=0;x<A.length;x++)u=A[x],u.element.scrollLeft=u.left,u.element.scrollTop=u.top}tc=!!Ze;$e=Ze=null;a.current=c;l=e;do try{for(x=a;null!==l;){var F=l.effectTag;F&36&&oj(x,l.alternate,l);if(F&128){A=void 0;var E=l.ref;if(null!==
E){var G=l.stateNode;switch(l.tag){case 5:A=G;break;default:A=G}"function"===typeof E?E(A):E.current=A}}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=null;Cj();p=f}else a.current=c;if(ld)ld=!1,Zb=a,$b=b;else for(l=e;null!==l;)b=l.nextEffect,l.nextEffect=null,l=b;b=a.firstPendingTime;0===b&&(La=null);1073741823===b?a===af?ac++:(ac=0,af=a):ac=0;"function"===typeof bf&&bf(c.stateNode,d);V(a);if(cd)throw cd=!1,a=Se,Se=null,a;if((p&Ye)!==H)return null;
ha();return null}function Bj(){for(;null!==l;){var a=l.effectTag;0!==(a&256)&&nj(l.alternate,l);0===(a&512)||ld||(ld=!0,Ng(97,function(){xb();return null}));l=l.nextEffect}}function xb(){if(90!==$b){var a=97<$b?97:$b;$b=90;return Da(a,Dj)}}function Dj(){if(null===Zb)return!1;var a=Zb;Zb=null;if((p&(ca|ma))!==H)throw Error(k(331));var b=p;p|=ma;for(a=a.current.firstEffect;null!==a;){try{var c=a;if(0!==(c.effectTag&512))switch(c.tag){case 0:case 11:case 15:case 22:Ah(5,c),Bh(5,c)}}catch(d){if(null===
a)throw Error(k(330));Za(a,d)}c=a.nextEffect;a.nextEffect=null;a=c}p=b;ha();return!0}function Vh(a,b,c){b=Le(c,b);b=Ih(a,b,1073741823);Fa(a,b);a=ed(a,1073741823);null!==a&&V(a)}function Za(a,b){if(3===a.tag)Vh(a,a,b);else for(var c=a.return;null!==c;){if(3===c.tag){Vh(c,a,b);break}else if(1===c.tag){var d=c.stateNode;if("function"===typeof c.type.getDerivedStateFromError||"function"===typeof d.componentDidCatch&&(null===La||!La.has(d))){a=Le(b,a);a=Jh(c,a,1073741823);Fa(c,a);c=ed(c,1073741823);null!==
c&&V(c);break}}c=c.return}}function xj(a,b,c){var d=a.pingCache;null!==d&&d.delete(b);U===a&&P===c?F===bd||F===ad&&1073741823===ta&&Y()-Re<Ph?$a(a,P):jd=!0:Kh(a,c)&&(b=a.lastPingedTime,0!==b&&b<c||(a.lastPingedTime=c,V(a)))}function qj(a,b){var c=a.stateNode;null!==c&&c.delete(b);b=0;0===b&&(b=ka(),b=Va(b,a,null));a=ed(a,b);null!==a&&V(a)}function Ej(a){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var b=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(b.isDisabled||!b.supportsFiber)return!0;try{var c=
b.inject(a);bf=function(a,e){try{b.onCommitFiberRoot(c,a,void 0,64===(a.current.effectTag&64))}catch(f){}};Ne=function(a){try{b.onCommitFiberUnmount(c,a)}catch(e){}}}catch(d){}return!0}function Fj(a,b,c,d){this.tag=a;this.key=c;this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null;this.index=0;this.ref=null;this.pendingProps=b;this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null;this.mode=d;this.effectTag=0;this.lastEffect=this.firstEffect=this.nextEffect=
null;this.childExpirationTime=this.expirationTime=0;this.alternate=null}function Ge(a){a=a.prototype;return!(!a||!a.isReactComponent)}function Gj(a){if("function"===typeof a)return Ge(a)?1:0;if(void 0!==a&&null!==a){a=a.$$typeof;if(a===zd)return 11;if(a===Ad)return 14}return 2}function Sa(a,b){var c=a.alternate;null===c?(c=la(a.tag,b,a.key,a.mode),c.elementType=a.elementType,c.type=a.type,c.stateNode=a.stateNode,c.alternate=a,a.alternate=c):(c.pendingProps=b,c.effectTag=0,c.nextEffect=null,c.firstEffect=
null,c.lastEffect=null);c.childExpirationTime=a.childExpirationTime;c.expirationTime=a.expirationTime;c.child=a.child;c.memoizedProps=a.memoizedProps;c.memoizedState=a.memoizedState;c.updateQueue=a.updateQueue;b=a.dependencies;c.dependencies=null===b?null:{expirationTime:b.expirationTime,firstContext:b.firstContext,responders:b.responders};c.sibling=a.sibling;c.index=a.index;c.ref=a.ref;return c}function Oc(a,b,c,d,e,f){var g=2;d=a;if("function"===typeof a)Ge(a)&&(g=1);else if("string"===typeof a)g=
5;else a:switch(a){case Ma:return Ha(c.children,e,f,b);case Hj:g=8;e|=7;break;case Af:g=8;e|=1;break;case kc:return a=la(12,c,b,e|8),a.elementType=kc,a.type=kc,a.expirationTime=f,a;case lc:return a=la(13,c,b,e),a.type=lc,a.elementType=lc,a.expirationTime=f,a;case yd:return a=la(19,c,b,e),a.elementType=yd,a.expirationTime=f,a;default:if("object"===typeof a&&null!==a)switch(a.$$typeof){case Cf:g=10;break a;case Bf:g=9;break a;case zd:g=11;break a;case Ad:g=14;break a;case Ef:g=16;d=null;break a;case Df:g=
22;break a}throw Error(k(130,null==a?a:typeof a,""));}b=la(g,c,b,e);b.elementType=a;b.type=d;b.expirationTime=f;return b}function Ha(a,b,c,d){a=la(7,a,d,b);a.expirationTime=c;return a}function qe(a,b,c){a=la(6,a,null,b);a.expirationTime=c;return a}function re(a,b,c){b=la(4,null!==a.children?a.children:[],a.key,b);b.expirationTime=c;b.stateNode={containerInfo:a.containerInfo,pendingChildren:null,implementation:a.implementation};return b}function Ij(a,b,c){this.tag=b;this.current=null;this.containerInfo=
a;this.pingCache=this.pendingChildren=null;this.finishedExpirationTime=0;this.finishedWork=null;this.timeoutHandle=-1;this.pendingContext=this.context=null;this.hydrate=c;this.callbackNode=null;this.callbackPriority=90;this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Kh(a,b){var c=a.firstSuspendedTime;a=a.lastSuspendedTime;return 0!==c&&c>=b&&a<=b}function Ya(a,b){var c=a.firstSuspendedTime,d=a.lastSuspendedTime;
c<b&&(a.firstSuspendedTime=b);if(d>b||0===c)a.lastSuspendedTime=b;b<=a.lastPingedTime&&(a.lastPingedTime=0);b<=a.lastExpiredTime&&(a.lastExpiredTime=0)}function yh(a,b){b>a.firstPendingTime&&(a.firstPendingTime=b);var c=a.firstSuspendedTime;0!==c&&(b>=c?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:b>=a.lastSuspendedTime&&(a.lastSuspendedTime=b+1),b>a.nextKnownPendingLevel&&(a.nextKnownPendingLevel=b))}function Ue(a,b){var c=a.lastExpiredTime;if(0===c||c>b)a.lastExpiredTime=b}
function md(a,b,c,d){var e=b.current,f=ka(),g=Vb.suspense;f=Va(f,e,g);a:if(c){c=c._reactInternalFiber;b:{if(Na(c)!==c||1!==c.tag)throw Error(k(170));var h=c;do{switch(h.tag){case 3:h=h.stateNode.context;break b;case 1:if(N(h.type)){h=h.stateNode.__reactInternalMemoizedMergedChildContext;break b}}h=h.return}while(null!==h);throw Error(k(171));}if(1===c.tag){var m=c.type;if(N(m)){c=Gg(c,m,h);break a}}c=h}else c=Ca;null===b.context?b.context=c:b.pendingContext=c;b=Ea(f,g);b.payload={element:a};d=void 0===
d?null:d;null!==d&&(b.callback=d);Fa(e,b);Ja(e,f);return f}function cf(a){a=a.current;if(!a.child)return null;switch(a.child.tag){case 5:return a.child.stateNode;default:return a.child.stateNode}}function Wh(a,b){a=a.memoizedState;null!==a&&null!==a.dehydrated&&a.retryTime<b&&(a.retryTime=b)}function df(a,b){Wh(a,b);(a=a.alternate)&&Wh(a,b)}function ef(a,b,c){c=null!=c&&!0===c.hydrate;var d=new Ij(a,b,c),e=la(3,null,null,2===b?7:1===b?3:0);d.current=e;e.stateNode=d;ne(e);a[Lb]=d.current;c&&0!==b&&
xi(a,9===a.nodeType?a:a.ownerDocument);this._internalRoot=d}function bc(a){return!(!a||1!==a.nodeType&&9!==a.nodeType&&11!==a.nodeType&&(8!==a.nodeType||" react-mount-point-unstable "!==a.nodeValue))}function Jj(a,b){b||(b=a?9===a.nodeType?a.documentElement:a.firstChild:null,b=!(!b||1!==b.nodeType||!b.hasAttribute("data-reactroot")));if(!b)for(var c;c=a.lastChild;)a.removeChild(c);return new ef(a,0,b?{hydrate:!0}:void 0)}function nd(a,b,c,d,e){var f=c._reactRootContainer;if(f){var g=f._internalRoot;
if("function"===typeof e){var h=e;e=function(){var a=cf(g);h.call(a)}}md(b,g,a,e)}else{f=c._reactRootContainer=Jj(c,d);g=f._internalRoot;if("function"===typeof e){var m=e;e=function(){var a=cf(g);m.call(a)}}Rh(function(){md(b,g,a,e)})}return cf(g)}function Kj(a,b,c){var d=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:gb,key:null==d?null:""+d,children:a,containerInfo:b,implementation:c}}function Xh(a,b){var c=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;
if(!bc(b))throw Error(k(200));return Kj(a,b,null,c)}if(!ea)throw Error(k(227));var ki=function(a,b,c,d,e,f,g,h,m){var n=Array.prototype.slice.call(arguments,3);try{b.apply(c,n)}catch(C){this.onError(C)}},yb=!1,gc=null,hc=!1,pd=null,li={onError:function(a){yb=!0;gc=a}},td=null,rf=null,mf=null,ic=null,cb={},jc=[],qd={},db={},rd={},wa=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),M=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.assign,
sd=null,eb=null,fb=null,ee=function(a,b){return a(b)},eg=function(a,b,c,d,e){return a(b,c,d,e)},vd=function(){},vf=ee,Oa=!1,wd=!1,Z=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.Scheduler,Lj=Z.unstable_cancelCallback,ff=Z.unstable_now,$f=Z.unstable_scheduleCallback,Mj=Z.unstable_shouldYield,Yh=Z.unstable_requestPaint,Pd=Z.unstable_runWithPriority,Nj=Z.unstable_getCurrentPriorityLevel,Oj=Z.unstable_ImmediatePriority,Zh=Z.unstable_UserBlockingPriority,ag=Z.unstable_NormalPriority,Pj=Z.unstable_LowPriority,
Qj=Z.unstable_IdlePriority,oi=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,wf=Object.prototype.hasOwnProperty,yf={},xf={},E={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(a){E[a]=
new L(a,0,!1,a,null,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(a){var b=a[0];E[b]=new L(b,1,!1,a[1],null,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(a){E[a]=new L(a,2,!1,a.toLowerCase(),null,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(a){E[a]=new L(a,2,!1,a,null,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(a){E[a]=
new L(a,3,!1,a.toLowerCase(),null,!1)});["checked","multiple","muted","selected"].forEach(function(a){E[a]=new L(a,3,!0,a,null,!1)});["capture","download"].forEach(function(a){E[a]=new L(a,4,!1,a,null,!1)});["cols","rows","size","span"].forEach(function(a){E[a]=new L(a,6,!1,a,null,!1)});["rowSpan","start"].forEach(function(a){E[a]=new L(a,5,!1,a.toLowerCase(),null,!1)});var gf=/[\-:]([a-z])/g,hf=function(a){return a[1].toUpperCase()};"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(a){var b=
a.replace(gf,hf);E[b]=new L(b,1,!1,a,null,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/1999/xlink",!1)});["xml:base","xml:lang","xml:space"].forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/XML/1998/namespace",!1)});["tabIndex","crossOrigin"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!1)});E.xlinkHref=new L("xlinkHref",1,
!1,"xlink:href","http://www.w3.org/1999/xlink",!0);["src","href","action","formAction"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!0)});var da=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;da.hasOwnProperty("ReactCurrentDispatcher")||(da.ReactCurrentDispatcher={current:null});da.hasOwnProperty("ReactCurrentBatchConfig")||(da.ReactCurrentBatchConfig={suspense:null});var si=/^(.*)[\\\/]/,Q="function"===typeof Symbol&&Symbol.for,Pc=Q?Symbol.for("react.element"):60103,gb=Q?Symbol.for("react.portal"):
60106,Ma=Q?Symbol.for("react.fragment"):60107,Af=Q?Symbol.for("react.strict_mode"):60108,kc=Q?Symbol.for("react.profiler"):60114,Cf=Q?Symbol.for("react.provider"):60109,Bf=Q?Symbol.for("react.context"):60110,Hj=Q?Symbol.for("react.concurrent_mode"):60111,zd=Q?Symbol.for("react.forward_ref"):60112,lc=Q?Symbol.for("react.suspense"):60113,yd=Q?Symbol.for("react.suspense_list"):60120,Ad=Q?Symbol.for("react.memo"):60115,Ef=Q?Symbol.for("react.lazy"):60116,Df=Q?Symbol.for("react.block"):60121,zf="function"===
typeof Symbol&&Symbol.iterator,od,xh=function(a){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(b,c,d,e){MSApp.execUnsafeLocalFunction(function(){return a(b,c,d,e)})}:a}(function(a,b){if("http://www.w3.org/2000/svg"!==a.namespaceURI||"innerHTML"in a)a.innerHTML=b;else{od=od||document.createElement("div");od.innerHTML="<svg>"+b.valueOf().toString()+"</svg>";for(b=od.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;b.firstChild;)a.appendChild(b.firstChild)}}),Wb=function(a,
b){if(b){var c=a.firstChild;if(c&&c===a.lastChild&&3===c.nodeType){c.nodeValue=b;return}}a.textContent=b},ib={animationend:nc("Animation","AnimationEnd"),animationiteration:nc("Animation","AnimationIteration"),animationstart:nc("Animation","AnimationStart"),transitionend:nc("Transition","TransitionEnd")},Id={},Of={};wa&&(Of=document.createElement("div").style,"AnimationEvent"in window||(delete ib.animationend.animation,delete ib.animationiteration.animation,delete ib.animationstart.animation),"TransitionEvent"in
window||delete ib.transitionend.transition);var $h=oc("animationend"),ai=oc("animationiteration"),bi=oc("animationstart"),ci=oc("transitionend"),Db="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Pf=new ("function"===typeof WeakMap?WeakMap:Map),Ab=null,wi=function(a){if(a){var b=a._dispatchListeners,c=a._dispatchInstances;
if(Array.isArray(b))for(var d=0;d<b.length&&!a.isPropagationStopped();d++)lf(a,b[d],c[d]);else b&&lf(a,b,c);a._dispatchListeners=null;a._dispatchInstances=null;a.isPersistent()||a.constructor.release(a)}},qc=[],Rd=!1,fa=[],xa=null,ya=null,za=null,Eb=new Map,Fb=new Map,Jb=[],Nd="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),
yi="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" "),dg={},cg=new Map,Td=new Map,Rj=["abort","abort",$h,"animationEnd",ai,"animationIteration",bi,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata",
"loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",ci,"transitionEnd","waiting","waiting"];Sd("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),
0);Sd("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1);Sd(Rj,2);(function(a,b){for(var c=0;c<a.length;c++)Td.set(a[c],b)})("change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),0);var Hi=Zh,Gi=Pd,tc=!0,Kb={animationIterationCount:!0,
borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,
strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},Sj=["Webkit","ms","Moz","O"];Object.keys(Kb).forEach(function(a){Sj.forEach(function(b){b=b+a.charAt(0).toUpperCase()+a.substring(1);Kb[b]=Kb[a]})});var Ii=M({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0}),ng="$",og="/$",$d="$?",Zd="$!",Ze=null,$e=null,We="function"===typeof setTimeout?setTimeout:void 0,vj="function"===
typeof clearTimeout?clearTimeout:void 0,jf=Math.random().toString(36).slice(2),Aa="__reactInternalInstance$"+jf,vc="__reactEventHandlers$"+jf,Lb="__reactContainere$"+jf,Ba=null,ce=null,wc=null;M(R.prototype,{preventDefault:function(){this.defaultPrevented=!0;var a=this.nativeEvent;a&&(a.preventDefault?a.preventDefault():"unknown"!==typeof a.returnValue&&(a.returnValue=!1),this.isDefaultPrevented=xc)},stopPropagation:function(){var a=this.nativeEvent;a&&(a.stopPropagation?a.stopPropagation():"unknown"!==
typeof a.cancelBubble&&(a.cancelBubble=!0),this.isPropagationStopped=xc)},persist:function(){this.isPersistent=xc},isPersistent:yc,destructor:function(){var a=this.constructor.Interface,b;for(b in a)this[b]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null;this.isPropagationStopped=this.isDefaultPrevented=yc;this._dispatchInstances=this._dispatchListeners=null}});R.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(a){return a.timeStamp||
Date.now()},defaultPrevented:null,isTrusted:null};R.extend=function(a){function b(){return c.apply(this,arguments)}var c=this,d=function(){};d.prototype=c.prototype;d=new d;M(d,b.prototype);b.prototype=d;b.prototype.constructor=b;b.Interface=M({},c.Interface,a);b.extend=c.extend;sg(b);return b};sg(R);var Tj=R.extend({data:null}),Uj=R.extend({data:null}),Ni=[9,13,27,32],de=wa&&"CompositionEvent"in window,cc=null;wa&&"documentMode"in document&&(cc=document.documentMode);var Vj=wa&&"TextEvent"in window&&
!cc,xg=wa&&(!de||cc&&8<cc&&11>=cc),wg=String.fromCharCode(32),ua={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},
dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},vg=!1,mb=!1,Wj={eventTypes:ua,extractEvents:function(a,b,c,d,e){var f;if(de)b:{switch(a){case "compositionstart":var g=ua.compositionStart;break b;case "compositionend":g=ua.compositionEnd;break b;case "compositionupdate":g=
ua.compositionUpdate;break b}g=void 0}else mb?tg(a,c)&&(g=ua.compositionEnd):"keydown"===a&&229===c.keyCode&&(g=ua.compositionStart);g?(xg&&"ko"!==c.locale&&(mb||g!==ua.compositionStart?g===ua.compositionEnd&&mb&&(f=rg()):(Ba=d,ce="value"in Ba?Ba.value:Ba.textContent,mb=!0)),e=Tj.getPooled(g,b,c,d),f?e.data=f:(f=ug(c),null!==f&&(e.data=f)),lb(e),f=e):f=null;(a=Vj?Oi(a,c):Pi(a,c))?(b=Uj.getPooled(ua.beforeInput,b,c,d),b.data=a,lb(b)):b=null;return null===f?b:null===b?f:[f,b]}},Qi={color:!0,date:!0,
datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0},Ag={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}},Mb=null,Nb=null,kf=!1;wa&&(kf=Tf("input")&&(!document.documentMode||9<document.documentMode));var Xj={eventTypes:Ag,_isInputEventSupported:kf,extractEvents:function(a,b,c,d,e){e=b?Pa(b):window;var f=
e.nodeName&&e.nodeName.toLowerCase();if("select"===f||"input"===f&&"file"===e.type)var g=Si;else if(yg(e))if(kf)g=Wi;else{g=Ui;var h=Ti}else(f=e.nodeName)&&"input"===f.toLowerCase()&&("checkbox"===e.type||"radio"===e.type)&&(g=Vi);if(g&&(g=g(a,b)))return zg(g,c,d);h&&h(a,e,b);"blur"===a&&(a=e._wrapperState)&&a.controlled&&"number"===e.type&&Ed(e,"number",e.value)}},dc=R.extend({view:null,detail:null}),Yi={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"},di=0,ei=0,fi=!1,gi=!1,ec=dc.extend({screenX:null,
screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:fe,button:null,buttons:null,relatedTarget:function(a){return a.relatedTarget||(a.fromElement===a.srcElement?a.toElement:a.fromElement)},movementX:function(a){if("movementX"in a)return a.movementX;var b=di;di=a.screenX;return fi?"mousemove"===a.type?a.screenX-b:0:(fi=!0,0)},movementY:function(a){if("movementY"in a)return a.movementY;var b=ei;ei=a.screenY;return gi?"mousemove"===
a.type?a.screenY-b:0:(gi=!0,0)}}),hi=ec.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),fc={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout",
"pointerover"]}},Yj={eventTypes:fc,extractEvents:function(a,b,c,d,e){var f="mouseover"===a||"pointerover"===a,g="mouseout"===a||"pointerout"===a;if(f&&0===(e&32)&&(c.relatedTarget||c.fromElement)||!g&&!f)return null;f=d.window===d?d:(f=d.ownerDocument)?f.defaultView||f.parentWindow:window;if(g){if(g=b,b=(b=c.relatedTarget||c.toElement)?Bb(b):null,null!==b){var h=Na(b);if(b!==h||5!==b.tag&&6!==b.tag)b=null}}else g=null;if(g===b)return null;if("mouseout"===a||"mouseover"===a){var m=ec;var n=fc.mouseLeave;
var l=fc.mouseEnter;var k="mouse"}else if("pointerout"===a||"pointerover"===a)m=hi,n=fc.pointerLeave,l=fc.pointerEnter,k="pointer";a=null==g?f:Pa(g);f=null==b?f:Pa(b);n=m.getPooled(n,g,c,d);n.type=k+"leave";n.target=a;n.relatedTarget=f;c=m.getPooled(l,b,c,d);c.type=k+"enter";c.target=f;c.relatedTarget=a;d=g;k=b;if(d&&k)a:{m=d;l=k;g=0;for(a=m;a;a=pa(a))g++;a=0;for(b=l;b;b=pa(b))a++;for(;0<g-a;)m=pa(m),g--;for(;0<a-g;)l=pa(l),a--;for(;g--;){if(m===l||m===l.alternate)break a;m=pa(m);l=pa(l)}m=null}else m=
null;l=m;for(m=[];d&&d!==l;){g=d.alternate;if(null!==g&&g===l)break;m.push(d);d=pa(d)}for(d=[];k&&k!==l;){g=k.alternate;if(null!==g&&g===l)break;d.push(k);k=pa(k)}for(k=0;k<m.length;k++)be(m[k],"bubbled",n);for(k=d.length;0<k--;)be(d[k],"captured",c);return 0===(e&64)?[n]:[n,c]}},Qa="function"===typeof Object.is?Object.is:Zi,$i=Object.prototype.hasOwnProperty,Zj=wa&&"documentMode"in document&&11>=document.documentMode,Eg={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},
dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},nb=null,he=null,Pb=null,ge=!1,ak={eventTypes:Eg,extractEvents:function(a,b,c,d,e,f){e=f||(d.window===d?d.document:9===d.nodeType?d:d.ownerDocument);if(!(f=!e)){a:{e=Jd(e);f=rd.onSelect;for(var g=0;g<f.length;g++)if(!e.has(f[g])){e=!1;break a}e=!0}f=!e}if(f)return null;e=b?Pa(b):window;switch(a){case "focus":if(yg(e)||"true"===e.contentEditable)nb=e,he=b,Pb=null;break;case "blur":Pb=he=nb=null;
break;case "mousedown":ge=!0;break;case "contextmenu":case "mouseup":case "dragend":return ge=!1,Dg(c,d);case "selectionchange":if(Zj)break;case "keydown":case "keyup":return Dg(c,d)}return null}},bk=R.extend({animationName:null,elapsedTime:null,pseudoElement:null}),ck=R.extend({clipboardData:function(a){return"clipboardData"in a?a.clipboardData:window.clipboardData}}),dk=dc.extend({relatedTarget:null}),ek={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",
Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},fk={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",
224:"Meta"},gk=dc.extend({key:function(a){if(a.key){var b=ek[a.key]||a.key;if("Unidentified"!==b)return b}return"keypress"===a.type?(a=Ac(a),13===a?"Enter":String.fromCharCode(a)):"keydown"===a.type||"keyup"===a.type?fk[a.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:fe,charCode:function(a){return"keypress"===a.type?Ac(a):0},keyCode:function(a){return"keydown"===a.type||"keyup"===a.type?a.keyCode:0},which:function(a){return"keypress"===
a.type?Ac(a):"keydown"===a.type||"keyup"===a.type?a.keyCode:0}}),hk=ec.extend({dataTransfer:null}),ik=dc.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:fe}),jk=R.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),kk=ec.extend({deltaX:function(a){return"deltaX"in a?a.deltaX:"wheelDeltaX"in a?-a.wheelDeltaX:0},deltaY:function(a){return"deltaY"in a?a.deltaY:"wheelDeltaY"in a?-a.wheelDeltaY:"wheelDelta"in a?
-a.wheelDelta:0},deltaZ:null,deltaMode:null}),lk={eventTypes:dg,extractEvents:function(a,b,c,d,e){e=cg.get(a);if(!e)return null;switch(a){case "keypress":if(0===Ac(c))return null;case "keydown":case "keyup":a=gk;break;case "blur":case "focus":a=dk;break;case "click":if(2===c.button)return null;case "auxclick":case "dblclick":case "mousedown":case "mousemove":case "mouseup":case "mouseout":case "mouseover":case "contextmenu":a=ec;break;case "drag":case "dragend":case "dragenter":case "dragexit":case "dragleave":case "dragover":case "dragstart":case "drop":a=
hk;break;case "touchcancel":case "touchend":case "touchmove":case "touchstart":a=ik;break;case $h:case ai:case bi:a=bk;break;case ci:a=jk;break;case "scroll":a=dc;break;case "wheel":a=kk;break;case "copy":case "cut":case "paste":a=ck;break;case "gotpointercapture":case "lostpointercapture":case "pointercancel":case "pointerdown":case "pointermove":case "pointerout":case "pointerover":case "pointerup":a=hi;break;default:a=R}b=a.getPooled(e,b,c,d);lb(b);return b}};(function(a){if(ic)throw Error(k(101));
ic=Array.prototype.slice.call(a);nf()})("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" "));(function(a,b,c){td=a;rf=b;mf=c})(ae,Hb,Pa);pf({SimpleEventPlugin:lk,EnterLeaveEventPlugin:Yj,ChangeEventPlugin:Xj,SelectEventPlugin:ak,BeforeInputEventPlugin:Wj});var ie=[],ob=-1,Ca={},B={current:Ca},G={current:!1},Ra=Ca,bj=Pd,je=$f,Rg=Lj,aj=Nj,Dc=Oj,Ig=Zh,Jg=ag,Kg=Pj,Lg=Qj,Qg={},yj=Mj,Cj=void 0!==Yh?Yh:function(){},qa=null,
Ec=null,ke=!1,ii=ff(),Y=1E4>ii?ff:function(){return ff()-ii},Ic={current:null},Hc=null,qb=null,Gc=null,Tg=0,Jc=2,Ga=!1,Vb=da.ReactCurrentBatchConfig,$g=(new ea.Component).refs,Mc={isMounted:function(a){return(a=a._reactInternalFiber)?Na(a)===a:!1},enqueueSetState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;d=Va(d,a,e);e=Ea(d,e);e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueReplaceState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;
d=Va(d,a,e);e=Ea(d,e);e.tag=1;e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueForceUpdate:function(a,b){a=a._reactInternalFiber;var c=ka(),d=Vb.suspense;c=Va(c,a,d);d=Ea(c,d);d.tag=Jc;void 0!==b&&null!==b&&(d.callback=b);Fa(a,d);Ja(a,c)}},Qc=Array.isArray,wb=ah(!0),Fe=ah(!1),Sb={},ja={current:Sb},Ub={current:Sb},Tb={current:Sb},D={current:0},Sc=da.ReactCurrentDispatcher,X=da.ReactCurrentBatchConfig,Ia=0,z=null,K=null,J=null,Uc=!1,Tc={readContext:W,useCallback:S,useContext:S,
useEffect:S,useImperativeHandle:S,useLayoutEffect:S,useMemo:S,useReducer:S,useRef:S,useState:S,useDebugValue:S,useResponder:S,useDeferredValue:S,useTransition:S},dj={readContext:W,useCallback:ih,useContext:W,useEffect:eh,useImperativeHandle:function(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;return ze(4,2,gh.bind(null,b,a),c)},useLayoutEffect:function(a,b){return ze(4,2,a,b)},useMemo:function(a,b){var c=ub();b=void 0===b?null:b;a=a();c.memoizedState=[a,b];return a},useReducer:function(a,b,c){var d=
ub();b=void 0!==c?c(b):b;d.memoizedState=d.baseState=b;a=d.queue={pending:null,dispatch:null,lastRenderedReducer:a,lastRenderedState:b};a=a.dispatch=ch.bind(null,z,a);return[d.memoizedState,a]},useRef:function(a){var b=ub();a={current:a};return b.memoizedState=a},useState:xe,useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=xe(a),d=c[0],e=c[1];eh(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=
xe(!1),c=b[0];b=b[1];return[ih(Ce.bind(null,b,a),[b,a]),c]}},ej={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Vc,useRef:dh,useState:function(a){return Vc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Vc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Vc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,
b,a),[b,a]),c]}},fj={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Wc,useRef:dh,useState:function(a){return Wc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Wc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Wc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,b,a),[b,a]),c]}},ra=null,Ka=null,Wa=
!1,gj=da.ReactCurrentOwner,ia=!1,Je={dehydrated:null,retryTime:0};var jj=function(a,b,c,d){for(c=b.child;null!==c;){if(5===c.tag||6===c.tag)a.appendChild(c.stateNode);else if(4!==c.tag&&null!==c.child){c.child.return=c;c=c.child;continue}if(c===b)break;for(;null===c.sibling;){if(null===c.return||c.return===b)return;c=c.return}c.sibling.return=c.return;c=c.sibling}};var wh=function(a){};var ij=function(a,b,c,d,e){var f=a.memoizedProps;if(f!==d){var g=b.stateNode;Ta(ja.current);a=null;switch(c){case "input":f=
Cd(g,f);d=Cd(g,d);a=[];break;case "option":f=Fd(g,f);d=Fd(g,d);a=[];break;case "select":f=M({},f,{value:void 0});d=M({},d,{value:void 0});a=[];break;case "textarea":f=Gd(g,f);d=Gd(g,d);a=[];break;default:"function"!==typeof f.onClick&&"function"===typeof d.onClick&&(g.onclick=uc)}Ud(c,d);var h,m;c=null;for(h in f)if(!d.hasOwnProperty(h)&&f.hasOwnProperty(h)&&null!=f[h])if("style"===h)for(m in g=f[h],g)g.hasOwnProperty(m)&&(c||(c={}),c[m]="");else"dangerouslySetInnerHTML"!==h&&"children"!==h&&"suppressContentEditableWarning"!==
h&&"suppressHydrationWarning"!==h&&"autoFocus"!==h&&(db.hasOwnProperty(h)?a||(a=[]):(a=a||[]).push(h,null));for(h in d){var k=d[h];g=null!=f?f[h]:void 0;if(d.hasOwnProperty(h)&&k!==g&&(null!=k||null!=g))if("style"===h)if(g){for(m in g)!g.hasOwnProperty(m)||k&&k.hasOwnProperty(m)||(c||(c={}),c[m]="");for(m in k)k.hasOwnProperty(m)&&g[m]!==k[m]&&(c||(c={}),c[m]=k[m])}else c||(a||(a=[]),a.push(h,c)),c=k;else"dangerouslySetInnerHTML"===h?(k=k?k.__html:void 0,g=g?g.__html:void 0,null!=k&&g!==k&&(a=a||
[]).push(h,k)):"children"===h?g===k||"string"!==typeof k&&"number"!==typeof k||(a=a||[]).push(h,""+k):"suppressContentEditableWarning"!==h&&"suppressHydrationWarning"!==h&&(db.hasOwnProperty(h)?(null!=k&&oa(e,h),a||g===k||(a=[])):(a=a||[]).push(h,k))}c&&(a=a||[]).push("style",c);e=a;if(b.updateQueue=e)b.effectTag|=4}};var kj=function(a,b,c,d){c!==d&&(b.effectTag|=4)};var pj="function"===typeof WeakSet?WeakSet:Set,wj="function"===typeof WeakMap?WeakMap:Map,sj=Math.ceil,gd=da.ReactCurrentDispatcher,
Uh=da.ReactCurrentOwner,H=0,Ye=8,ca=16,ma=32,Xa=0,hd=1,Oh=2,ad=3,bd=4,Xe=5,p=H,U=null,t=null,P=0,F=Xa,id=null,ta=1073741823,Yb=1073741823,kd=null,Xb=0,jd=!1,Re=0,Ph=500,l=null,cd=!1,Se=null,La=null,ld=!1,Zb=null,$b=90,bb=null,ac=0,af=null,dd=0,Ja=function(a,b){if(50<ac)throw ac=0,af=null,Error(k(185));a=ed(a,b);if(null!==a){var c=Cc();1073741823===b?(p&Ye)!==H&&(p&(ca|ma))===H?Te(a):(V(a),p===H&&ha()):V(a);(p&4)===H||98!==c&&99!==c||(null===bb?bb=new Map([[a,b]]):(c=bb.get(a),(void 0===c||c>b)&&bb.set(a,
b)))}};var zj=function(a,b,c){var d=b.expirationTime;if(null!==a){var e=b.pendingProps;if(a.memoizedProps!==e||G.current)ia=!0;else{if(d<c){ia=!1;switch(b.tag){case 3:sh(b);Ee();break;case 5:bh(b);if(b.mode&4&&1!==c&&e.hidden)return b.expirationTime=b.childExpirationTime=1,null;break;case 1:N(b.type)&&Bc(b);break;case 4:se(b,b.stateNode.containerInfo);break;case 10:d=b.memoizedProps.value;e=b.type._context;y(Ic,e._currentValue);e._currentValue=d;break;case 13:if(null!==b.memoizedState){d=b.child.childExpirationTime;
if(0!==d&&d>=c)return th(a,b,c);y(D,D.current&1);b=sa(a,b,c);return null!==b?b.sibling:null}y(D,D.current&1);break;case 19:d=b.childExpirationTime>=c;if(0!==(a.effectTag&64)){if(d)return vh(a,b,c);b.effectTag|=64}e=b.memoizedState;null!==e&&(e.rendering=null,e.tail=null);y(D,D.current);if(!d)return null}return sa(a,b,c)}ia=!1}}else ia=!1;b.expirationTime=0;switch(b.tag){case 2:d=b.type;null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;e=pb(b,B.current);rb(b,c);e=we(null,
b,d,a,e,c);b.effectTag|=1;if("object"===typeof e&&null!==e&&"function"===typeof e.render&&void 0===e.$$typeof){b.tag=1;b.memoizedState=null;b.updateQueue=null;if(N(d)){var f=!0;Bc(b)}else f=!1;b.memoizedState=null!==e.state&&void 0!==e.state?e.state:null;ne(b);var g=d.getDerivedStateFromProps;"function"===typeof g&&Lc(b,d,g,a);e.updater=Mc;b.stateNode=e;e._reactInternalFiber=b;pe(b,d,a,c);b=Ie(null,b,d,!0,f,c)}else b.tag=0,T(null,b,e,c),b=b.child;return b;case 16:a:{e=b.elementType;null!==a&&(a.alternate=
null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;ri(e);if(1!==e._status)throw e._result;e=e._result;b.type=e;f=b.tag=Gj(e);a=aa(e,a);switch(f){case 0:b=He(null,b,e,a,c);break a;case 1:b=rh(null,b,e,a,c);break a;case 11:b=nh(null,b,e,a,c);break a;case 14:b=oh(null,b,e,aa(e.type,a),d,c);break a}throw Error(k(306,e,""));}return b;case 0:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),He(a,b,d,e,c);case 1:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),rh(a,b,d,e,c);
case 3:sh(b);d=b.updateQueue;if(null===a||null===d)throw Error(k(282));d=b.pendingProps;e=b.memoizedState;e=null!==e?e.element:null;oe(a,b);Qb(b,d,null,c);d=b.memoizedState.element;if(d===e)Ee(),b=sa(a,b,c);else{if(e=b.stateNode.hydrate)Ka=kb(b.stateNode.containerInfo.firstChild),ra=b,e=Wa=!0;if(e)for(c=Fe(b,null,d,c),b.child=c;c;)c.effectTag=c.effectTag&-3|1024,c=c.sibling;else T(a,b,d,c),Ee();b=b.child}return b;case 5:return bh(b),null===a&&De(b),d=b.type,e=b.pendingProps,f=null!==a?a.memoizedProps:
null,g=e.children,Yd(d,e)?g=null:null!==f&&Yd(d,f)&&(b.effectTag|=16),qh(a,b),b.mode&4&&1!==c&&e.hidden?(b.expirationTime=b.childExpirationTime=1,b=null):(T(a,b,g,c),b=b.child),b;case 6:return null===a&&De(b),null;case 13:return th(a,b,c);case 4:return se(b,b.stateNode.containerInfo),d=b.pendingProps,null===a?b.child=wb(b,null,d,c):T(a,b,d,c),b.child;case 11:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),nh(a,b,d,e,c);case 7:return T(a,b,b.pendingProps,c),b.child;case 8:return T(a,
b,b.pendingProps.children,c),b.child;case 12:return T(a,b,b.pendingProps.children,c),b.child;case 10:a:{d=b.type._context;e=b.pendingProps;g=b.memoizedProps;f=e.value;var h=b.type._context;y(Ic,h._currentValue);h._currentValue=f;if(null!==g)if(h=g.value,f=Qa(h,f)?0:("function"===typeof d._calculateChangedBits?d._calculateChangedBits(h,f):1073741823)|0,0===f){if(g.children===e.children&&!G.current){b=sa(a,b,c);break a}}else for(h=b.child,null!==h&&(h.return=b);null!==h;){var m=h.dependencies;if(null!==
m){g=h.child;for(var l=m.firstContext;null!==l;){if(l.context===d&&0!==(l.observedBits&f)){1===h.tag&&(l=Ea(c,null),l.tag=Jc,Fa(h,l));h.expirationTime<c&&(h.expirationTime=c);l=h.alternate;null!==l&&l.expirationTime<c&&(l.expirationTime=c);Sg(h.return,c);m.expirationTime<c&&(m.expirationTime=c);break}l=l.next}}else g=10===h.tag?h.type===b.type?null:h.child:h.child;if(null!==g)g.return=h;else for(g=h;null!==g;){if(g===b){g=null;break}h=g.sibling;if(null!==h){h.return=g.return;g=h;break}g=g.return}h=
g}T(a,b,e.children,c);b=b.child}return b;case 9:return e=b.type,f=b.pendingProps,d=f.children,rb(b,c),e=W(e,f.unstable_observedBits),d=d(e),b.effectTag|=1,T(a,b,d,c),b.child;case 14:return e=b.type,f=aa(e,b.pendingProps),f=aa(e.type,f),oh(a,b,e,f,d,c);case 15:return ph(a,b,b.type,b.pendingProps,d,c);case 17:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),b.tag=1,N(d)?(a=!0,Bc(b)):a=!1,rb(b,c),Yg(b,d,e),pe(b,d,e,c),Ie(null,
b,d,!0,a,c);case 19:return vh(a,b,c)}throw Error(k(156,b.tag));};var bf=null,Ne=null,la=function(a,b,c,d){return new Fj(a,b,c,d)};ef.prototype.render=function(a){md(a,this._internalRoot,null,null)};ef.prototype.unmount=function(){var a=this._internalRoot,b=a.containerInfo;md(null,a,null,function(){b[Lb]=null})};var Di=function(a){if(13===a.tag){var b=Fc(ka(),150,100);Ja(a,b);df(a,b)}};var Yf=function(a){13===a.tag&&(Ja(a,3),df(a,3))};var Bi=function(a){if(13===a.tag){var b=ka();b=Va(b,a,null);Ja(a,
b);df(a,b)}};sd=function(a,b,c){switch(b){case "input":Dd(a,c);b=c.name;if("radio"===c.type&&null!=b){for(c=a;c.parentNode;)c=c.parentNode;c=c.querySelectorAll("input[name="+JSON.stringify(""+b)+'][type="radio"]');for(b=0;b<c.length;b++){var d=c[b];if(d!==a&&d.form===a.form){var e=ae(d);if(!e)throw Error(k(90));Gf(d);Dd(d,e)}}}break;case "textarea":Lf(a,c);break;case "select":b=c.value,null!=b&&hb(a,!!c.multiple,b,!1)}};(function(a,b,c,d){ee=a;eg=b;vd=c;vf=d})(Qh,function(a,b,c,d,e){var f=p;p|=4;
try{return Da(98,a.bind(null,b,c,d,e))}finally{p=f,p===H&&ha()}},function(){(p&(1|ca|ma))===H&&(uj(),xb())},function(a,b){var c=p;p|=2;try{return a(b)}finally{p=c,p===H&&ha()}});var mk={Events:[Hb,Pa,ae,pf,qd,lb,function(a){Kd(a,Ki)},sf,tf,sc,pc,xb,{current:!1}]};(function(a){var b=a.findFiberByHostInstance;return Ej(M({},a,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:da.ReactCurrentDispatcher,findHostInstanceByFiber:function(a){a=Sf(a);
return null===a?null:a.stateNode},findFiberByHostInstance:function(a){return b?b(a):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))})({findFiberByHostInstance:Bb,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"});I.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=mk;I.createPortal=Xh;I.findDOMNode=function(a){if(null==a)return null;if(1===a.nodeType)return a;var b=a._reactInternalFiber;if(void 0===
b){if("function"===typeof a.render)throw Error(k(188));throw Error(k(268,Object.keys(a)));}a=Sf(b);a=null===a?null:a.stateNode;return a};I.flushSync=function(a,b){if((p&(ca|ma))!==H)throw Error(k(187));var c=p;p|=1;try{return Da(99,a.bind(null,b))}finally{p=c,ha()}};I.hydrate=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!0,c)};I.render=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!1,c)};I.unmountComponentAtNode=function(a){if(!bc(a))throw Error(k(40));return a._reactRootContainer?
(Rh(function(){nd(null,null,a,!1,function(){a._reactRootContainer=null;a[Lb]=null})}),!0):!1};I.unstable_batchedUpdates=Qh;I.unstable_createPortal=function(a,b){return Xh(a,b,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)};I.unstable_renderSubtreeIntoContainer=function(a,b,c,d){if(!bc(c))throw Error(k(200));if(null==a||void 0===a._reactInternalFiber)throw Error(k(38));return nd(a,b,c,!1,d)};I.version="16.13.1"});
</script>
    <script>const e = React.createElement;

function pathToString(path) {
  if (path[0] === '/') {
    return '/' + path.slice(1).join('/');
  } else {
    return path.join('/');
  }
}

function findCommonPath(files) {
  if (!files || !files.length) {
    return [];
  }

  function isPrefix(arr, prefix) {
    if (arr.length < prefix.length) {
      return false;
    }
    for (let i = prefix.length - 1; i >= 0; --i) {
      if (arr[i] !== prefix[i]) {
        return false;
      }
    }
    return true;
  }

  let commonPath = files[0].path.slice(0, -1);
  while (commonPath.length) {
    if (files.every(file => isPrefix(file.path, commonPath))) {
      break;
    }
    commonPath.pop();
  }
  return commonPath;
}

function findFolders(files) {
  if (!files || !files.length) {
    return [];
  }

  let folders = files.filter(file => file.path.length > 1).map(file => file.path[0]);
  folders = [...new Set(folders)]; // unique
  folders.sort();

  folders = folders.map(folder => {
    let filesInFolder = files
      .filter(file => file.path[0] === folder)
      .map(file => ({
        ...file,
        path: file.path.slice(1),
        parent: [...file.parent, file.path[0]],
      }));

    const children = findFolders(filesInFolder); // recursion

    return {
      is_folder: true,
      path: [folder],
      parent: files[0].parent,
      children,
      covered: children.reduce((sum, file) => sum + file.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.coverable, 0),
      prevRun: {
        covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
        coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
      }
    };
  });

  return [
    ...folders,
    ...files.filter(file => file.path.length === 1),
  ];
}

class App extends React.Component {
  constructor(...args) {
    super(...args);

    this.state = {
      current: [],
    };
  }

  componentDidMount() {
    this.updateStateFromLocation();
    window.addEventListener("hashchange", () => this.updateStateFromLocation(), false);
  }

  updateStateFromLocation() {
    if (window.location.hash.length > 1) {
      const current = window.location.hash.substr(1).split('/');
      this.setState({current});
    } else {
      this.setState({current: []});
    }
  }

  getCurrentPath() {
    let file = this.props.root;
    let path = [file];
    for (let p of this.state.current) {
      file = file.children.find(file => file.path[0] === p);
      if (!file) {
        return path;
      }
      path.push(file);
    }
    return path;
  }

  render() {
    const path = this.getCurrentPath();
    const file = path[path.length - 1];

    let w = null;
    if (file.is_folder) {
      w = e(FilesList, {
        folder: file,
        onSelectFile: this.selectFile.bind(this),
        onBack: path.length > 1 ? this.back.bind(this) : null,
      });
    } else {
      w = e(DisplayFile, {
        file,
        onBack: this.back.bind(this),
      });
    }

    return e('div', {className: 'app'}, w);
  }

  selectFile(file) {
    this.setState(({current}) => {
      return {current: [...current, file.path[0]]};
    }, () => this.updateHash());
  }

  back(file) {
    this.setState(({current}) => {
      return {current: current.slice(0, current.length - 1)};
    }, () => this.updateHash());
  }

  updateHash() {
    if (!this.state.current || !this.state.current.length) {
      window.location = '#';
    } else {
      window.location = '#' + this.state.current.join('/');
    }
  }
}

function FilesList({folder, onSelectFile, onBack}) {
  let files = folder.children;
  return e('div', {className: 'display-folder'},
    e(FileHeader, {file: folder, onBack}),
    e('table', {className: 'files-list'},
      e('thead', {className: 'files-list__head'},
        e('tr', null,
          e('th', null, "Path"),
          e('th', null, "Coverage")
        )
      ),
      e('tbody', {className: 'files-list__body'},
        files.map(file => e(File, {file, onClick: onSelectFile}))
      )
    )
  );
}

function File({file, onClick}) {
  const coverage = file.coverable ? file.covered / file.coverable * 100 : -1;
  const coverageDelta = file.prevRun &&
    (file.covered / file.coverable * 100 - file.prevRun.covered / file.prevRun.coverable * 100);

  return e('tr', {
      className: 'files-list__file'
        + (coverage >= 0 && coverage < 50 ? ' files-list__file_low': '')
        + (coverage >= 50 && coverage < 80 ? ' files-list__file_medium': '')
        + (coverage >= 80 ? ' files-list__file_high': '')
        + (file.is_folder ? ' files-list__file_folder': ''),
      onClick: () => onClick(file),
    },
    e('td', null, e('a', null, pathToString(file.path))),
    e('td', null,
      file.covered + ' / ' + file.coverable +
      (coverage >= 0 ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e('span', {title: 'Change from the previous run'},
        (coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : ''))
    )
  );
}

function DisplayFile({file, onBack}) {
  return e('div', {className: 'display-file'},
    e(FileHeader, {file, onBack}),
    e(FileContent, {file})
  );
}

function FileHeader({file, onBack}) {
  const coverage = file.covered / file.coverable * 100;
  const coverageDelta = file.prevRun && (coverage - file.prevRun.covered / file.prevRun.coverable * 100);

  return e('div', {className: 'file-header'},
    onBack ? e('a', {className: 'file-header__back', onClick: onBack}, 'Back') : null,
    e('div', {className: 'file-header__name'}, pathToString([...file.parent, ...file.path])),
    e('div', {className: 'file-header__stat'},
      'Covered: ' + file.covered + ' of ' + file.coverable +
      (file.coverable ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e('span', {title: 'Change from the previous run'},
        (coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : ''))
    )
  );
}

function FileContent({file}) {
  return e('pre', {className: 'file-content'},
    file.content.split(/\r?\n/).map((line, index) => {
      const trace = file.traces.find(trace => trace.line === index + 1);
      const covered = trace && trace.stats.Line;
      const uncovered = trace && !trace.stats.Line;
      return e('code', {
          className: 'code-line'
            + (covered ? ' code-line_covered' : '')
            + (uncovered ? ' code-line_uncovered' : ''),
          title: trace ? JSON.stringify(trace.stats, null, 2) : null,
        }, line);
    })
  );
}

(function(){
  const commonPath = findCommonPath(data.files);
  const prevFilesMap = new Map();

  previousData && previousData.files.forEach((file) => {
    const path = file.path.slice(commonPath.length).join('/');
    prevFilesMap.set(path, file);
  });

  const files = data.files.map((file) => {
    const path = file.path.slice(commonPath.length);
    const { covered = 0, coverable = 0 } = prevFilesMap.get(path.join('/')) || {};
    return {
      ...file,
      path,
      parent: commonPath,
      prevRun: { covered, coverable },
    };
  });

  const children = findFolders(files);

  const root = {
    is_folder: true,
    children,
    path: commonPath,
    parent: [],
    covered: children.reduce((sum, file) => sum + file.covered, 0),
    coverable: children.reduce((sum, file) => sum + file.coverable, 0),
    prevRun: {
      covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
    }
  };

  ReactDOM.render(e(App, {root, prevFilesMap}), document.getElementById('root'));
}());
</script>
</body>
</html>